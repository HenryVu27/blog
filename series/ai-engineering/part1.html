<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>What Fills the Context Window: A Guide to Context Engineering | Henry Vu</title>
  <meta name="description" content="A practitioner's guide to context engineering for LLMs. Covers the seven components of a context window, four strategies (write, select, compress, isolate), failure modes, token budgets, and measuring context quality.">
  <meta name="author" content="Henry Vu">
  <meta name="robots" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
  <link rel="canonical" href="https://www.henryvu.blog/series/ai-engineering/part1.html">

  <!-- Open Graph -->
  <meta property="og:type" content="article">
  <meta property="og:title" content="What Fills the Context Window: A Guide to Context Engineering">
  <meta property="og:description" content="The seven components of a context window, four strategies for managing them, failure modes, token budgets, and measuring context quality. A practitioner's deep dive.">
  <meta property="og:url" content="https://www.henryvu.blog/series/ai-engineering/part1.html">
  <meta property="og:site_name" content="Henry Vu's Blog">
  <meta property="og:image" content="https://www.henryvu.blog/series/ai-engineering/OG.jpg">
  <meta property="article:published_time" content="2026-02-24T00:00:00Z">
  <meta property="article:author" content="https://www.henryvu.blog/">
  <meta property="article:section" content="AI Engineering">
  <meta property="article:tag" content="context engineering">
  <meta property="article:tag" content="LLM">
  <meta property="article:tag" content="AI engineering">
  <meta property="article:tag" content="context window">
  <meta property="article:tag" content="RAG">
  <meta property="article:tag" content="token budget">
  <meta property="article:tag" content="prompt engineering">
  <meta property="article:tag" content="context window optimization">
  <meta property="article:tag" content="LLM context management">
  <meta property="article:tag" content="AI agents">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@HenryVu27">
  <meta name="twitter:creator" content="@HenryVu27">
  <meta name="twitter:title" content="What Fills the Context Window: A Guide to Context Engineering">
  <meta name="twitter:description" content="The seven components of a context window, four strategies for managing them, failure modes, and token budgets.">
  <meta name="twitter:image" content="https://www.henryvu.blog/series/ai-engineering/OG.jpg">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">

  <!-- RSS Feed -->
  <link rel="alternate" type="application/rss+xml" title="Henry Vu's Blog" href="https://www.henryvu.blog/feed.xml">

  <!-- Fonts & Styles (EXISTING - do not change) -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:ital,wght@0,300..700;1,300..700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../../css/styles.css">
  <link rel="stylesheet" href="../../css/post-styles.css">
  <link rel="stylesheet" href="../../css/deep-dive-layout.css">
  <link rel="stylesheet" href="../../css/components.css">
  <link rel="stylesheet" href="../../css/mode-toggle.css">
  <link rel="stylesheet" href="../../css/prism-blog.css" media="print" onload="this.media='all'">
  <link rel="stylesheet" href="../../css/code-blocks.css" media="print" onload="this.media='all'">
  <script>try{if(localStorage.getItem('blog-reading-mode')!=='tldr')document.documentElement.classList.add('deep-dive-mode')}catch(e){}</script>
  <style>
    /* Remove top margin so hero is flush with viewport top */
    .page { margin-top: 0; }

    /* Hero: full-width container so background never clips.
       .page-header.hero-header beats .page-header specificity in deep-dive-layout.css */
    .page-header.hero-header {
      position: relative;
      max-width: none;
      width: 100vw;
      margin-left: calc(-50vw + 50%);
      padding: 104px 16px 204px;
      background: url('OG1.jpg') center / cover no-repeat;
      box-sizing: border-box;
    }
    .page-header.hero-header::after {
      content: '';
      position: absolute;
      inset: 0;
      background: rgba(0, 0, 0, 0.35);
      z-index: 0;
      pointer-events: none;
    }
    .page-header.hero-header > * {
      position: relative;
      z-index: 1;
      max-width: 800px;
      margin-left: auto;
      margin-right: auto;
    }
    .toc ul {
      margin-top: 0;
    }
    .hero-header .meta {
      margin-bottom: 20px;
    }
    .hero-header .mode-toggle {
      margin-top: 0;
      margin-bottom: 12px;
    }
    .hero-header .title,
    .hero-header .meta,
    .hero-header .back-btn,
    .hero-header .series-nav h3,
    .hero-header .series-nav li {
      color: #fff;
    }
    /* Mode toggle: inactive = translucent, active = solid white */
    .hero-header .mode-toggle button {
      color: rgba(255,255,255,0.65);
      position: relative;
    }
    .hero-header .mode-toggle button::after {
      content: '';
      position: absolute;
      left: 0;
      bottom: -2px;
      width: 100%;
      height: 2px;
      background: rgba(255,255,255,0.65);
      transform: scaleX(0);
      transform-origin: left;
      transition: transform 250ms ease;
    }
    .hero-header .mode-toggle button:hover:not(.active)::after {
      transform: scaleX(1);
    }
    .hero-header .mode-toggle button.active {
      color: #fff;
      border-bottom-color: #fff;
    }
    .hero-header .mode-toggle button:hover:not(.active) {
      color: rgba(255,255,255,0.9);
    }
    /* Series nav links: translucent, brighten on hover */
    .hero-header .series-nav a {
      color: rgba(255,255,255,0.7);
      transition: color 150ms ease;
      position: relative;
    }
    .hero-header .series-nav a::after {
      content: '';
      position: absolute;
      left: 0;
      bottom: -1px;
      width: 100%;
      height: 1px;
      background: rgba(255,255,255,0.7);
      transform: scaleX(0);
      transform-origin: left;
      transition: transform 250ms ease;
    }
    .hero-header .series-nav a:hover::after {
      transform: scaleX(1);
    }
    .hero-header .series-nav a:hover {
      color: #fff;
    }
    .hero-header .back-btn {
      color: rgba(255,255,255,0.7);
    }
    .hero-header .back-btn:hover {
      color: #fff;
    }
    .hero-header hr {
      display: none;
    }
    @media (max-width: 1024px) {
      .page-header.hero-header {
        padding: 52px 16px 56px;
      }
    }
  </style>
  <!-- KaTeX for Deep Dive math -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" media="print" onload="this.media='all'">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>

  <!-- Google Analytics (GA4) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-3CJTJES82Q"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-3CJTJES82Q');
  </script>

  <!-- JSON-LD Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "What Fills the Context Window: A Guide to Context Engineering",
    "description": "A practitioner's guide to context engineering for LLMs. Covers the seven components of a context window, four strategies (write, select, compress, isolate), failure modes, token budgets, and measuring context quality.",
    "image": "https://www.henryvu.blog/series/ai-engineering/OG.jpg",
    "datePublished": "2026-02-24",
    "dateModified": "2026-02-24",
    "author": {
      "@type": "Person",
      "@id": "https://www.henryvu.blog/#author",
      "name": "Henry Vu",
      "url": "https://www.henryvu.blog",
      "sameAs": [
        "https://x.com/HenryVu27",
        "https://www.linkedin.com/in/henry-vu27/",
        "https://github.com/HenryVu27"
      ]
    },
    "publisher": {
      "@type": "Person",
      "name": "Henry Vu",
      "url": "https://www.henryvu.blog"
    },
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://www.henryvu.blog/series/ai-engineering/part1.html"
    },
    "isPartOf": {
      "@type": "CreativeWorkSeries",
      "name": "AI Engineering Series",
      "url": "https://www.henryvu.blog/"
    },
    "timeRequired": "PT25M",
    "keywords": ["context engineering", "AI engineering", "LLM", "context window", "token budget", "RAG", "prompt engineering", "AI agents"],
    "abstract": "Context engineering is the discipline of building the right information into an LLM's context window at inference time. A context window is composed of seven components: system prompts, user prompts, conversation state, long-term memory, retrieved knowledge (RAG), tool definitions, and output schemas. The four strategies for managing these components are write (authoring static instructions), select (retrieving relevant information via RAG or memory), compress (reducing token usage through summarization or distillation), and isolate (splitting work across multiple LLM calls to stay within context limits). Common failure modes include context dilution (too much irrelevant information drowning out the signal), context distraction (conflicting instructions confusing the model), context clash (contradictory information in different parts of the window), and context confusion (ambiguous references the model cannot resolve). Effective context engineering requires measuring context quality through metrics like sufficiency, noise ratio, and utilization, rather than treating the context window as an unlimited text dump.",
    "speakable": {
      "@type": "SpeakableSpecification",
      "cssSelector": ["#seven-components", "#four-strategies", "#failure-modes"]
    },
    "breadcrumb": {
      "@type": "BreadcrumbList",
      "itemListElement": [
        {
          "@type": "ListItem",
          "position": 1,
          "name": "Home",
          "item": "https://www.henryvu.blog/"
        },
        {
          "@type": "ListItem",
          "position": 2,
          "name": "AI Engineering",
          "item": "https://www.henryvu.blog/series/ai-engineering/part1.html"
        }
      ]
    }
  }
  </script>

  <!-- FAQ Schema (invisible structured data for rich results) -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "FAQPage",
    "mainEntity": [
      {
        "@type": "Question",
        "name": "What is context engineering?",
        "acceptedAnswer": {
          "@type": "Answer",
          "text": "Context engineering is the discipline of building the right information into an LLM's context window at inference time. Unlike prompt engineering, which focuses on phrasing instructions, context engineering manages the full lifecycle of everything the model sees: system prompts, user input, conversation history, retrieved knowledge (RAG), tool definitions, output schemas, and long-term memory. The term was popularized by Andrej Karpathy and Shopify CEO Tobi Lutke in 2025."
        }
      },
      {
        "@type": "Question",
        "name": "What are the components of an LLM context window?",
        "acceptedAnswer": {
          "@type": "Answer",
          "text": "An LLM context window is composed of seven components: (1) system prompts that set behavior and constraints, (2) user prompts containing the current request, (3) conversation state preserving multi-turn history, (4) long-term memory from external storage like databases, (5) retrieved knowledge via RAG from vector stores or search, (6) tool definitions describing available functions the model can call, and (7) output schemas that constrain the response format. Each component competes for the same finite token budget."
        }
      },
      {
        "@type": "Question",
        "name": "What is the difference between context engineering and prompt engineering?",
        "acceptedAnswer": {
          "@type": "Answer",
          "text": "Prompt engineering focuses on crafting the wording of instructions to get better model outputs. Context engineering is broader: it manages all seven components that fill the context window, including retrieval, memory, tool selection, conversation history, and token budget allocation. Prompt engineering is one technique within the 'write' strategy of context engineering. As Tobi Lutke put it, the biggest unlock is 'context engineering over prompt engineering.'"
        }
      },
      {
        "@type": "Question",
        "name": "What are common context window failure modes?",
        "acceptedAnswer": {
          "@type": "Answer",
          "text": "The four main failure modes are: context dilution (too much irrelevant information drowning out the signal, causing the model to miss key details), context distraction (conflicting or contradictory instructions confusing the model about which to follow), context clash (contradictory facts in different parts of the window, such as outdated retrieved documents conflicting with newer information), and context confusion (ambiguous references the model cannot resolve, like unclear pronoun references across conversation turns)."
        }
      },
      {
        "@type": "Question",
        "name": "How do you manage token budgets in LLM applications?",
        "acceptedAnswer": {
          "@type": "Answer",
          "text": "Token budget management requires explicitly allocating your context window across all seven components. A practical approach: reserve tokens for system prompts (fixed cost), allocate a portion for RAG retrieval results, set limits on conversation history (summarizing or truncating older turns), and leave headroom for the model's output. The four strategies are write (author static instructions), select (retrieve only relevant information), compress (summarize or distill to reduce tokens), and isolate (split work across multiple LLM calls when one context window isn't enough)."
        }
      },
      {
        "@type": "Question",
        "name": "How do you measure context quality for LLMs?",
        "acceptedAnswer": {
          "@type": "Answer",
          "text": "Context quality can be measured through three dimensions: sufficiency (does the context contain all information needed to answer correctly), noise ratio (what fraction of tokens are relevant vs. irrelevant filler), and utilization (how much of the available context window is being used effectively). Benchmarks like ContextBench and LongMemEval test these dimensions. In practice, tracking retrieval precision, monitoring context window utilization, and measuring task completion rates against context configurations are the most actionable metrics."
        }
      }
    ]
  }
  </script>
</head>
<body>
  <main class="page">
    <div class="page-header hero-header">
      <nav class="nav">
        <a href="../../index.html" class="back-btn">←</a>
      </nav>

      <header>
        <h1 class="title">What Fills the Context Window</h1>
        <div class="meta">
          2026-02-24 · <span class="deep-dive-only">25 min read</span><span class="tldr-only">5 min read</span>
        </div>
        <div class="mode-toggle">
          <button data-mode="deep-dive">Deep Dive</button>
          <button data-mode="tldr" class="active">TLDR</button>
        </div>
        <hr>
      </header>

      <div class="series-nav">
        <h3>AI Engineering Series</h3>
        <ul>
          <li>Part 1: What Fills the Context Window (current)</li>
          <li>Part 2: RAG, From Retrieval to Reasoning (coming soon)</li>
          <li>Part 3: Memory Engineering (coming soon)</li>
          <li>Part 4: Building AI Agents (coming soon)</li>
          <li>Part 5: Multi-Agent Systems (coming soon)</li>
        </ul>
      </div>
    </div>



    <div class="layout" style="margin-top: 48px;">
      <aside class="toc deep-dive-only">
        <div class="toc-sticky">
          <h2>Contents</h2>
          <ul>
            <li class="deep-dive-only"><a href="#from-prompts-to-context"><span class="toc-section-number">1</span><span class="toc-section-title">From Prompts to Context</span></a></li>
            <li><a href="#seven-components"><span class="toc-section-number">2</span><span class="toc-section-title">Seven Components</span></a></li>
            <li><a href="#four-strategies"><span class="toc-section-number">3</span><span class="toc-section-title">Write / Select / Compress / Isolate</span></a></li>
            <li><a href="#failure-modes"><span class="toc-section-number">4</span><span class="toc-section-title">How Context Fails</span></a></li>
            <li><a href="#token-budget"><span class="toc-section-number">5</span><span class="toc-section-title">A Real Token Budget</span></a></li>
            <li><a href="#measuring"><span class="toc-section-number">6</span><span class="toc-section-title">Measuring Context Quality</span></a></li>
            <li><a href="#what-i-learned"><span class="toc-section-number">7</span><span class="toc-section-title">Lesson</span></a></li>
            <li class="deep-dive-only"><a href="#references"><span class="toc-section-number">8</span><span class="toc-section-title">References</span></a></li>
          </ul>
        </div>
      </aside>

      <article class="post">

        <!-- ============================================================ -->
        <!-- SECTION 1: From Prompts to Context                           -->
        <!-- ============================================================ -->
        <section id="from-prompts-to-context" class="deep-dive-only">
          <h2>From Prompts to Context</h2>

          <p>When GPT-3 landed in 2020, early adopters quickly discovered that tiny changes in phrasing produced wildly different outputs. Swap "summarize" for "explain briefly," add "step by step," rearrange a few words, tell it to roleplay as a character with no safety guidelines, and the same model would go from incoherent to useful to giving you a recipe for homemade biological weapons. That sensitivity created an entire discipline called "prompt engineering", the craft of writing instructions that reliably steer language models toward useful behavior.</p>

          <p>For a while, prompt engineering was the whole game. Your system prompt was a paragraph or two, the context window was 4K tokens, and the main skill was wordsmithing, finding the exact phrasing, the right few-shot examples, the magic "think step by step" incantation that unlocked the behavior you wanted.</p>

          <p>Context windows eventually went from 4K to 200K tokens, and models became intelligent enough that phrasing stopped being the bottleneck. Sometime around mid 2025, the community started using the term "context engineering" instead, and the new label caught on fast. <a href="#ref-karpathy">Andrej Karpathy</a> called it "the delicate art and science of filling the context window with just the right information for the next step." I like that framing because it puts the emphasis on information selection, not wordsmithing.</p>

          <p>Most agent failures I encounter today are context failures, where the model can do what I need but doesn't have the right information when it needs it. The system prompt is but one of seven components that fill the context window, and for a simple single-turn task, careful prompt engineering is all you need. But the moment you add retrieval, tools, multi-step reasoning, or agent workflows, the challenge shifts from "how do I phrase this instruction" to "what information does the model need to see right now, and how do I assemble it reliably."</p>

          <p>This post walks through those seven components, the strategies for managing them, how they fail, and what a real token budget looks like in a diabetes management coaching agent I built with LangGraph. I'll go deeper on RAG in Part 2, memory in Part 3, agents in Part 4, and multi-agent coordination in Part 5.</p>

          <div class="theory-only">
            <div class="callout">
              <p><a href="#ref-mei">Mei et al.'s 166-page survey</a> (arXiv:2507.13334, analyzing 1,411 papers) provides the first mathematical formalization for context engineering. They model context as a structured assembly of six typed components:</p>
              <p>$$C = A(c_\text{instr},\; c_\text{know},\; c_\text{tools},\; c_\text{mem},\; c_\text{state},\; c_\text{query}).$$</p>
              <p>Context engineering then becomes a constrained optimization problem, where you find the assembly function $F$ that maximizes expected reward across tasks, subject to a hard window size limit:</p>
              <p>$$F^* = \arg\max_F \; \mathbb{E}_{\tau \sim \mathcal{T}} \left[\text{Reward}\!\left(P_\theta(Y \mid C_{F}(\tau)),\; Y^*_\tau\right)\right] \quad \text{s.t.} \;\; |C| \leq L_\text{max}.$$</p>
              <p>My first reaction was that this is ML researchers dressing up practitioner intuition in math to make it paper-worthy. But coming from an optimization background, the formulation maps onto an intuition I'm already familiar with.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 2: The Seven Components of Context                   -->
        <!-- ============================================================ -->
        <section id="seven-components">
          <h2>Seven Components</h2>

          <div class="deep-dive-only">
          <p>Every LLM call consumes a context window, a fixed-size buffer of tokens containing everything the model can see. What you put in that buffer determines what the model can do. Several people have converged on roughly the same decomposition (<a href="#ref-manus">Manus</a>, <a href="#ref-anthropic">Anthropic</a>, <a href="#ref-langchain">LangChain</a>, Google), and seven components turns out to be the right granularity.</p>
          </div>

          <div style="border-radius:8px; padding:12px 14px; margin:24px 0; background:#ece6d6;">
            <div style="display:flex; justify-content:space-between; align-items:center; margin-bottom:16px;">
              <span style="font-size:14px; font-weight:500; text-transform:uppercase; letter-spacing:0.05em; color:var(--fg);">Context Window</span>
              <span style="font-size:14px; color:var(--fg);">8K-200K tokens</span>
            </div>
            <div style="background:#f9f7f3; border-radius:6px; padding:16px 18px;">
              <div style="display:flex; flex-direction:column; gap:6px; font-size:13px;">
                <div style="display:flex; align-items:center; gap:10px;">
                  <div style="width:20%; background:#efe6d4; border:1.5px solid #b8a88a; border-radius:3px; padding:6px 10px; color:#8a7a62; font-weight:600; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">System prompt</div>
                  <span style="color:var(--muted); font-size:12px;">500-2,000</span>
                </div>
                <div style="display:flex; align-items:center; gap:10px;">
                  <div style="width:15%; background:#f2ebdd; border:1.5px solid #c4b69e; border-radius:3px; padding:6px 10px; color:#9a8c74; font-weight:600; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">User prompt</div>
                  <span style="color:var(--muted); font-size:12px; font-style:italic;">variable</span>
                </div>
                <div style="display:flex; align-items:center; gap:10px;">
                  <div style="width:50%; background:#e8dece; border:1.5px solid #9a8970; border-radius:3px; padding:6px 10px; color:#7a6c58; font-weight:600; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">State + conversation history</div>
                  <span style="color:var(--muted); font-size:12px;">500-5,000</span>
                </div>
                <div style="display:flex; align-items:center; gap:10px;">
                  <div style="width:15%; background:#f2ebdd; border:1.5px solid #c4b69e; border-radius:3px; padding:6px 10px; color:#9a8c74; font-weight:600; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Long-term memory</div>
                  <span style="color:var(--muted); font-size:12px; font-style:italic;">variable</span>
                </div>
                <div style="display:flex; align-items:center; gap:10px;">
                  <div style="width:100%; background:#e2d7c5; border:1.5px solid #7a6c58; border-radius:3px; padding:6px 10px; color:#5e5344; font-weight:600; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Retrieved information (RAG)</div>
                  <span style="color:var(--muted); font-size:12px; white-space:nowrap;">0-10,000</span>
                </div>
                <div style="display:flex; align-items:center; gap:10px;">
                  <div style="width:30%; background:#ece3d3; border:1.5px solid #a8987e; border-radius:3px; padding:6px 10px; color:#7e7060; font-weight:600; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Tool definitions</div>
                  <span style="color:var(--muted); font-size:12px;">500-3,000</span>
                </div>
                <div style="display:flex; align-items:center; gap:10px;">
                  <div style="width:5%; min-width:80px; background:#f4ede0; border:1.5px solid #d1c4ac; border-radius:3px; padding:6px 10px; color:#8a7d6a; font-weight:600; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Schema</div>
                  <span style="color:var(--muted); font-size:12px;">50-500</span>
                </div>
              </div>
            </div>
          </div>

          <div class="deep-dive-only">
          <p>One thing that diagram doesn't show is that those advertised window sizes are aspirational. Models claim 200K, 1M, even 2M token windows, but performance degrades well before you hit the ceiling. <a href="#ref-ruler">NVIDIA's RULER benchmark</a> tested 34 models on tasks harder than simple needle-in-a-haystack retrieval, and found that only about half could maintain quality at 32K tokens, despite most claiming support for 128K or more. The effective context length, the range where the model actually reasons reliably rather than just pattern-matching on nearby tokens, tends to be a fraction of the number on the spec sheet. I've seen this consistently across providers where my agent's output quality degrades substantially somewhere around 80-100K tokens, even on a 200K model,.</p>
          </div>

          <div class="tldr-only">
            <ul class="list-tight">
              <li>Version system prompts like code</li>
              <li>Use pull-based RAG (model requests via tools) over front-loading</li>
              <li>Full conversation dumps hurt performance; trim aggressively</li>
              <li>Tool definitions consume tokens whether used or not</li>
            </ul>
          </div>

          <div class="deep-dive-only">
          <p>Of the seven, the <strong>system prompt</strong> is where most people start. It sets behavioral guidelines, role definitions, and rules, i.e. the entire personality of your agent. What I didn't expect when I started building agents is how much the system prompt wants to grow. Every failure mode you encounter tempts you to add another instruction, and before long you're at 4,000 tokens of rules that sometimes contradict each other. I've learned (the expensive way) to start minimal and iteratively add instructions based on observed failures. <a href="#ref-spotify">Spotify's engineering team</a> found that larger, static, version-controlled prompts proved more predictable than dynamic tool-based approaches. I treat my system prompts as code now, versioned, reviewed, and tested.</p>

          <p>My agent's system prompt is broken into XML-tagged sections:</p>
          </div>

<div class="code-block" data-filename="system_prompt.xml" data-lang="xml">
<pre><code>&lt;identity&gt;
  Personality, voice, coach name
&lt;/identity&gt;

&lt;boundaries&gt;
  5 hard scope rules + output gate awareness
  "If your response recommends specific medications...
   the entire response will be discarded."
&lt;/boundaries&gt;

&lt;patient-context&gt;
  Structured profile, session summary, goals
&lt;/patient-context&gt;

&lt;approach&gt;
  Progressive profiling, evidence-based guidance
&lt;/approach&gt;

&lt;tools&gt;
  When to search, when to update profile
&lt;/tools&gt;

&lt;response-guide&gt;
  Adaptive length, tone, situation matching
&lt;/response-guide&gt;

&lt;examples&gt;
  6 few-shot patient coaching conversations
&lt;/examples&gt;</code></pre>
</div>

          <div class="deep-dive-only">
          <p>I started using XML tags mostly for my own sanity (it's easier to review a prompt when you can collapse sections), but it turns out LLMs respond measurably better to structured input than unstructured dumps. Both <a href="#ref-anthropic">Anthropic</a> and Google recommend structured delimiters for this reason. The tags also give you a natural unit for version control diffs, which matters more than you'd think once your prompt is 2,000+ tokens and three people are editing it.</p>

          <p>Once your prompt gets complex enough, you start wanting to break individual sections out into their own files and load them on demand. Agent frameworks like Claude Code already do this with skills (workflow instructions like "debug systematically" or "do TDD") and plugins (third-party extensions) that only enter the context window when the current task calls for them. I used Claude Code to research this post, with agents spawning subagents and 70+ tools to synthesize points from research papers and technical posts. Here's how the context budget split mid-session:</p>

          <div style="display:flex; gap:24px; align-items:flex-start; margin:24px 0;">
            <figure style="margin:0; flex:0 0 55%;">
              <img src="claude-code-context-usage.png" alt="Claude Code context usage breakdown: system prompt 3.4k tokens (1.7%), system tools 17.8k (8.9%), custom agents 399 (0.2%), memory files 9.8k (4.9%), skills 766 (0.4%), messages 51.6k (25.8%), free space 83k (41.6%), autocompact buffer 33k (16.5%), out of 200k total" style="max-width:100%; height:auto;" />
            </figure>
            <div style="flex:1; font-size:14px; line-height:1.7; color:var(--text);">
              <p style="margin:0 0 20px 0;"><strong>Autocompact buffer (33k)</strong><br>The compress strategy baked into infrastructure. The system reserves 16.5% of the window for summarizing older messages as the conversation grows.</p>
              <p style="margin:0 0 20px 0;"><strong>Subagent isolation</strong><br>Each subagent gets its own 200k window rather than sharing the orchestrator's. That's the isolate strategy, splitting work across multiple context windows instead of cramming everything into one.</p>
              <p style="margin:0;"><strong>Composable modules</strong><br>Skills, plugins, memory files, and agent definitions each managed independently. Instead of one monolithic prompt, assemble each agent's context from only the pieces it needs.</p>
            </div>
          </div>

          <p>One system prompt tip from <a href="#ref-openai-prompting">OpenAI's GPT-4.1 prompting guide</a> you may find helpful: three specific instructions that increased their SWE-bench score by ~20%. (1) Persistence: "keep going until the user's query is completely resolved." (2) Tool-calling: "use your tools to answer questions rather than relying on memory." (3) Planning: explicitly asking the model to plan before acting. Three sentences transformed the model "from a chatbot-like state into a much more eager agent," which says something about how sensitive agent behavior is to system prompt phrasing even in 2025.</p>

          <p>I spend the least time worrying about the <strong>user prompt</strong> because it's the one thing I don't control. The immediate message from the human can be anything, from well-structured to incoherent, concise to rambling, and your context engineering has to be robust enough to handle whatever arrives.</p>

          <p>Where I've seen the most waste is <strong>state and short-term history</strong>, the current conversation turns and prior exchanges that serve as the working memory of your system. The <a href="#ref-longmemeval">LongMemEval benchmark</a> (Wu et al., ICLR 2025) showed that models given the full ~115K-token conversation history performed worse than models given only the relevant subset, which tells you everything about the cost of unfocused context. What you remove from history matters at least as much as what you keep.</p>

          <p>The underlying design principle here is that in mature systems, the authoritative data lives outside the window (database, filesystem, structured JSON store), and the context assembly function selects a <em>projection</em> for each turn. The context window is but a view, not the source of truth. My agent's patient profile lives in a persistent store; what the model sees is a snapshot assembled fresh on every turn based on what's relevant right now. This becomes much more important in Part 4 when the filesystem itself becomes the agent's working memory, but even in a single-session agent, treating the window as a read-only view of external state keeps you from accidentally coupling your model's behavior to stale conversation history.</p>

          <p>A simple block with state information is injected to give the model temporal awareness and continuity across the ReAct loop:</p>
          </div>

<div class="code-block" data-filename="state.py" data-lang="python">
<pre><code>def build_conversation_state(turn, phase, recent_tool_calls, active_topic, current_datetime):
    lines = [f"  &lt;turn&gt;{turn}&lt;/turn&gt;", f"  &lt;phase&gt;{phase}&lt;/phase&gt;"]
    if current_datetime:
        day_name = current_datetime.strftime("%A")
        time_of_day = "morning" if hour &lt; 12 else "afternoon" if hour &lt; 17 else "evening"
        lines.append(f"  &lt;datetime&gt;{day_name} {time_of_day}&lt;/datetime&gt;")
    if recent_tool_calls:
        lines.append(f"  &lt;last_tools&gt;{', '.join(recent_tool_calls)}&lt;/last_tools&gt;")
    if active_topic:
        lines.append(f"  &lt;focus&gt;{active_topic}&lt;/focus&gt;")
    return "&lt;conversation_state&gt;\n" + "\n".join(lines) + "\n&lt;/conversation_state&gt;"</code></pre>
</div>

          <div class="deep-dive-only">
          <p>This 50-100 token block tells the model what turn it's on, what phase the conversation is in, what time of day it is, and which tools it already called.</p>

          </div>

          <div class="deep-dive-only">
          <p>I spend the most engineering time on <strong>retrieved information</strong>, the RAG layer. External knowledge from documents, databases, and APIs gets injected on-demand, and the design decision I keep coming back to is to let the model pull what it needs via tool calls rather than front-loading everything. The tricky part is evaluating retrieval quality, because the model's output can fail for reasons that have nothing to do with what you retrieved, and standard metrics like recall@k don't capture whether the retrieved context actually helped the model reason correctly. Part 2 goes deep on retrieval and evaluation.</p>

          </div>

<div class="code-block" data-filename="tools.py" data-lang="python">
<pre><code>@tool
async def search_knowledge_base(query, document_type=None, tags=None, condition_type=None):
    state = await session_store.get(session_id)

    # Auto-apply condition filter from patient profile
    if not condition_type and state.patient_profile.diagnosis:
        condition_type = state.patient_profile.diagnosis

    filters = RetrievalFilters(document_type=document_type, tags=tags, condition_type=condition_type)
    response = await retriever.retrieve(query=query, filters=filters, state=state)</code></pre>
</div>

          <div class="deep-dive-only">
          <p>The agent decides when to search (pull, not push), and the tool enriches the query behind the scenes. The condition filter means a Type 2 patient never sees Type 1 insulin pump troubleshooting guides, without the patient or the model needing to specify that constraint explicitly.</p>

          <p><strong>Tool definitions</strong> are the sneaky budget item. Function signatures the system can invoke consume context tokens whether they're used or not, and there's a genuine tension in how to manage them. <a href="#ref-inngest">Inngest</a> recommends removing any tool used less than 10% of the time because performance degrades as tool count grows. But <a href="#ref-manus">Manus</a> found that dynamically loading and removing tools breaks KV-cache (a 10x cost difference between cached and uncached tokens), so their solution is logits masking, where tools stay in context but get suppressed at the decoding level through a state machine.</p>

          <p>I don't think there's a clean answer here yet. I'm currently leaning toward keeping tool counts low rather than worrying about cache-aware masking, mostly because my agents have 4-6 tools, not 40. If you're at Manus's scale with dozens of tools, the cache math probably dominates.</p>

          <p>The broader principle from <a href="#ref-manus">Manus's engineering blog</a> is worth internalizing regardless of tool count, and it's straightforward: never reorder or mutate tokens already in the KV-cache prefix. Treat your prompt prefix as append-only. Even changing JSON key ordering in a tool definition invalidates the cache from that point forward, and with agentic workloads running at roughly a 100:1 input-to-output token ratio, cache hits are existential for cost at scale. Both Anthropic and OpenAI now offer explicit cache control with up to 90% discounts on cached input tokens, but the gotchas will bite you. JSON key ordering instability in some languages (Swift, Go) breaks caches silently, toggling features like web search invalidates the system cache, and the cache follows a strict hierarchy (<code>tools</code> → <code>system</code> → <code>messages</code>) where changes at any level invalidate everything after it.</p>

          <p><strong>Structured output</strong> specifications (JSON schemas, type definitions, output constraints) are easy to overlook in a token budget because they feel like "free" structure. They're not. At scale they add up, and I've seen schemas that consume 300+ tokens before the model even starts generating.</p>
          </div>

          <div class="algorithm-box deep-dive-only" style="border:none; background:#ece6d6;">
            <h4>Typical Token Allocation</h4>
            <div style="background:#f9f7f3; border-radius:6px; overflow:hidden;">
            <table style="margin:0; border-radius:6px;">
              <thead>
                <tr><th>Component</th><th>Tokens</th><th>Behavior</th></tr>
              </thead>
              <tbody>
                <tr><td>System instructions</td><td>500-2,000</td><td>Static per session</td></tr>
                <tr><td>Conversation history</td><td>500-5,000</td><td>Grows, needs trimming</td></tr>
                <tr><td>RAG results</td><td>0-10,000</td><td>On-demand, per tool call</td></tr>
                <tr><td>Tool definitions</td><td>500-3,000</td><td>Permanent, always present</td></tr>
                <tr><td>State / structured output</td><td>50-500</td><td>Dynamic per turn</td></tr>
              </tbody>
            </table>
            </div>
          </div>

          <div class="theory-only">
            <div class="callout">
              <p><a href="#ref-mei">Mei et al.'s survey</a> also formalizes optimal retrieval as an information-theoretic problem. The best retrieval function maximizes mutual information between the target answer and retrieved knowledge, conditioned on the query:</p>
              <p>$$\text{Retrieve}^* = \arg\max_{\text{Retrieve}} \; I(Y^*;\; c_\text{know} \mid c_\text{query}).$$</p>
              <p>And context assembly can be framed as Bayesian posterior inference, combining the likelihood of the query given a context with the prior over contexts given interaction history:</p>
              <p>$$P(C \mid c_\text{query}, \ldots) \propto P(c_\text{query} \mid C) \cdot P(C \mid \text{History}, \text{World}).$$</p>
              <p>Again, more math than most will ever need. But I found the mutual information framing helpful for one thing in particular. When two retrieval approaches return different documents, the one that tells the model more *new* information (higher mutual information with the answer, conditional on the query) is the better one. This provides some intuition as to why hybrid retrieval beats pure semantic search.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 3: Write / Select / Compress / Isolate               -->
        <!-- ============================================================ -->
        <section id="four-strategies">
          <h2>Write / Select / Compress / Isolate</h2>

          <div class="tldr-only">
            <ul class="list-tight">
              <li><strong>Write:</strong> move information out of the window into external storage</li>
              <li><strong>Select:</strong> retrieve relevant information back via RAG or memory queries</li>
              <li><strong>Compress:</strong> tiered approach (raw context → compact by stripping filler → summarize only when needed), compress proactively at ~75% capacity, trim in atomic turn groups. Never compress instructions and data together</li>
              <li><strong>Isolate:</strong> split work across separate LLM calls so each gets focused context</li>
            </ul>
            <p>Most teams over-invest in writing and selecting; the real gains are in compression and isolation.</p>
          </div>

          <div class="deep-dive-only">
          <p>Every production context system I've seen uses some combination of four strategies, and <a href="#ref-langchain">LangChain's framework</a> gives them clean names.</p>

          <p><strong>Writing</strong> means getting information out of the context window and into external storage for later retrieval. Scratchpads let the agent write intermediate notes during a session (observations, partial results, plans) that persist via tool calls or state objects without occupying the window continuously. Memories go further, enabling cross-session retention by extracting reflections or facts and storing them in a persistent backend. The <a href="#ref-manus">Manus</a> team has a concrete version of this, their agents maintain a <code>todo.md</code> file during complex tasks, writing and re-reading their plan to counteract the "lost-in-the-middle" problem across ~50 average tool calls. It's charmingly simple for a state-of-the-art agent. (Though Manus later found that roughly a third of all agent actions were spent updating the todo list, and shifted to a dedicated planner agent instead. Even clever patterns have costs.)</p>

          <p>My first version wrote a rolling summary every 5 turns and called it a day. That's fine for a prototype, but production systems benefit from a tiered approach that matches the intensity of compression to how much pressure the window is actually under (imagine Claude Code running on million-line codebase and websearching the entire Internet in parallel). I think of it as three tiers:</p>

          <ol>
            <li><strong>Raw context</strong> when below ~75% capacity. If the window has room, don't compress at all. Raw turns carry more signal than any summary.</li>
            <li><strong>Compact first</strong> when approaching the budget. Strip low-signal content (greetings, filler acknowledgments, tool call boilerplate) while keeping messages structurally intact. Low/no LLM call needed.</li>
            <li><strong>Summarize</strong> only when compaction isn't enough. Replace older turns with a generated summary, preserving the recent window verbatim.</li>
          </ol>

          <p>The 75% threshold matters more than it might seem. Let's call it the <strong>pre-rot threshold</strong>: compress proactively at ~75% capacity, not reactively at 95%. By the time you're near the wall, attention quality has already degraded across the last 20% of growth (this connects directly to the <a href="#failure-modes">distraction failure mode</a> in Section 4). Compressing early keeps the model in its high-performance zone.</p>
          </div>

<div class="code-block" data-filename="context_manager.py" data-lang="python">
<pre><code>async def manage_context(self, session_id, current_turn, window_budget):
    messages = await self._store.get_messages(session_id)
    current_usage = estimate_tokens(messages)

    # Tier 1: Raw context fits — do nothing
    if current_usage < window_budget * 0.75:
        return messages

    # Tier 2: Compact — strip low-signal content, keep messages intact
    compacted = self._compact(messages)
    if estimate_tokens(compacted) < window_budget * 0.85:
        return compacted

    # Tier 3: Summarize — replace old turns with generated summary
    cutoff = self._find_summary_boundary(compacted)
    summary = await self._summarize(compacted[:cutoff])
    return [summary_message(summary)] + compacted[cutoff:]</code></pre>
</div>

          <div class="deep-dive-only">
          <p>The <code>_compact</code> step does straightforward string surgery, stripping "thanks!", "ok sounds good", empty assistant acknowledgments, and tool call metadata that the model doesn't need to see on future turns. The summarization tier only fires when compaction alone can't get below 85% capacity, which in practice means conversations beyond ~15 turns.</p>

          <p>One finding I keep coming back to from the <a href="#ref-rcc">recurrent context compression</a> research is that compressing instructions and context simultaneously degrades responses. You need to compress the data but preserve the instructions separately (they are treated differently at an attention level). I was summarizing entire turns including the system-injected guidance in my first attempt, and the model started ignoring its own rules.</p>

          <p><strong>Selecting</strong> is the complement, retrieving relevant information back when needed. This includes reading back scratchpad notes from earlier steps, querying stored memories using embeddings or keyword search, and full RAG pipelines over documents or code. If you have many tools, you can use RAG over tool descriptions to select the right one. The <a href="#ref-rag-mcp">RAG-MCP paper</a> (Writer.com) measured a 3x improvement in selection accuracy compared to exposing all tools at once (from 13.6% to 43.1%); the agent needs a search engine just to find its own capabilities.</p>

          <p>Selection quality depends heavily on query quality. My agent rewrites the user's message into a self-contained retrieval query before searching:</p>
          </div>

<div class="code-block" data-filename="query_rewriter.py" data-lang="python">
<pre><code># "My numbers have been all over the place lately"
# -&gt; "blood sugar management strategies Type 2 patient on metformin irregular post-meal readings"

async def rewrite(self, query, conversation_history, patient_profile):
    recent_turns = conversation_history[-3:]
    prompt = f"""Rewrite the search query to be self-contained.
    Resolve pronouns, add implicit context from conversation.
    Known: diagnosis={profile.diagnosis}, medications={profile.current_medications}

    Rules:
    - Keep it concise (under 30 words)
    - Resolve pronouns ("it" -&gt; the patient's condition)
    - Strip emotional language, focus on the information need"""</code></pre>
</div>

          <div class="deep-dive-only">
          <p>The comment at the top shows why this matters. "My numbers have been all over the place lately" has almost zero retrieval value as-is because there's no condition type, no medication context, no timeframe. The rewriter infers "post-meal readings" from the last 3 turns, adds the patient's diagnosis and medication context, and produces a query that actually hits relevant documents.</p>

          <p><strong>Compression</strong> reduces tokens while maintaining task performance. Summarization replaces older conversation history with a condensed version (Claude Code applies auto-compact when approaching context limits). Trimming removes older messages, and the critical detail is how you trim. Per-message FIFO (drop the oldest message) is the naive approach, and it breaks things in subtle ways. Dropping an individual assistant message can orphan a tool result from its tool call, producing a conversation that the API rejects or the model misinterprets. Production SDKs from both <a href="#ref-openai-agents">OpenAI</a> and Anthropic trim in <strong>atomic turn groups</strong>: a user message plus all assistant messages and tool results that follow it, removed as a unit.</p>
          </div>

<div class="code-block" data-filename="trimming.py" data-lang="python">
<pre><code>def trim_oldest_turn(messages):
    """Remove the oldest complete turn group atomically."""
    if not messages:
        return messages
    i = 0
    while i < len(messages):
        if i > 0 and messages[i].role == "user":
            break
        i += 1
    return messages[i:]</code></pre>
</div>

          <div class="deep-dive-only">
          <p>The function walks forward from the start until it hits the next user message, then slices off everything before it. One complete turn (user message, assistant response, any tool calls and results in between) gets removed as an atomic unit. My agent calls this in a loop until the conversation fits within the character budget, which handles the same two problems as a message cap plus character budget but without the risk of orphaned tool results.</p>

          <p><strong>Isolation</strong> means splitting information across separate processing units so each one gets a clean, focused context window. Multi-agent systems give each sub-agent its own window focused on a specific subtask, returning a condensed summary (1,000-2,000 tokens) to the lead agent. HuggingFace's CodeAgent isolates token-heavy objects in sandbox environments, keeping only references in the main context. You can also separate LLM-exposed fields from auxiliary context storage in your state schema, because not everything the system knows needs to be in the window.</p>

          <p>My agent isolates safety classification into separate LLM calls so the guardrail context never contaminates the main patient conversation:</p>
          </div>

<div class="code-block" data-filename="graph.py" data-lang="python">
<pre><code># Each node runs in its own isolated LLM call with its own context
graph = StateGraph(CoachingState)

graph.add_node("input_gate", input_gate_node)       # Safety classifier
graph.add_node("pro_react_agent", pro_agent)         # Main agent (complex queries)
graph.add_node("flash_react_agent", flash_agent)     # Main agent (simple messages)
graph.add_node("output_gate", output_gate_node)      # Scope validator

graph.set_entry_point("input_gate")
graph.add_conditional_edges("input_gate", route_after_input_gate, {
    END: END,                                        # Blocked -&gt; stop
    "pro_react_agent": "pro_react_agent",            # Complex -&gt; Pro + thinking
    "flash_react_agent": "flash_react_agent",        # Simple -&gt; Flash
})
graph.add_edge("pro_react_agent", "output_gate")
graph.add_edge("flash_react_agent", "output_gate")
graph.add_edge("output_gate", END)</code></pre>
</div>

          <div class="deep-dive-only">
          <p>The input gate sees only the latest user message and a short classification prompt. The output gate sees only the agent's response and a scope-checking prompt. Neither gate's context (safety rules, classification examples) appears in the main agent's window, keeping the patient conversation clean and focused.</p>

          <p>If you're building an agent and something feels off about the outputs, run through these four categories and ask which one you're neglecting. In my experience the answer is almost always compression or isolation; people over-invest in writing and selecting because those feel like "building features," while trimming old history and splitting contexts feel like cleanup work.</p>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 4: How Context Fails                                 -->
        <!-- ============================================================ -->
        <section id="failure-modes">
          <h2>How Context Fails</h2>

          <div class="tldr-only">
            <p>Four context failure modes to watch for when an agent misbehaves: poisoning (errors compound), distraction (too much history), confusion (noise as signal), and clash (contradictory instructions). Different failures need different fixes.</p>
          </div>

          <div class="deep-dive-only">
          <p>When an agent misbehaves, my first question is always "what kind of context failure is this?" because the mitigations are completely different depending on the answer. The following four failure modes cover most of what goes wrong.</p>
          </div>

          <div class="algorithm-box" style="border:none; background:#ece6d6;">
            <h4>Four Context Failure Modes</h4>
            <div style="background:#f9f7f3; border-radius:6px; overflow:hidden;">
            <table style="margin:0; border-radius:6px;">
              <thead>
                <tr><th>Mode</th><th>Cause</th><th>Fix</th></tr>
              </thead>
              <tbody>
                <tr><td>Poisoning</td><td>Error enters context, compounds downstream</td><td>Validate before memory writes</td></tr>
                <tr><td>Distraction</td><td>History too long, drowns out training</td><td>Aggressive trimming + summarization</td></tr>
                <tr><td>Confusion</td><td>Noise treated as signal</td><td>Curate ruthlessly, earn every token</td></tr>
                <tr><td>Clash</td><td>Contradictory instructions in prompt</td><td>Clear precedence hierarchy</td></tr>
              </tbody>
            </table>
            </div>
          </div>

          <div class="deep-dive-only">
          <p><strong>Context poisoning</strong> is the scariest one. A hallucination or error enters the context and gets repeatedly referenced, compounding mistakes over time. Once a wrong fact lands in the conversation history, the model treats it as ground truth and builds on it. Google DeepMind's Gemini 2.5 technical report showed just how bad this gets. A Pokemon-playing agent hallucinated the existence of an item called "TEA," wrote it into its goals scratchpad, and then spent hundreds of actions trying to find something that doesn't exist in the game. The agent also developed a "black-out strategy" (intentionally fainting all its Pokemon to teleport) rather than navigating normally. Once a hallucination enters a persistent scratchpad, the model treats it as ground truth and builds increasingly nonsensical plans on top of it. The fix is to validate information before writing to long-term memory, treating memory writes like database writes where you check constraints before committing.</p>

          <p><strong>Context distraction</strong> is more subtle. The context grows so long that the model over-focuses on accumulated history and neglects what it learned during training. Beyond ~100k tokens, I've noticed agents tend toward repeating actions from history rather than synthesizing novel plans. Aggressive trimming and summarization help, along with actively removing completed or irrelevant sections. I suspect most people's context windows are 2-3x larger than they need to be.</p>

          <p>This is why the <a href="#four-strategies">pre-rot threshold from Section 3</a> matters so much. If you wait until 95% capacity to compress, attention quality has already degraded across the last 20% of growth. Compressing proactively at ~75% avoids the worst of it.</p>

          <p><strong>Context confusion</strong> is what happens when superfluous information gets treated as signal because the model can't distinguish noise from relevant information when everything is dumped in together. <a href="#ref-anthropic">Anthropic's</a> guiding principle is the right one here: "find the smallest set of high-signal tokens that maximize the likelihood of your desired outcome." Every token should "earn" its place.</p>

          <p><strong>Context clash</strong> is the one I find most often in my own code, probably because it's the easiest to create accidentally. It happens when new information conflicts with existing information already in the prompt, and contradictory instructions produce unpredictable behavior. My own agent has one I caught during an audit for this post. The system prompt says "When READING information you already have in the patient context above, use it directly, do not re-fetch with <code>get_patient_profile</code>." But <code>get_patient_profile</code> is still available as a callable tool. The instruction and the tool list contradict each other. The agent sometimes calls the tool anyway, wasting a round-trip to fetch data that's already in the prompt. The fix is straightforward (remove the tool), but the clash was easy to miss because the instruction is in the <code>&lt;tools&gt;</code> section of the prompt and the tool definition is in Python code, and I never reviewed them side by side until I went looking for exactly this kind of problem.</p>
          </div>

          <div class="theory-only">
            <div class="callout">
              <p>There's a deeper mechanism behind context distraction that <a href="#ref-anthropic">Anthropic</a> calls <strong>context rot</strong>. They attribute it to the $O(n^2)$ pairwise token relationships in self-attention, where at 100K+ tokens the model's attention budget is spread across billions of pair interactions, and critical information gets drowned out. The full story is more nuanced than that framing suggests. Factors like positional encoding degradation (especially RoPE-based encodings at positions beyond the training distribution), attention sink phenomena where early tokens absorb disproportionate weight, or training data skewing toward shorter contexts, all contribute.</p>
              <p><a href="#ref-mei">Mei et al.'s survey</a> cites the <strong>lost-in-the-middle</strong> finding (<a href="#ref-liu">Liu et al., 2023</a>), which showed that performance degrades significantly when relevant information sits in the middle of long contexts versus the beginning or end. Liu et al. found a consistent U-shaped curve across models and settings, with accuracy dropping by 20%+ in some configurations. The exact magnitude varies by model, task, and number of documents, but the core result (middle positions perform worst) has been replicated across multiple studies and model families.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 5: A Real Token Budget                               -->
        <!-- ============================================================ -->
        <section id="token-budget">
          <h2>A Real Token Budget</h2>

          <div class="tldr-only">
            <p>Token budget from the diabetes coaching agent I built with LangGraph:</p>
          </div>

          <div class="deep-dive-only">
          <p>All of the above is easier to understand with a concrete example, so let me walk through the diabetes management coaching assistant I built as a ReAct agent with LangGraph. The system prompt ranges from ~2,000 to ~4,050 tokens depending on session maturity, assembled dynamically from several blocks.</p>
          </div>

          <div style="border-radius:8px; padding:16px; margin:24px 0; background:#ece6d6;">
            <h4 style="font-family:var(--font-serif); margin:0 0 12px 0; font-size:19px; font-weight:500;">Gated Dual-Agent Pipeline</h4>
            <div style="background:#f9f7f3; border-radius:6px; padding:24px; overflow-x:auto;">
              <div style="display:flex; align-items:center; gap:0; min-width:700px; justify-content:center; flex-wrap:nowrap;">
                <div style="background:#dce8f5; border:2px solid #4a7fb5; border-radius:8px; padding:8px 12px; text-align:center; min-width:70px;">
                  <div style="font-weight:600; font-size:13px; color:#3b6a9a;">Patient</div>
                  <div style="font-size:10px; color:#6a9ac7;">message</div>
                </div>
                <div style="color:var(--border); font-size:18px; padding:0 6px;">&rarr;</div>
                <div style="background:#fce4de; border:2px solid #d4694a; border-radius:8px; padding:8px 12px; text-align:center; min-width:70px;">
                  <div style="font-weight:600; font-size:13px; color:#b8513a;">Input Gate</div>
                  <div style="font-size:10px; color:#d4694a;">safety check</div>
                </div>
                <div style="color:var(--border); font-size:18px; padding:0 6px;">&rarr;</div>
                <div style="display:flex; flex-direction:column; gap:4px;">
                  <div style="background:#d9eedc; border:2px solid #4a9e5c; border-radius:8px; padding:6px 12px; text-align:center;">
                    <div style="font-weight:600; font-size:13px; color:#3a8249;">Pro Agent</div>
                    <div style="font-size:10px; color:#5aaa6a;">complex + thinking</div>
                  </div>
                  <div style="background:#d6eded; border:2px solid #3d9494; border-radius:8px; padding:6px 12px; text-align:center;">
                    <div style="font-weight:600; font-size:13px; color:#2e7a7a;">Flash Agent</div>
                    <div style="font-size:10px; color:#4ea0a0;">simple queries</div>
                  </div>
                </div>
                <div style="color:var(--border); font-size:18px; padding:0 6px;">&rarr;</div>
                <div style="background:#fce4de; border:2px solid #d4694a; border-radius:8px; padding:8px 12px; text-align:center; min-width:70px;">
                  <div style="font-weight:600; font-size:13px; color:#b8513a;">Output Gate</div>
                  <div style="font-size:10px; color:#d4694a;">scope check</div>
                </div>
                <div style="color:var(--border); font-size:18px; padding:0 6px;">&rarr;</div>
                <div style="background:#fdf0d5; border:2px solid #c9953a; border-radius:8px; padding:8px 12px; text-align:center; min-width:70px;">
                  <div style="font-weight:600; font-size:13px; color:#a67c2e;">Response</div>
                </div>
              </div>
              <div style="display:flex; justify-content:center; margin-top:12px;">
                <div style="border:1.5px dashed #9b7cc4; border-radius:8px; padding:8px 16px; display:flex; gap:16px; font-size:11px; color:#7a5faa; background:#efe8f6;">
                  <span>Background: rolling summary</span>
                  <span style="color:#c4b3da;">|</span>
                  <span>profile updates</span>
                  <span style="color:#c4b3da;">|</span>
                  <span>outcome tracking</span>
                </div>
              </div>
              <div style="text-align:center; margin-top:8px; font-size:12px; color:var(--muted);">Each node runs in its own isolated LLM call with its own context window</div>
            </div>
          </div>

          <table>
            <thead>
              <tr><th>Component</th><th>Tokens</th><th>Type</th></tr>
            </thead>
            <tbody>
              <tr><td>Conversation state (turn count, phase, datetime)</td><td>50-100</td><td>Dynamic (every turn)</td></tr>
              <tr><td>Identity block (personality, voice)</td><td>~350</td><td>Static</td></tr>
              <tr><td>Boundaries (5 hard scope rules)</td><td>~280</td><td>Static</td></tr>
              <tr><td>Patient context (profile, summary, goals)</td><td>80-700</td><td>Dynamic (per session)</td></tr>
              <tr><td>Approach + tools + response guides</td><td>~470</td><td>Static</td></tr>
              <tr><td>Few-shot examples (6)</td><td>~550</td><td>Static</td></tr>
              <tr><td>RAG results (last 3, conditional)</td><td>0-1,500</td><td>Conditional</td></tr>
              <tr><td><strong>System prompt total</strong></td><td><strong>~2,000-4,050</strong></td><td><strong>Mixed</strong></td></tr>
              <tr><td>Conversation window (6 turns max)</td><td>500-5,000</td><td>Dynamic (rolling)</td></tr>
              <tr><td>Tool results (<code>search_knowledge_base</code>)</td><td>0-10,000</td><td>On-demand</td></tr>
            </tbody>
          </table>

          <div class="deep-dive-only">
          <p>The conversation window holds 6 turns max, char-budgeted at 120,000 characters. Messages from turns already covered by the rolling summary are excluded.</p>

          <p>Everything gets assembled in a single <code>prepare_context</code> hook that runs before every LLM call in the ReAct loop. The static sections form a stable prefix for KV-cache hits, and the volatile conversation state goes at the end:</p>
          </div>

<div class="code-block" data-filename="context.py" data-lang="python">
<pre><code>async def prepare_context(state: CoachingState):
    session_id = state.get("session_id", "default")
    session_state = await session_store.get(session_id)
    turn_count = session_state.turn_count
    cache_key = (session_id, turn_count)

    # Skip rebuild if nothing mutated since last call
    has_mutation = _has_mutating_tool(state["messages"])
    if cache_key in _prompt_cache and not has_mutation:
        system_prompt = _prompt_cache[cache_key]
    else:
        summary = await session_store.get_latest_summary(session_id)
        episodes = await session_store.get_recent_episodes(session_id, limit=5)
        tool_results = await session_store.get_recent_tool_results(session_id, limit=3)

        system_prompt = build_system_prompt(
            profile=session_state.patient_profile,
            active_strategies=session_state.active_strategies,
            goals=session_state.goals, outcomes=session_state.outcomes,
            session_summary=summary_text,
        )

        # Static prefix first (cacheable), volatile state appended last
        state_block = build_conversation_state(turn_count, phase, recent_tool_names, ...)
        system_prompt = system_prompt + "\n\n" + state_block

    # Trim conversation: drop oldest complete turn groups atomically
    conversation = [m for m in messages if not isinstance(m, SystemMessage)]
    while estimate_chars(conversation) &gt; remaining_budget and len(conversation) &gt; 2:
        conversation = trim_oldest_turn(conversation)

    return {"llm_input_messages": [SystemMessage(system_prompt)] + conversation}</code></pre>
</div>

          <div class="deep-dive-only">
          <p>This function is where all the context engineering actually happens. Load state, build the prompt from components, append the volatile state block at the end (so the static sections form a cacheable prefix), and trim the conversation using turn-group-aware removal. Every technique I described in the earlier sections (XML structure, tiered compression, atomic trimming, isolation) converges in this one function.</p>
          </div>

          <div class="algorithm-box deep-dive-only" style="border:none; background:#ece6d6;">
            <h4>Context Engineering Techniques Used</h4>
            <div style="background:#f9f7f3; border-radius:6px; overflow:hidden;">
            <table style="margin:0; border-radius:6px;">
              <thead><tr><th>Category</th><th>Technique</th><th>Implementation</th></tr></thead>
              <tbody>
                <tr><td>Structure</td><td>XML-tagged sections</td><td><code>&lt;identity&gt;</code>, <code>&lt;boundaries&gt;</code>, <code>&lt;patient-context&gt;</code>, etc.</td></tr>
                <tr><td>Compression</td><td>Tiered (raw → compact → summarize)</td><td>Pre-rot threshold at 75% capacity</td></tr>
                <tr><td>Compression</td><td>Atomic turn-group trimming</td><td>Drop oldest complete turn, never orphan tool results</td></tr>
                <tr><td>Memory</td><td>Semantic (patient profile)</td><td>Structured schema, tool-driven updates</td></tr>
                <tr><td>Memory</td><td>Episodic (outcomes)</td><td>Created on <code>track_outcome</code>, stored with emotion</td></tr>
                <tr><td>RAG</td><td>Hybrid search</td><td>Dense + sparse + RRF + cross-encoder reranking</td></tr>
                <tr><td>RAG</td><td>Query rewriting</td><td>Pronoun resolution + profile context injection</td></tr>
                <tr><td>Routing</td><td>Dual-model</td><td>Flash for simple messages, Pro with thinking for complex</td></tr>
                <tr><td>Safety</td><td>Isolated gates</td><td>Input + output classifiers in separate LLM calls</td></tr>
              </tbody>
            </table>
            </div>
          </div>

          <h3>Two Catches from My Own Audit</h3>

          <div class="tldr-only">
            <ul class="list-tight">
              <li><strong>KV-cache violation:</strong> Volatile state prepended to prompt prefix, invalidating cache every turn (10x cost). Fix: move to end.</li>
              <li><strong>Profile-in-prompt-and-tool clash:</strong> <code>get_patient_profile</code> duplicates data already in system prompt; the tool's existence signals to the model that the profile might not be in context, making it less likely to trust data it already has.</li>
            </ul>
          </div>

          <div class="deep-dive-only">
          <p>When I audited this agent against the best practices I'd just finished researching, two problems jumped out that I wouldn't have caught without specifically looking.</p>

          <p>The expensive one is a <strong>KV-cache violation</strong>. Look at the <code>prepare_context</code> code above. The volatile <code>&lt;conversation_state&gt;</code> block (which changes every turn with new turn count, new timestamp, new tool history) gets prepended to the start of the system prompt. KV-cache works by matching a prefix, so if the first N tokens are identical between calls, the provider can reuse the cached key-value pairs and charge you the cached rate. By putting volatile data at the very start, every single turn invalidates the entire cache. <a href="#ref-manus">Manus</a> reports this is a 10x cost difference (<span class="nokatex">$0.30/MTok cached vs $3/MTok uncached</span> on Claude Sonnet). To fix, just move the state block to the end of the prompt instead of the beginning, so the static sections (identity, boundaries, examples) form a stable prefix that caches across turns. The broader principle from <a href="#seven-components">Section 2</a> applies here too. Treat the prompt prefix as append-only, sort sections by volatility, and let the stable parts form a cacheable prefix.</p>
          </div>

          <div style="border-radius:8px; padding:16px; margin:24px 0; background:#ece6d6;">
            <h4 style="font-family:var(--font-serif); margin:0 0 12px 0; font-size:19px; font-weight:500;">KV-Cache Prefix Ordering</h4>
            <div style="background:#f9f7f3; border-radius:6px; padding:24px;">
              <div style="margin-bottom:16px;">
                <div style="display:flex; align-items:center; gap:8px; margin-bottom:10px;">
                  <span style="font-size:14px; font-weight:600; color:var(--vermillion);">Before</span>
                  <span style="font-size:12px; color:var(--muted);">Cache breaks on every turn</span>
                </div>
                <div style="display:flex; gap:2px; font-size:11px; border-radius:6px; overflow:hidden;">
                  <div style="background:#e8cbc0; border:1px solid #c9a99e; padding:8px 10px; min-width:100px; text-align:center; position:relative;">
                    <div style="font-weight:600;">conversation_state</div>
                    <div style="color:var(--vermillion); font-size:10px;">volatile</div>
                  </div>
                  <div style="background:#e5dccb; border:1px solid var(--border); padding:8px 10px; flex:1; text-align:center;">
                    <div style="font-weight:500;">identity</div>
                    <div style="color:var(--muted); font-size:10px;">static</div>
                  </div>
                  <div style="background:#e5dccb; border:1px solid var(--border); padding:8px 10px; flex:1; text-align:center;">
                    <div style="font-weight:500;">boundaries</div>
                    <div style="color:var(--muted); font-size:10px;">static</div>
                  </div>
                  <div style="background:#e5dccb; border:1px solid var(--border); padding:8px 10px; flex:1; text-align:center;">
                    <div style="font-weight:500;">examples</div>
                    <div style="color:var(--muted); font-size:10px;">static</div>
                  </div>
                  <div style="background:#e5dccb; border:1px solid var(--border); padding:8px 10px; flex:1; text-align:center; color:var(--muted);">...</div>
                </div>
                <div style="font-size:10px; color:var(--vermillion); margin-top:4px;">&uarr; Prefix changes every turn. 0% cache hit rate.</div>
              </div>
              <div>
                <div style="display:flex; align-items:center; gap:8px; margin-bottom:10px;">
                  <span style="font-size:14px; font-weight:600; color:#5c7a4a;">After</span>
                  <span style="font-size:12px; color:var(--muted);">Static prefix caches across turns</span>
                </div>
                <div style="display:flex; gap:2px; font-size:11px; border-radius:6px; overflow:hidden;">
                  <div style="background:#d5dfc8; border:1px solid #b3c4a0; padding:8px 10px; flex:1; text-align:center;">
                    <div style="font-weight:500;">identity</div>
                    <div style="color:#5c7a4a; font-size:10px;">cached</div>
                  </div>
                  <div style="background:#d5dfc8; border:1px solid #b3c4a0; padding:8px 10px; flex:1; text-align:center;">
                    <div style="font-weight:500;">boundaries</div>
                    <div style="color:#5c7a4a; font-size:10px;">cached</div>
                  </div>
                  <div style="background:#d5dfc8; border:1px solid #b3c4a0; padding:8px 10px; flex:1; text-align:center;">
                    <div style="font-weight:500;">examples</div>
                    <div style="color:#5c7a4a; font-size:10px;">cached</div>
                  </div>
                  <div style="background:#d5dfc8; border:1px solid #b3c4a0; padding:8px 10px; flex:1; text-align:center; color:#5c7a4a;">...</div>
                  <div style="background:#ece3c8; border:1px solid #d5c9a0; padding:8px 10px; min-width:100px; text-align:center;">
                    <div style="font-weight:600;">conversation_state</div>
                    <div style="color:#8a7a3c; font-size:10px;">volatile</div>
                  </div>
                </div>
                <div class="nokatex" style="font-size:10px; color:#5c7a4a; margin-top:4px;">&uarr; Stable prefix. $0.30/MTok cached vs $3.00/MTok uncached (10x).</div>
              </div>
            </div>
          </div>

          <div class="deep-dive-only">
          <p>The subtler one is a <strong>profile-in-prompt-and-tool clash</strong>. The patient profile is already injected into the <code>&lt;patient-context&gt;</code> section of the system prompt on every turn. But <code>get_patient_profile</code> also exists as a callable tool. The tool exists to let the agent "check what it knows," but the agent already knows; it's right there in the prompt. I mentioned this in <a href="#failure-modes">Section 4</a> as a context clash, and it is, but it's also context confusion in a way I didn't realize until I watched the agent's behavior. The tool's existence signals to the model that the profile might <em>not</em> be in context, which makes it less likely to trust the data it already has. The instruction says "use the profile in <code>&lt;patient-context&gt;</code> directly," but the tool's mere availability creates an implicit counter-signal. Removing the tool entirely fixed the redundant fetches and, more importantly, made the agent more confident in referencing profile data from the prompt.</p>
          </div>

          <div class="theory-only">
            <div class="callout">
              <p>The benchmarks suggest context engineering matters a lot more than most people realize. On GAIA, human accuracy is 92% while GPT-4 hits 15%. A 77-point gap. On GTA, GPT-4 completes fewer than 50% of tasks. On WebArena, the top agent (IBM CUGA) reaches only 61.7%. These benchmarks all require integrating information from multiple sources, using tools, and maintaining state across steps, which is exactly what context engineering addresses.</p>
              <p>Memory systems fare poorly too. LongMemEval (500 curated questions) finds 30% accuracy degradation in commercial assistants during extended interactions. GPT-4, Claude, and Llama 3.1 all struggle with episodic memory involving interconnected events, even in brief contexts. The gap between model capability on narrow benchmarks and system capability on realistic tasks is, I think, the context engineering gap.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 6: Measuring Context Quality                         -->
        <!-- ============================================================ -->
        <section id="measuring">
          <h2>Measuring Context Quality</h2>

          <div class="tldr-only">
            <p>Track cache hit rate, cost per task, and task completion rate vs. context size. Run A/B tests on context strategies, not just prompts. Benchmarks exist (ContextBench, Letta Context-Bench) but your own eval suite matters more.</p>
          </div>

          <div class="deep-dive-only">
          <p>The one metric I track reliably is <strong>cache hit rate</strong>, because Anthropic hands it to you for free. Every API response includes <code>cache_read_input_tokens</code> and <code>cache_creation_input_tokens</code>, and the ratio tells you whether your prefix ordering is stable across turns. After fixing the <a href="#token-budget">KV-cache violation from Section 5</a>, my cache hit rate went from ~0% to ~85% on turns 2+, which I could verify directly from the billing dashboard. If your cache hit rate is low, something in your prompt prefix is changing between calls, and the fix is almost always moving volatile content later in the prompt.</p>

          <p>Beyond that, I keep an eye on <strong>cost per conversation</strong> (total API spend divided by completed sessions) because it rolls up cache efficiency, context size, model routing, and retrieval volume into a single number. <a href="#ref-letta-bench">Letta's Context-Bench</a> reinforces why this matters more than per-token price: <span class="nokatex">Claude Sonnet 4.5 led their benchmark at 74.0% accuracy for $24.58, while GPT-5 reached 72.67% for $43.56</span>, almost double the cost for slightly lower performance. Models with higher unit costs sometimes use far fewer tokens, so the aggregate figure is what counts.</p>

          <p>The metrics I <em>want</em> but haven't built yet are per-component token budgets (logging how much each prompt section actually consumes versus my design-time estimates) and task completion rate bucketed by context size. The second one would tell me whether more context is actually helping or triggering the <a href="#failure-modes">distraction failure mode</a> from Section 4.</p>

          <p>On the benchmarking side, <a href="#ref-contextbench">ContextBench</a> (February 2025) is worth knowing about. It tests whether coding agents can retrieve the right context from 66 real repositories across 8 languages, and the headline numbers are sobering, as even SOTA models achieve block-level F1 below 0.45 and line-level F1 below 0.35. Higher recall was consistently favored over precision, suggesting it's better to include a few irrelevant chunks than to miss a critical one. Sophisticated scaffolding didn't necessarily lead to better retrieval either, which complicates the "just add more RAG infrastructure" instinct.</p>

          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 7: What I Learned                                    -->
        <!-- ============================================================ -->
        <section id="what-i-learned">
          <h2>Lesson</h2>

          <div class="tldr-only">
            <p>Context engineering is mostly about removal, not addition. Every improvement involved taking something out or moving it around. Context strategy isn't portable across providers; test on every model you support. Input tokens outnumber output tokens ~100:1 in production agents, so context-side optimization has outsized economic leverage, and that makes this a durable skill.</p>
          </div>

          <div class="deep-dive-only">
          <p>If I had to distill this post into one actionable idea, it's that context engineering is mostly about removal, not addition. The instinct is always to add more information, more tools, more history, more instructions. But every improvement I've made to my agent involved taking something out or moving it around, not putting more in. Remove the redundant tool. Move the volatile state block to the end of the prompt. Compress proactively at 75% capacity instead of reactively at 95%. Trim in atomic turn groups instead of shaving individual messages.</p>

          <p>This covers within-session context management for a single agent. Multi-session continuity and memory systems that improve over time rather than accumulating garbage are Part 3 territory. Production agent harnesses where the filesystem becomes the source of truth, and the context window is just a view into external state, are Part 4. If you're building a single-session agent today, the patterns here apply directly; the later parts extend them to longer time horizons and more complex architectures.</p>

          <p>One caveat worth keeping in mind as you apply any of this. Context strategy isn't portable across providers. Different models have different attention patterns, different context window behaviors, and different sensitivities to prompt structure. What works for Claude might fail on GPT might fail on Gemini. I've been burned by this enough times that I now test prompts on every model I plan to support, rather than assuming my architecture generalizes.</p>

          <h3>Why I Think This Skill Has Legs</h3>

          <p>We've spent decades aggregating essentially the entire body of human knowledge work into model weights you can query for cents. A question that once took a junior analyst half a day now costs a few pennies in API calls. But cheap isn't free, and you're still buying tokens. In production agent systems, input tokens outnumber output tokens by roughly 100:1, which means the context side of the bill dominates. I showed earlier that a single ordering mistake in my prompt produced a 10x cost difference, and if you scale that kind of waste to millions of conversations, even a 1% optimization on the input side ends up mattering more than a 50% optimization on the output side, simply because there's so much more of it.</p>

          <p>Wherever there's a cost curve with room to optimize, there tends to be demand for people who can optimize it. That was true for database query planning, for network packet routing, for GPU kernel tuning, and I think it'll be true for context assembly. Models will keep getting cheaper per token, but usage is growing faster than prices are dropping, and the gap between "the model can do this task" and "the model does this task reliably at scale" is exactly where context engineering lives.</p>

          <p>If you spot errors or have war stories from your own context engineering work, I'd love to hear about it on <a href="https://x.com/HenryVu27">X</a> or <a href="https://www.linkedin.com/in/henry-vu27/">LinkedIn</a>.</p>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 8: References                                        -->
        <!-- ============================================================ -->
        <section id="references" class="deep-dive-only">
          <h2>References</h2>

          <ol>
            <li id="ref-karpathy">Karpathy, A. "Context Engineering." <a href="https://x.com/karpathy/status/1937902205765607626">X/Twitter</a>, June 2025.</li>
            <li id="ref-lutke">Lutke, T. "Context Engineering over Prompt Engineering." <a href="https://x.com/tobi/status/1935533422589399127">X/Twitter</a>, June 2025.</li>
            <li id="ref-anthropic">Rajasekaran, P. et al. "Effective Context Engineering for AI Agents." <a href="https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents">Anthropic Engineering</a>, September 2025.</li>
            <li id="ref-langchain">Martin, L. "Context Engineering for Agents." <a href="https://blog.langchain.com/context-engineering-for-agents/">LangChain Blog</a>, July 2025.</li>

            <li id="ref-manus">Ji, Y. "Context Engineering for AI Agents: Lessons from Building Manus." <a href="https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus">Manus Blog</a>, July 2025.</li>
            <li id="ref-spotify">Spotify Engineering. "Context Engineering: Background Coding Agents Part 2." <a href="https://engineering.atspotify.com/2025/11/context-engineering-background-coding-agents-part-2">engineering.atspotify.com</a>, November 2025.</li>
            <li id="ref-inngest">Inngest. "Five Critical Lessons for Context Engineering." <a href="https://www.inngest.com/blog/five-lessons-for-context-engineering">inngest.com</a>, 2025.</li>
            <li id="ref-mei">Mei, Z. et al. "A Survey of Context Engineering for Large Language Models." <a href="https://arxiv.org/abs/2507.13334">arXiv:2507.13334</a>, July 2025.</li>
            <li id="ref-willison">Willison, S. "Context Engineering." <a href="https://simonwillison.net/2025/Jun/27/context-engineering/">simonwillison.net</a>, June 2025.</li>
            <li id="ref-osmani">Osmani, A. "Context Engineering: Bringing Engineering Discipline to AI." <a href="https://addyo.substack.com/p/context-engineering-bringing-engineering">Substack</a>, 2025.</li>
            <li id="ref-fowler">Fowler, M. "Context Engineering for Coding Agents." <a href="https://martinfowler.com/articles/exploring-gen-ai/context-engineering-coding-agents.html">martinfowler.com</a>, 2025.</li>
            <li id="ref-ruler">Hsieh, C.-P. et al. "RULER: What's the Real Context Size of Your Long-Context Language Models?" <a href="https://arxiv.org/abs/2404.06654">arXiv:2404.06654</a>, 2024. Published at COLM 2024.</li>
            <li id="ref-liu">Liu, N.F. et al. "Lost in the Middle: How Language Models Use Long Contexts." <a href="https://arxiv.org/abs/2307.03172">arXiv:2307.03172</a>, 2023. Published in TACL, 2024.</li>
            <li id="ref-longmemeval">Wu, D. et al. "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory." <a href="https://arxiv.org/abs/2410.10813">arXiv:2410.10813</a>, 2024. Published at ICLR 2025.</li>
            <li id="ref-gaia">Mialon, G. et al. "GAIA: A Benchmark for General AI Assistants." <a href="https://arxiv.org/abs/2311.12983">arXiv:2311.12983</a>, 2023. Published at ICLR 2024.</li>
            <li id="ref-anthropic-harness">Anthropic. "How We Built Our Multi-Agent Research System." <a href="https://www.anthropic.com/engineering/built-multi-agent-research-system">Anthropic Engineering</a>, November 2025.</li>
            <li id="ref-openai-agents">OpenAI. "Agents SDK." <a href="https://github.com/openai/openai-agents-python">GitHub</a>, 2025.</li>
            <li id="ref-rcc">Huang, Y. et al. "Recurrent Context Compression: Efficiently Expanding the Context Window of LLM." <a href="https://arxiv.org/abs/2406.06110">arXiv:2406.06110</a>, 2024.</li>
            <li id="ref-rag-mcp">Writer.com. "RAG-MCP: Mitigating Prompt Bloat in Tool-Augmented LLMs." <a href="https://arxiv.org/abs/2505.03275">arXiv:2505.03275</a>, 2025.</li>
            <li id="ref-openai-prompting">OpenAI. "GPT-4.1 Prompting Guide." <a href="https://cookbook.openai.com/examples/gpt4-1_prompting_guide">OpenAI Cookbook</a>, 2025.</li>
            <li id="ref-letta-bench">Letta. "Context-Bench: Benchmarking Long-Horizon Agent Memory." <a href="https://www.letta.com/blog/context-bench">letta.com</a>, October 2025.</li>
            <li id="ref-contextbench">Fournier, C. et al. "ContextBench: A Benchmark for Context Retrieval in Coding Agents." <a href="https://arxiv.org/abs/2602.05892">arXiv:2602.05892</a>, February 2025.</li>
          </ol>
        </section>

      </article>
    </div>

    <footer class="footer">
      <a href="https://x.com/HenryVu27">X</a> · <a href="https://www.linkedin.com/in/henry-vu27/">LinkedIn</a> · <a href="#">GitHub</a>
    </footer>
  </main>

  <script src="../../js/katex-init.js"></script>
  <script src="../../js/mode-toggle.js"></script>
  <!-- Prism.js: syntax highlighting -->
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
  <script src="../../js/code-blocks.js"></script>
</body>
</html>
