<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Context Engineering Fundamentals · Technical Blog</title>
  <link rel="stylesheet" href="../../css/styles.css">
  <link rel="stylesheet" href="../../css/post-styles.css">
  <link rel="stylesheet" href="../../css/deep-dive-layout.css">
  <link rel="stylesheet" href="../../css/components.css">
  <link rel="stylesheet" href="../../css/mode-toggle.css">
  <script>try{if(localStorage.getItem('blog-reading-mode')==='deep-dive')document.documentElement.classList.add('deep-dive-mode')}catch(e){}</script>
  <!-- KaTeX for Deep Dive math -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
</head>
<body>
  <main class="page">
    <div class="page-header">
      <nav class="nav">
        <a href="../../index.html" class="back-btn">← Back</a>
      </nav>

      <header>
        <h1 class="title">Context Engineering Fundamentals</h1>
        <div class="meta">
          2026-02-23 · 20 min read
          <span class="mode-toggle">
            <button data-mode="builder" class="active">Builder</button>
            <button data-mode="deep-dive">Deep Dive</button>
          </span>
        </div>
        <hr>
      </header>

      <div class="series-nav">
        <h3>AI Engineering Series</h3>
        <ul>
          <li><strong>Part 1: Context Engineering Fundamentals</strong> (current)</li>
          <li>Part 2: RAG, From Naive to Agentic (coming soon)</li>
          <li>Part 3: Memory Engineering (coming soon)</li>
          <li>Part 4: Building AI Agents (coming soon)</li>
          <li>Part 5: Multi-Agent Systems (coming soon)</li>
        </ul>
      </div>
    </div>

    <div class="content-divider"></div>

    <div class="layout">
      <aside class="toc">
        <h2>Contents</h2>
        <ul>
          <li><a href="#from-prompts-to-context"><span class="toc-section-number">1</span><span class="toc-section-title">From Prompts to Context</span></a></li>
          <li><a href="#seven-components"><span class="toc-section-number">2</span><span class="toc-section-title">The Seven Components of Context</span></a></li>
          <li><a href="#four-strategies"><span class="toc-section-number">3</span><span class="toc-section-title">Write / Select / Compress / Isolate</span></a></li>
          <li><a href="#failure-modes"><span class="toc-section-number">4</span><span class="toc-section-title">How Context Fails</span></a></li>
          <li><a href="#token-budget"><span class="toc-section-number">5</span><span class="toc-section-title">The Token Budget in Practice</span></a></li>
          <li><a href="#best-practices"><span class="toc-section-number">6</span><span class="toc-section-title">Best Practices</span></a></li>
          <li><a href="#references"><span class="toc-section-number">7</span><span class="toc-section-title">References</span></a></li>
        </ul>
      </aside>

      <article class="post">

        <!-- ============================================================ -->
        <!-- SECTION 1: From Prompts to Context                           -->
        <!-- ============================================================ -->
        <section id="from-prompts-to-context">
          <h2>From Prompts to Context</h2>

          <p>In mid-2025, the AI engineering community quietly retired "prompt engineering" as the label for what practitioners actually do when building LLM systems. The replacement, <strong>context engineering</strong>, stuck because it describes the real work: deciding what information fills the context window, not how to phrase a single instruction.</p>

          <p>Andrej Karpathy put it directly: context engineering is "the delicate art and science of filling the context window with just the right information for the next step." Philipp Schmid defined it as "designing and building dynamic systems that provide the right information and tools, in the right format, at the right time." Both definitions point at the same thing. This is systems design, not copywriting.</p>

          <p>The distinction matters because the failure mode changed. In 2023, most LLM failures were model failures: the model couldn't reason well enough, or hallucinated, or refused. By 2025, most agent failures are <strong>context failures</strong>: the model had the capability but didn't have the right information at the right time. Schmid claims that proper context engineering can push success rates from ~30% to over 90%.</p>

          <p>The two disciplines differ in important ways:</p>

          <table>
            <thead>
              <tr>
                <th>Dimension</th>
                <th>Prompt Engineering</th>
                <th>Context Engineering</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>Focus</strong></td>
                <td>How you phrase the instruction</td>
                <td>What information fills the window</td>
              </tr>
              <tr>
                <td><strong>Scope</strong></td>
                <td>Single text string / template</td>
                <td>Entire information architecture</td>
              </tr>
              <tr>
                <td><strong>Nature</strong></td>
                <td>Static, creative writing</td>
                <td>Dynamic, systems design</td>
              </tr>
              <tr>
                <td><strong>Core question</strong></td>
                <td>"How should I phrase this?"</td>
                <td>"What does the model need access to right now?"</td>
              </tr>
              <tr>
                <td><strong>Skills required</strong></td>
                <td>Writing, creativity</td>
                <td>Systems design, data engineering</td>
              </tr>
              <tr>
                <td><strong>Dominates when</strong></td>
                <td>Self-contained tasks (summarization, classification)</td>
                <td>Multi-step, multi-turn, agentic systems</td>
              </tr>
            </tbody>
          </table>

          <p>Context engineering is a <strong>superset</strong> of prompt engineering. The prompt (system instruction) is one of seven components that fill the context window. For a simple classification task, prompt engineering is enough. For anything with retrieval, tools, multi-step reasoning, or agent workflows, you need context engineering.</p>

          <p>This post covers what those seven components are, the strategies for managing them, how context fails, and what a real token budget looks like in production.</p>

          <div class="theory-only">
            <div class="callout">
              <p><span class="mode-label">Deep Dive</span></p>
              <p>The first rigorous mathematical treatment of context engineering appeared in Mei et al.'s 166-page survey (arXiv:2507.13334, 1,411 papers analyzed). They formalize context as a structured assembly:</p>
              <p>$$C = A(c_\text{instr},\; c_\text{know},\; c_\text{tools},\; c_\text{mem},\; c_\text{state},\; c_\text{query})$$</p>
              <p>where $A$ is an assembly function orchestrating six typed components: instructions, external knowledge, tool definitions, persistent memory, dynamic state, and the immediate query. Context engineering then becomes an optimization problem:</p>
              <p>$$F^* = \arg\max_F \; \mathbb{E}_{\tau \sim \mathcal{T}} \left[\text{Reward}\!\left(P_\theta(Y \mid C_{F}(\tau)),\; Y^*_\tau\right)\right] \quad \text{s.t.} \;\; |C| \leq L_\text{max}$$</p>
              <p>Find the assembly function $F$ that maximizes expected reward across tasks $\tau$, subject to a hard context length constraint $L_\text{max}$. This framing connects context engineering to information theory, Bayesian inference, and constrained optimization. It isn't just "craft."</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 2: The Seven Components of Context                   -->
        <!-- ============================================================ -->
        <section id="seven-components">
          <h2>The Seven Components of Context</h2>

          <p>Every LLM call consumes a context window: a fixed-size buffer of tokens that contains everything the model can see. What you put in that buffer determines what the model can do. Based on Philipp Schmid's framework (corroborated by Anthropic, LangChain, and Google), context decomposes into seven components.</p>

          <h3>1. Instructions / System Prompt</h3>

          <p>Behavioral guidelines, role definitions, rules, and personality. This is the "DNA" of the agent. It shapes every response.</p>

          <p>Anthropic recommends finding the "right altitude": balancing specificity with flexibility. Start minimal, then iteratively add instructions based on observed failure modes. Organize into distinct sections using XML tags or Markdown headers.</p>

          <p>A counterintuitive finding from Spotify: larger, static, version-controlled prompts proved more predictable than dynamic tool-based approaches. Treat your system prompt as code: versioned, reviewed, tested.</p>

          <h3>2. User Prompt</h3>

          <p>The immediate task, question, or message from the human. The one component you don't control.</p>

          <h3>3. State / Short-term History</h3>

          <p>Current conversation turns and prior exchanges. The "working memory" of the system.</p>

          <p>A critical finding from FlowHunt: <strong>a focused 300-token context often outperforms an unfocused 113,000-token context.</strong> What you remove can matter as much as what you keep. Don't send raw histories verbatim. Greetings, acknowledgments, and off-topic content waste tokens.</p>

          <h3>4. Long-term Memory</h3>

          <p>Persistent knowledge across conversations: preferences, summaries, facts, learned patterns. This is complex enough that Part 3 of this series covers it in depth.</p>

          <h3>5. Retrieved Information (RAG)</h3>

          <p>External knowledge from documents, databases, and APIs, injected on-demand. Part 2 covers retrieval systems in detail.</p>

          <p>The key design decision: <strong>pull, don't push.</strong> Let the AI determine what context it needs via tool calls rather than front-loading everything. Inngest found that providing minimal foundational context and letting the agent call tools as needed produces better results than pre-loading.</p>

          <h3>6. Available Tools</h3>

          <p>Function definitions the system can invoke. These definitions themselves consume context tokens. Every tool you expose costs tokens whether it's used or not.</p>

          <p>Two production rules:</p>
          <ul class="list-tight">
            <li><strong>Remove rarely-used tools.</strong> Inngest: remove any tool used less than 10% of the time. Performance degrades as tool count grows.</li>
            <li><strong>Mask, don't remove.</strong> Manus: dynamically loading/removing tools breaks KV-cache (10x cost difference between cached and uncached tokens). Use logits masking instead. Tools stay in context but are suppressed at the decoding level.</li>
          </ul>

          <h3>7. Structured Output</h3>

          <p>Response format specifications: JSON schemas, type definitions, output constraints. These guide the model's generation but also consume tokens.</p>

          <h3>Token Budget Overview</h3>

          <p>A rough allocation for a typical agentic system:</p>

          <table>
            <thead>
              <tr>
                <th>Component</th>
                <th>Typical Tokens</th>
                <th>Nature</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>System instructions</td>
                <td>500–2,000</td>
                <td>Static per session</td>
              </tr>
              <tr>
                <td>User prompt</td>
                <td>50–500</td>
                <td>Per turn</td>
              </tr>
              <tr>
                <td>Conversation history</td>
                <td>500–5,000</td>
                <td>Growing, needs trimming</td>
              </tr>
              <tr>
                <td>Long-term memory</td>
                <td>200–1,000</td>
                <td>Injected per session</td>
              </tr>
              <tr>
                <td>RAG results</td>
                <td>0–10,000</td>
                <td>On-demand per tool call</td>
              </tr>
              <tr>
                <td>Tool definitions</td>
                <td>500–3,000</td>
                <td>Static (but maskable)</td>
              </tr>
              <tr>
                <td>Output schema</td>
                <td>50–200</td>
                <td>Static per task type</td>
              </tr>
            </tbody>
          </table>

          <p>The total has to fit within the model's context window (8k–200k tokens depending on the model). The art is maximizing signal per token.</p>

          <div class="theory-only">
            <div class="callout">
              <p><span class="mode-label">Deep Dive</span></p>
              <p>The survey formalizes optimal retrieval as an information-theoretic problem:</p>
              <p>$$\text{Retrieve}^* = \arg\max_{\text{Retrieve}} \; I(Y^*;\; c_\text{know} \mid c_\text{query})$$</p>
              <p>Optimal retrieval maximizes the mutual information between the target answer $Y^*$ and retrieved knowledge $c_\text{know}$, conditioned on the query. This connects RAG design to information theory. The best retrieval function is the one that reduces the most uncertainty about the answer.</p>
              <p>Context can also be framed as Bayesian posterior inference:</p>
              <p>$$P(C \mid c_\text{query}, \ldots) \propto P(c_\text{query} \mid C) \cdot P(C \mid \text{History}, \text{World})$$</p>
              <p>We're combining the likelihood of the query given a context assembly with the prior over contexts given interaction history and world state.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 3: Write / Select / Compress / Isolate               -->
        <!-- ============================================================ -->
        <section id="four-strategies">
          <h2>Write / Select / Compress / Isolate</h2>

          <p>LangChain's framework identifies four strategies for managing context. Every production system uses some combination of these.</p>

          <h3>1. Write Context</h3>

          <p>Store information <em>outside</em> the context window for later retrieval.</p>

          <ul class="list-tight">
            <li><strong>Scratchpads:</strong> The agent writes intermediate notes during a session (observations, partial results, plans). These persist via tool calls or state objects, available for later steps without occupying the context window continuously.</li>
            <li><strong>Memories:</strong> Cross-session retention. The agent generates reflections or facts from the current conversation and stores them in a persistent backend (vector DB, knowledge graph, plain files).</li>
          </ul>

          <p>The Manus team uses a concrete version of this: agents maintain a <code>todo.md</code> file during complex tasks, writing and re-reading their plan to counteract the "lost-in-the-middle" problem across ~50 average tool calls.</p>

          <h3>2. Select Context</h3>

          <p>Retrieve relevant information when needed.</p>

          <ul class="list-tight">
            <li><strong>Scratchpad access:</strong> Reading back notes from earlier steps via tool calls.</li>
            <li><strong>Memory retrieval:</strong> Querying stored memories using embeddings, keyword search, or knowledge graphs.</li>
            <li><strong>Tool selection via RAG:</strong> When you have many tools, use RAG over tool descriptions to select the right one. This improves selection accuracy 3x compared to exposing all tools simultaneously.</li>
            <li><strong>Knowledge retrieval:</strong> Full RAG pipelines combining grep, semantic search, and re-ranking over documents or code.</li>
          </ul>

          <h3>3. Compress Context</h3>

          <p>Reduce tokens while maintaining task performance.</p>

          <ul class="list-tight">
            <li><strong>Summarization:</strong> Replace older conversation history with a condensed summary. Claude Code applies auto-compact at 95% context utilization. Can be recursive (summarize summaries) or hierarchical (different compression levels for different age brackets).</li>
            <li><strong>Trimming:</strong> Remove older messages using heuristics (drop oldest N) or trained pruners. The Provence model is a learned context pruner that identifies which messages contribute least to task performance.</li>
          </ul>

          <p>A critical finding from recurrent context compression research: <strong>compressing instructions and context simultaneously degrades responses.</strong> Compress the data, but preserve the instructions separately.</p>

          <h3>4. Isolate Context</h3>

          <p>Split information across separate processing units.</p>

          <ul class="list-tight">
            <li><strong>Multi-agent systems:</strong> Each sub-agent maintains its own isolated context window, focused on a specific subtask. Returns a condensed summary (1,000–2,000 tokens) to the lead agent.</li>
            <li><strong>Sandboxing:</strong> HuggingFace's CodeAgent isolates token-heavy objects (large code files, dataframes) in sandbox environments, keeping only references in the main context.</li>
            <li><strong>State schema design:</strong> Separate LLM-exposed fields from auxiliary context storage. Not everything the system knows needs to be in the window.</li>
          </ul>

          <p>These strategies map to common scenarios like this:</p>

          <table>
            <thead>
              <tr>
                <th>Strategy</th>
                <th>Technique</th>
                <th>When to Use</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>Write</strong></td>
                <td>Scratchpads, memory writes</td>
                <td>Long tasks, cross-session needs</td>
              </tr>
              <tr>
                <td><strong>Select</strong></td>
                <td>RAG, tool selection, memory queries</td>
                <td>Large knowledge bases, many tools</td>
              </tr>
              <tr>
                <td><strong>Compress</strong></td>
                <td>Summarization, trimming</td>
                <td>Approaching context limits</td>
              </tr>
              <tr>
                <td><strong>Isolate</strong></td>
                <td>Sub-agents, sandboxing</td>
                <td>Complex multi-step tasks</td>
              </tr>
            </tbody>
          </table>

          <p>Most production systems use all four. The question is the mix, and that depends on your task profile, model, and latency budget.</p>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 4: How Context Fails                                 -->
        <!-- ============================================================ -->
        <section id="failure-modes">
          <h2>How Context Fails</h2>

          <p>Drew Breunig identified four failure modes that explain most context-related degradation. Understanding these is essential because each has a different mitigation strategy.</p>

          <h3>1. Context Poisoning</h3>

          <p>A hallucination or error enters the context and gets repeatedly referenced, compounding mistakes over time. Once a wrong fact is in the conversation history, the model treats it as ground truth and builds on it.</p>

          <p>Google DeepMind's Gemini 2.5 technical report confirmed this: if the "goals" section of an agent's context was poisoned, agents developed completely nonsensical strategies downstream.</p>

          <p><strong>Mitigation:</strong> Validate information before writing to long-term memory. Use structured schemas with field-level validation. Treat memory writes like database writes: check constraints before committing.</p>

          <h3>2. Context Distraction</h3>

          <p>Context grows so long that the model over-focuses on accumulated history, neglecting what it learned during training. Beyond ~100k tokens, agents tend toward repeating actions from history rather than synthesizing novel plans.</p>

          <p><strong>Mitigation:</strong> Aggressive trimming. Summarization. Actively remove completed or irrelevant sections. The "less is more" principle: 300 focused tokens beat 113,000 unfocused ones.</p>

          <h3>3. Context Confusion</h3>

          <p>Superfluous information in the context is treated as signal. The model can't distinguish noise from relevant information when everything is dumped in together.</p>

          <p><strong>Mitigation:</strong> Curate ruthlessly. Every token should earn its place. Anthropic's guiding principle: "find the smallest set of high-signal tokens that maximize the likelihood of your desired outcome."</p>

          <h3>4. Context Clash</h3>

          <p>New information conflicts with existing information already in the prompt. Contradictory instructions produce unpredictable behavior.</p>

          <p><strong>Mitigation:</strong> Establish a hierarchy of instructions with clear precedence rules. Test for contradictions. When updating context, check for conflicts with existing content.</p>

          <div class="callout">
            <p><strong>Key takeaway:</strong> These four failure modes (poisoning, distraction, confusion, clash) are the context engineering equivalent of debugging categories. When an agent misbehaves, diagnose which failure mode is active before applying a fix. The mitigations are different for each.</p>
          </div>

          <div class="theory-only">
            <div class="callout">
              <p><span class="mode-label">Deep Dive</span></p>
              <p><strong>Context rot</strong> is the deeper mechanism behind distraction. Anthropic's team explains it through the transformer's attention architecture: recall accuracy diminishes as token counts increase because the number of pairwise token relationships scales as $O(n^2)$. At 100k+ tokens, the model's attention budget is spread thin across billions of pair interactions, and critical information gets drowned out.</p>
              <p>The survey quantifies this with the <strong>lost-in-the-middle</strong> finding: performance degrades by up to <strong>73%</strong> when relevant information is positioned in the middle of long contexts, compared to the beginning or end. This isn't a model bug. It's an architectural property of self-attention. The practical implication: position matters. Put the most important context at the beginning and end of the window, not the middle.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 5: The Token Budget in Practice                      -->
        <!-- ============================================================ -->
        <section id="token-budget">
          <h2>The Token Budget in Practice</h2>

          <p>Abstract principles are useful, but what does a real context budget look like? Consider a production ReAct agent (an ADHD coaching assistant built with LangGraph) and the exact token allocation the LLM receives on each turn.</p>

          <h3>What the LLM Actually Sees</h3>

          <p>The system prompt ranges from ~2,000 to ~4,050 tokens depending on session maturity. It assembles dynamically from these blocks:</p>

          <table>
            <thead>
              <tr>
                <th>Block</th>
                <th>Tokens</th>
                <th>Nature</th>
                <th>Purpose</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Conversation state</td>
                <td>50–100</td>
                <td>Every turn</td>
                <td>Turn count, phase, datetime, active topic</td>
              </tr>
              <tr>
                <td>Identity</td>
                <td>~350</td>
                <td>Static</td>
                <td>Personality, voice guidelines</td>
              </tr>
              <tr>
                <td>Boundaries</td>
                <td>~280</td>
                <td>Static</td>
                <td>5 hard scope rules + output gate awareness</td>
              </tr>
              <tr>
                <td>Family context</td>
                <td>80–700</td>
                <td>Dynamic</td>
                <td>Profile fields, session summary, goals</td>
              </tr>
              <tr>
                <td>Approach + tools + guides</td>
                <td>~470</td>
                <td>Static</td>
                <td>Behavioral instructions, response format</td>
              </tr>
              <tr>
                <td>Few-shot examples</td>
                <td>~550</td>
                <td>Static</td>
                <td>6 canonical exchanges covering key scenarios</td>
              </tr>
              <tr>
                <td>Prior search evidence</td>
                <td>0–1,500</td>
                <td>Conditional</td>
                <td>Last 3 RAG results from knowledge base</td>
              </tr>
            </tbody>
          </table>

          <p>On top of the system prompt, the conversation window holds 6 turns / 12 messages max, char-budgeted at 120,000 characters. Messages from turns already covered by the rolling summary are excluded. Tool results add 0–10,000 tokens on-demand.</p>

          <h3>Techniques Used</h3>

          <p>This system uses nearly every context engineering pattern we've discussed:</p>

          <table>
            <thead>
              <tr>
                <th>Category</th>
                <th>Technique</th>
                <th>Implementation</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>Structure</strong></td>
                <td>XML-tagged sections</td>
                <td><code>&lt;identity&gt;</code>, <code>&lt;boundaries&gt;</code>, <code>&lt;family-context&gt;</code></td>
              </tr>
              <tr>
                <td><strong>Compress</strong></td>
                <td>Rolling summary</td>
                <td>Every 5 turns via Gemini Flash. Accumulates progressively.</td>
              </tr>
              <tr>
                <td><strong>Compress</strong></td>
                <td>History trimming</td>
                <td>6-turn window + char budget. Older turns excluded.</td>
              </tr>
              <tr>
                <td><strong>Select</strong></td>
                <td>On-demand RAG</td>
                <td>Agent calls <code>search_knowledge_base</code> when needed (pull, not push)</td>
              </tr>
              <tr>
                <td><strong>Select</strong></td>
                <td>Hybrid search</td>
                <td>Dense + sparse + RRF fusion + tag boosting + cross-encoder reranking</td>
              </tr>
              <tr>
                <td><strong>Write</strong></td>
                <td>Semantic memory</td>
                <td>Family profile with structured schema (child_name, challenges, strategies)</td>
              </tr>
              <tr>
                <td><strong>Write</strong></td>
                <td>Episodic memory</td>
                <td>Outcome events stored with emotion + strategy + turn</td>
              </tr>
              <tr>
                <td><strong>Isolate</strong></td>
                <td>Guardrail isolation</td>
                <td>Input/output safety gates run in separate context, no contamination</td>
              </tr>
            </tbody>
          </table>

          <h3>What's Missing</h3>

          <p>Even a well-designed system has context engineering gaps. Analyzing this system against current research reveals several:</p>

          <table>
            <thead>
              <tr>
                <th>Gap</th>
                <th>Impact</th>
                <th>Research Recommendation</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>No procedural memory</td>
                <td>High</td>
                <td>System prompt should evolve based on what works (LangMem)</td>
              </tr>
              <tr>
                <td>No episodic→semantic consolidation</td>
                <td>High</td>
                <td>5 positive outcomes for same strategy should become one fact (Stanford, A-MEM)</td>
              </tr>
              <tr>
                <td>No importance scoring</td>
                <td>High</td>
                <td>Memories retrieved by recency only. Should weight importance + relevance (Stanford formula)</td>
              </tr>
              <tr>
                <td>No forgetting/decay</td>
                <td>High</td>
                <td>Old data never expires. Active deletion is essential (Monigatti)</td>
              </tr>
              <tr>
                <td>KV-cache not optimized</td>
                <td>Medium</td>
                <td>Volatile fields at START of prompt break cache prefix (Manus: 10x cost)</td>
              </tr>
            </tbody>
          </table>

          <p>This analysis illustrates a general point: context engineering is iterative. You build, observe failure modes, and add techniques. No system gets it right on the first pass.</p>

          <div class="theory-only">
            <div class="callout">
              <p><span class="mode-label">Deep Dive</span></p>
              <p><strong>The GAIA Gap.</strong> How much does context engineering matter? The GAIA benchmark provides the starkest number: <strong>human accuracy is 92%, GPT-4 accuracy is 15%.</strong> A 77-point gap. The model isn't the bottleneck. Context assembly is. On the GTA benchmark, GPT-4 completes fewer than 50% of tasks. On WebArena, the top agent (IBM CUGA) reaches only 61.7%.</p>
              <p>These benchmarks require integrating information from multiple sources, using tools, maintaining state across steps. Exactly the problems context engineering addresses. The gap between model capability (measured on narrow benchmarks) and system capability (measured on realistic tasks) is the context engineering gap.</p>
              <p>Commercial memory systems fare poorly too. LongMemEval (500 curated questions) finds <strong>30% accuracy degradation</strong> in commercial assistants during extended interactions. GPT-4, Claude, and Llama 3.1 all struggle with episodic memory involving interconnected events, even in brief contexts.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 6: Best Practices                                    -->
        <!-- ============================================================ -->
        <section id="best-practices">
          <h2>Best Practices</h2>

          <p>Synthesized across Anthropic, Manus, Spotify, Inngest, LangChain, and the survey literature. Ranked by practical impact.</p>

          <h3>Context Assembly</h3>

          <table>
            <thead>
              <tr>
                <th>#</th>
                <th>Practice</th>
                <th>Source</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>1</td>
                <td><strong>Start minimal, iterate on failures.</strong> Don't front-load context. Add instructions only when you observe a specific failure mode.</td>
                <td>Anthropic, Spotify</td>
              </tr>
              <tr>
                <td>2</td>
                <td><strong>Less is more.</strong> A focused 300-token context outperforms 113,000 unfocused tokens. Curate ruthlessly.</td>
                <td>FlowHunt</td>
              </tr>
              <tr>
                <td>3</td>
                <td><strong>Structure your context.</strong> Use XML tags, Markdown headers, clear delimiters. LLMs respond better to structured input.</td>
                <td>Anthropic, Google</td>
              </tr>
              <tr>
                <td>4</td>
                <td><strong>Pull, don't push.</strong> Let agents dynamically retrieve context via tools rather than pre-loading everything.</td>
                <td>Inngest, Anthropic</td>
              </tr>
              <tr>
                <td>5</td>
                <td><strong>Version control your prompts.</strong> Static, versioned prompts are more reliable than dynamic ones. Treat them as code.</td>
                <td>Spotify</td>
              </tr>
            </tbody>
          </table>

          <h3>Performance & Cost</h3>

          <table>
            <thead>
              <tr>
                <th>#</th>
                <th>Practice</th>
                <th>Source</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>6</td>
                <td><strong>Preserve cache stability.</strong> Keep prompt prefixes stable and append-only. Any mutation invalidates KV-cache. Cached tokens cost $0.30/MTok vs $3/MTok uncached (10x).</td>
                <td>Manus</td>
              </tr>
              <tr>
                <td>7</td>
                <td><strong>Minimize tools.</strong> Remove tools used less than 10% of the time. Fewer tools = fewer dimensions of unpredictability.</td>
                <td>Inngest, Spotify</td>
              </tr>
              <tr>
                <td>8</td>
                <td><strong>Compress proactively.</strong> Summarize older conversation history. Keep recent messages verbatim, compress older ones.</td>
                <td>LangChain, Claude Code</td>
              </tr>
            </tbody>
          </table>

          <h3>Agent-Specific</h3>

          <table>
            <thead>
              <tr>
                <th>#</th>
                <th>Practice</th>
                <th>Source</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>9</td>
                <td><strong>Isolate with sub-agents.</strong> Give specialized sub-agents clean, focused context windows rather than one massive shared context.</td>
                <td>Anthropic, Google ADK</td>
              </tr>
              <tr>
                <td>10</td>
                <td><strong>Keep failure traces.</strong> Don't clean up errors from context. Models learn from seeing what went wrong and reduce mistake repetition.</td>
                <td>Manus</td>
              </tr>
              <tr>
                <td>11</td>
                <td><strong>Build observability.</strong> Log every tool call with inputs, outputs, and timing. Use this data for evaluation and iteration.</td>
                <td>Inngest</td>
              </tr>
              <tr>
                <td>12</td>
                <td><strong>Match architecture to model.</strong> Different models have different strengths. Design context strategy around your model's capabilities.</td>
                <td>Inngest</td>
              </tr>
            </tbody>
          </table>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 7: References                                        -->
        <!-- ============================================================ -->
        <section id="references">
          <h2>References</h2>

          <ol>
            <li>Karpathy, A. "Context Engineering." <a href="https://x.com/karpathy/status/1937902205765607626">X/Twitter</a>, June 2025.</li>
            <li>Lutke, T. "Context Engineering over Prompt Engineering." <a href="https://x.com/tobi/status/1935533422589399127">X/Twitter</a>, June 2025.</li>
            <li>Schmid, P. "Context Engineering." <a href="https://www.philschmid.de/context-engineering">philschmid.de</a>, 2025.</li>
            <li>Rajasekaran, P. et al. "Effective Context Engineering for AI Agents." <a href="https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents">Anthropic Engineering</a>, September 2025.</li>
            <li>Martin, L. "Context Engineering for Agents." <a href="https://blog.langchain.com/context-engineering-for-agents/">LangChain Blog</a>, July 2025.</li>
            <li>Breunig, D. "How Contexts Fail and How to Fix Them." <a href="https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html">dbreunig.com</a>, June 2025.</li>
            <li>Ji, Y. "Context Engineering for AI Agents: Lessons from Building Manus." <a href="https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus">manus.im</a>, July 2025.</li>
            <li>Spotify Engineering. "Context Engineering: Background Coding Agents Part 2." <a href="https://engineering.atspotify.com/2025/11/context-engineering-background-coding-agents-part-2">engineering.atspotify.com</a>, November 2025.</li>
            <li>Inngest. "Five Critical Lessons for Context Engineering." <a href="https://www.inngest.com/blog/five-lessons-for-context-engineering">inngest.com</a>, 2025.</li>
            <li>Mei, Z. et al. "A Survey of Context Engineering for Large Language Models." <a href="https://arxiv.org/abs/2507.13334">arXiv:2507.13334</a>, July 2025.</li>
            <li>Willison, S. "Context Engineering." <a href="https://simonwillison.net/2025/Jun/27/context-engineering/">simonwillison.net</a>, June 2025.</li>
            <li>Osmani, A. "Context Engineering: Bringing Engineering Discipline to AI." <a href="https://addyo.substack.com/p/context-engineering-bringing-engineering">Substack</a>, 2025.</li>
            <li>Gartner. "Context Engineering Is In." <a href="https://www.gartner.com/en/articles/context-engineering">gartner.com</a>, July 2025.</li>
            <li>FlowHunt. "Context Engineering: The Definitive Guide." <a href="https://www.flowhunt.io/blog/context-engineering/">flowhunt.io</a>, 2025.</li>
            <li>Fowler, M. "Context Engineering for Coding Agents." <a href="https://martinfowler.com/articles/exploring-gen-ai/context-engineering-coding-agents.html">martinfowler.com</a>, 2025.</li>
          </ol>
        </section>

      </article>
    </div>

    <footer class="footer">© Henry Vu</footer>
  </main>

  <script src="../../js/katex-init.js"></script>
  <script src="../../js/mode-toggle.js"></script>
</body>
</html>
