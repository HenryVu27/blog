<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Multi-Armed Bandits Part 1: Foundations · Technical Blog</title>
  <link rel="stylesheet" href="styles.css">
  <link rel="stylesheet" href="post-styles.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <style>
    /* Safari-specific KaTeX fixes */
    .katex {
      line-height: 1 !important;
    }
    
    .katex-display {
      line-height: 1 !important;
      margin: 1em 0 !important;
    }
    
    /* Fix for sigma and other large operators in Safari */
    .katex .mop {
      vertical-align: baseline !important;
    }
    
    /* Ensure proper alignment of subscripts and superscripts */
    .katex .msubsup {
      vertical-align: baseline !important;
    }
    
    /* Override global page styles for mixed layout */
    .page{
      max-width: none;
      margin: 80px 0 64px 0;
      padding: 0;
    }
    
    .page-header {
      max-width: 800px;
      margin: 0 auto;
      padding: 0 16px;
    }
    
    @media (max-width: 768px) {
      .page {
        margin: 40px 0 32px 0;
      }
      .page-header {
        padding: 0 16px;
      }
      .layout {
        display: block;
        max-width: 800px;
        padding: 0 16px;
      }
      .toc {
        position: static;
        border-right: none;
        border-bottom: 1px solid var(--border);
        padding: 0 0 16px 0;
        margin-bottom: 16px;
      }
      .toc ul {
        display: flex;
        flex-wrap: wrap;
        gap: 12px;
      }
      .toc li a {
        font-size: 16px;
        line-height: 1.4;
        white-space: nowrap;
      }
    }
    
    .layout{
      display: grid;
      grid-template-columns: 300px minmax(720px, 1000px) 1fr;
      gap: 32px;
      max-width: 1600px;
      margin: 0 auto;
      padding: 0 16px;
      align-items: start;
    }
    
    @media(min-width: 1200px){
      .layout{
        grid-template-columns: 320px minmax(800px, 1200px) 1fr;
        max-width: 1800px;
      }
    }
    .toc{
      position: static; 
      height:fit-content;
      border: none;
      border-right: 1px solid var(--border);
      border-radius: 0;
      padding: 0 16px 0 0;
      margin: 0;
      background: transparent;
    }
    @media(min-width: 1800px){
      .toc{
        padding: 0 24px 0 0;
        margin-right: 24px;
      }
    }
    .toc ul{list-style:none; padding-left:0}
    .toc li{margin:8px 0}
    .toc li a{
      display: flex;
      align-items: baseline;
      text-decoration: none;
      color: var(--text);
      font-size: 22px;
      line-height: 1.5;
    }
    .toc li a:hover{
      text-decoration: underline;
    }
    .toc-section-number{
      color: var(--text);
      font-weight: 400;
      margin-right: 8px;
      min-width: 20px;
      font-size: 22px;
    }
    @media(min-width: 1800px){
      .toc-section-number{
        margin-right: 16px;
        min-width: 28px;
      }
    }
    .toc-section-title{
      color: var(--text);
      font-weight: 400;
    }
    
    .toc h2{
      font-size: 29px;
      margin: 0 0 16px 0;
      font-weight: 600;
    }
    .series-nav {
      background: transparent;
      border: none;
      border-radius: 0;
      padding: 0;
      margin: 24px 0 0 0;
    }
    .series-nav h3 {
      margin: 0 0 12px 0;
      font-size: 22px;
    }
    .series-nav ul {
      margin: 0;
      padding-left: 20px;
    }
    .algorithm-box {
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 16px;
      margin: 16px 0;
      background: var(--code-bg);
    }
    .algorithm-box h4 {
      margin: 0 0 12px 0;
      font-size: 19px;
      font-weight: 700;
    }
    .edit-green { background: #e6ffed; border-radius: 3px; padding: 0 2px; }
    
    .content-divider {
      height: 1px;
      background-color: var(--border);
      /* Robust full-bleed divider */
      box-shadow: 0 0 0 100vmax var(--border);
      clip-path: inset(0 -100vmax);
      margin: 64px 0; /* equal spacing above and below */
    }

    /* Make the header's hr full-bleed as well */
    .page-header hr{
      border: 0;
      border-top: 1px solid var(--border);
      /* Full-bleed */
      box-shadow: 0 0 0 100vmax var(--border);
      clip-path: inset(0 -100vmax);
      margin: 32px 0; /* keeps header spacing tidy */
    }
  </style>
</head>
<body>
  <main class="page">
    <div class="page-header">
      <nav class="nav">
        <a href="index.html" class="back-btn">← Back</a>
      </nav>

      <header>
        <h1 class="title">Multi-Armed Bandits: Foundations</h1>
        <div class="meta">2025-09-17</div>
        <hr>
      </header>

      <div class="series-nav">
        <h3>Multi-Armed Bandit Series</h3>
        <ul>
          <li><strong>Part 1: Foundations</strong> (current)</li>
          <li><a href="bandits-part2.html">Part 2: Advanced Algorithms</a></li>
          <li><a href="bandits-part3.html">Part 3: Modern Theory</a></li>
        </ul>
      </div>
    </div>

    <div class="content-divider"></div>

    <div class="layout">
      <aside class="toc">
        <h2>Contents</h2>
        <ul>
          <li><a href="#intro"><span class="toc-section-number">1</span><span class="toc-section-title">Introduction</span></a></li>
          <li><a href="#classic"><span class="toc-section-number">2</span><span class="toc-section-title">A Classic Dilemma</span></a></li>
          <li><a href="#warmup"><span class="toc-section-number">3</span><span class="toc-section-title">Finite-Armed Stochastic Bandits</span></a></li>
          <li><a href="#etc"><span class="toc-section-number">4</span><span class="toc-section-title">Explore-then-Commit</span></a></li>
          <li><a href="#ucb"><span class="toc-section-number">5</span><span class="toc-section-title">Upper Confidence Bound</span></a></li>
          <li><a href="#optimality"><span class="toc-section-number">6</span><span class="toc-section-title">Optimality and Information Theory</span></a></li>
          <li><a href="#minimax"><span class="toc-section-number">7</span><span class="toc-section-title">Minimax Lower Bounds</span></a></li>
          <li><a href="#instance"><span class="toc-section-number">8</span><span class="toc-section-title">Instance-Dependent Bounds</span></a></li>
          <li><a href="#conclusion"><span class="toc-section-number">9</span><span class="toc-section-title">Looking Ahead</span></a></li>
        </ul>
      </aside>

      <article class="post">
        <section id="intro">
          <h2>Multi-Armed Bandits and the Exploration–Exploitation Trade-off</h2>

          <p>If you have been around modern LLMs, you have probably heard of reinforcement learning. At its core, RL is an interaction loop: a learner chooses an action, receives feedback from the environment (reward), updates its rule for acting next time, and repeats. Much of today’s LLM finetuning to align with human preferences can be seen through this lens.</p>

          <p>Unlike representation learning where the learner tries to "imitate" the data, RL forces the learner to interact with the environment. The central question in this interaction is how to balance <i>exploration</i> and <i>exploitation</i>. Should the learner take advantage of current knowledge and exploit the option that currently looks best to maximize immediate reward? Or should it take some risks to explore, potentially sacrificing short-term gains to improve knowledge and long-term performance? Too much greed misses better options; too much exploration wastes opportunities.</p>

          <p>This fundamental tension between exploration and exploitation appears everywhere in sequential decision-making. Multi-armed bandits capture this trade-off in its simplest form: a single-state Markov decision process focused only on action selection under uncertainty. We now see applications of bandits across industries where information arrives in sequential manner. Examples range from ad placement, recommender systems, packet routing, to more theoretical settings like convex optimization and Brownian motion.</p>

          <p>But what are bandit problems exactly?</p>

          <p>A key early motivation came from clinical trials. <strong>William R. Thompson</strong>, in his 1933 paper, proposed assigning treatments adaptively using accumulating evidence, favoring promising options while still testing alternatives enough to learn.</p>

          <p>The name “multi-armed bandit” came later. In the 1950s, Mosteller and Bush studied learning in animals and humans. Mice faced left/right choices in a T-shaped maze with uncertain rewards. For humans, they built a two-armed machine: each lever paid out randomly with unknown probabilities. The setup echoed the “one-armed bandit” slot machine, and the term “two-armed,” then “multi-armed,” bandit stuck.</p>
        </section>

        <section id="classic">
          <h2>A Classic Dilemma</h2>

          <figure>
            <img src="MAB.jpg" alt="Choosing among uncertain options in a multi-armed bandit" style="max-width:100%; height:auto;" />
          </figure>

          <p>Imagine you're playing a three-armed bandit machine. You've already pulled each lever several times and observed these payoffs (in dollars):</p>

          <table style="border-collapse: collapse; margin: 16px 0;">
            <tr>
              <td style="border: 1px solid #ccc; padding: 8px; font-weight: bold;">Round</td>
              <td style="border: 1px solid #ccc; padding: 8px;">1</td>
              <td style="border: 1px solid #ccc; padding: 8px;">2</td>
              <td style="border: 1px solid #ccc; padding: 8px;">3</td>
              <td style="border: 1px solid #ccc; padding: 8px;">4</td>
              <td style="border: 1px solid #ccc; padding: 8px;">5</td>
              <td style="border: 1px solid #ccc; padding: 8px;">6</td>
            </tr>
            <tr>
              <td style="border: 1px solid #ccc; padding: 8px; font-weight: bold;">Arm 1</td>
              <td style="border: 1px solid #ccc; padding: 8px;">$5</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">$5</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">$5</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
            </tr>
            <tr>
              <td style="border: 1px solid #ccc; padding: 8px; font-weight: bold;">Arm 2</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">$0</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">$0</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
            </tr>
            <tr>
              <td style="border: 1px solid #ccc; padding: 8px; font-weight: bold;">Arm 3</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">$15</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">$0</td>
            </tr>
          </table>

          <p>So far, Arm 1 averages <span>$5</span>, Arm 2 averages <span>$0</span>, and Arm 3 averages <span>$7.50</span>. You have 10 more pulls remaining. Do you stick with Arm 1 (reliable <span>$5</span>), gamble on Arm 3 (high variance but higher average), or give Arm 2 another chance? How do you balance exploiting what seems best versus exploring to learn more?</p>

          <p>This illustrates the core dilemma in bandit problems: balancing <strong>exploration</strong> (trying uncertain options to gather information) and <strong>exploitation</strong> (using the currently best-performing option to maximize immediate reward).</p>
        </section>

        <section id="warmup">
          <h2>Finite-Armed Stochastic Bandits: Warming Up</h2>

          <p>Bandit theory can quickly become intimidating as you could dive arbitrarily deep into measure theory, σ-algebras, or martingales. But that isn't necessary to grasp the core ideas. Instead, let's build intuition with the simplest meaningful setting: finite-armed stochastic bandits. This framework captures the essence of exploration versus exploitation while remaining concrete enough to analyze common solution strategies.</p>

          <p>The problem statement is as follows: we have $K$ arms, where pulling arm $i$ at time $t$ yields reward $X_{i,t}$ drawn from an unknown distribution $\nu_i$ with mean $\mu_i$. The learner's goal is to minimize <em>regret</em>, which is the difference between rewards achieved and those from always using the optimal arm.</p>

          <p>Why not evaluate a learner by raw cumulative reward $\sum_{t=1}^n X_t$?</p>
          <ul>
            <li>It is random: comparing policies requires a utility for the distribution of $S_n = \sum_{t=1}^n X_t$.</li>
            <li>The instance is unknown: a policy that maximizes $\mathbb{E}[S_n]$ for one set of rewards may perform poorly on another.</li>
          </ul>
          <p><em>Regret</em> avoids both by comparing the learner to an oracle that always pulls the best arm.</p>

          <p>Define the optimal arm as $i^* = \arg\max_{i \in [K]} \mu_i$ with optimal mean $\mu^* = \mu_{i^*}$. The gap for arm $i$ is $\Delta_i = \mu^* - \mu_i$. After $n$ rounds, the cumulative regret is:</p>

          <p>$$R_n = n\mu^* - \mathbb{E}\!\left[\sum_{t=1}^n X_{I_t, t}\right]$$</p>

          <p>where $I_t$ is the arm selected at time $t$. Minimizing regret is equivalent to maximizing expected reward, but regret normalizes performance relative to the (unknown) optimum. If rewards were known, always pulling $i^*$ would give $R_n=0$.</p>
          <div class="algorithm-box">
            <h4>Regret decomposition</h4>
            <p>Let $\Delta_i = \mu^* - \mu_i$ and let $N_i(n) = \sum_{t=1}^n \mathbb{1}_{\{I_t = i\}}$ be the (random) number of pulls of arm $i$ by round $n$. Then</p>
            <p>$$R_n = \sum_{i=1}^K \Delta_i\, \mathbb{E}[N_i(n)].$$</p>
            <p>By linearity, $\mathbb{E}[\sum_{t=1}^n X_{I_t,t}] = \sum_i \mu_i\, \mathbb{E}[N_i(n)]$.</p>
            <p>Since $R_n = n\mu^* - \mathbb{E}[\sum_{t=1}^n X_{I_t,t}]$, we get $R_n = \sum_i (\mu^* - \mu_i)\, \mathbb{E}[N_i(n)] = \sum_i \Delta_i\, \mathbb{E}[N_i(n)]$.</p>
          </div>
          <p>This identity exposes the exploration–exploitation trade-off: each pull of a suboptimal arm incurs $\Delta_i$ expected loss. Good algorithms keep $\mathbb{E}[N_i(n)]$ small for large gaps while still exploring when gaps are small, targeting sublinear regret $R_n = o(n)$. The decomposition also provides a powerful framework for algorithm analysis. Instead of directly bounding the complex random quantity $R_n$, we can analyze each $\mathbb{E}[N_i(n)]$ separately.</p>

          <p>Now let's formalize the problem setup.</p>

          <div class="algorithm-box">
            <h4>Problem Setup</h4>
            <ul>
              <li>$K$ arms with unknown reward distributions $\nu_1, \ldots, \nu_K$</li>
              <li>At round $t$, learner selects arm $I_t$ and receives reward $X_{I_t,t} \sim \nu_{I_t}$</li>
              <li>Arm $i$ has mean reward $\mu_i = \mathbb{E}[\nu_i]$</li>
              <li>Assumption: rewards are 1-sub-Gaussian (e.g., bounded in [0,1])</li>
              <li>Optimal arm: $i^* = \arg\max_i \mu_i$, gap: $\Delta_i = \mu^* - \mu_i$</li>
              <li>Goal: Minimize regret $R_n = n\mu^* - \mathbb{E}[\sum_{t=1}^n X_{I_t,t}]$</li>
            </ul>
          </div>
        </section>

        <section id="etc">
          <h2>First Steps: Explore-then-Commit</h2>

          <p>The simplest principled approach is Explore-then-Commit (ETC): pull each arm $m$ times, then select the empirically best arm for all remaining rounds. This two-phase strategy cleanly separates exploration (gathering information) from exploitation (maximizing reward).</p>

          <div class="algorithm-box">
            <h4>Explore-then-Commit Algorithm</h4>
            <p><strong>Parameters:</strong> Exploration budget $m$</p>
            <ol>
              <li><strong>Exploration:</strong> Pull each arm $m$ times</li>
              <li><strong>Estimation:</strong> Compute empirical means $\hat{\mu}_i = \frac{1}{m}\sum_{j=1}^m X_{i,j}$</li>
              <li><strong>Commitment:</strong> Select best arm $\hat{i} = \arg\max_i \hat{\mu}_i$ and pull it for remaining $n - K m$ rounds</li>
            </ol>
          </div>

          <p>To analyze ETC, we bound the probability of selecting a suboptimal arm. Using Hoeffding's inequality, for any $\delta > 0$:</p>

          <p>$$\mathbb{P}[|\hat{\mu}_i - \mu_i| \geq \delta] \leq 2\exp(-2m\delta^2)$$</p>

          <p>The algorithm fails when the empirically best arm differs from the truly best. This occurs when either:</p>
          <ol>
            <li>The optimal arm's empirical mean is underestimated: $\hat{\mu}_{i^*} < \mu^* - \epsilon$</li>
            <li>Some suboptimal arm's empirical mean is overestimated: $\hat{\mu}_i > \mu_i + \epsilon$</li>
          </ol>

          <p>Setting $\epsilon = \Delta_i/2$ and applying union bound gives the regret bound:</p>

          <p>$$R_n \leq m \sum_{i=1}^k \Delta_i + (n - mk) \sum_{i=1}^k \Delta_i \exp\left(-\frac{m\Delta_i^2}{4}\right)$$</p>

          <p>The optimal choice is $m = \max\left\{1, \left\lceil \frac{4}{\Delta^2} \log\left(\frac{n\Delta^2}{4}\right) \right\rceil \right\}$ for gap $\Delta$. For the worst-case over gaps, this gives:</p>

          <p>$$R_n \leq \Delta + C\sqrt{n}$$</p>

          <p>where $C > 0$ is a universal constant. When $\Delta \leq 1$, we get $R_n \leq 1 + C\sqrt{n}$.</p>
          <p>This bound is called <em>worst-case</em>, <em>problem-free</em>, or <em>problem-independent</em> because it only depends on the horizon $n$ and number of arms $k$, not the specific instance. The reason is that the bound only depends on the worst possible gap configuration. In contrast, bounds that depend on specific gaps like $\Delta_i$ are called <em>gap/problem/distribution/instance dependent</em>.</p>

          <pre><code>import numpy as np
import matplotlib.pyplot as plt

def explore_then_commit(rewards, m, T):
    """
    Explore-then-Commit algorithm

    Args:
        rewards: K x T matrix of rewards
        m: exploration budget per arm
        T: time horizon

    Returns:
        cumulative_regret: array of regret at each time step
    """
    K = rewards.shape[0]

    # Exploration phase
    empirical_means = np.mean(rewards[:, :m], axis=1)
    best_arm = np.argmax(empirical_means)

    # Track regret
    optimal_rewards = np.max(np.mean(rewards, axis=1))
    cumulative_regret = np.zeros(T)

    # Exploration regret
    for t in range(K * m):
        arm = t % K
        reward = rewards[arm, t // K]
        regret = optimal_rewards - reward
        cumulative_regret[t] = cumulative_regret[t-1] + regret if t > 0 else regret

    # Exploitation regret
    for t in range(K * m, T):
        reward = rewards[best_arm, t - K * m]
        regret = optimal_rewards - reward
        cumulative_regret[t] = cumulative_regret[t-1] + regret

    return cumulative_regret</code></pre>
        </section>

        <section id="ucb">
          <h2>The Upper Confidence Bound (UCB) Algorithm</h2>

          <p>UCB elegantly addresses ETC's limitations by adaptively balancing exploration and exploitation. Instead of rigid phases, UCB constructs confidence intervals around arm means and selects the arm with highest upper confidence bound - embodying the principle of optimism under uncertainty.</p>

          <p>At time $t$, having selected arm $i$ a total of $N_i(t-1)$ times and observed empirical mean $\hat{\mu}_i(t-1)$, UCB selects:</p>

          <p>$$I_t = \arg\max_{i \in [K]} \left( \hat{\mu}_i(t-1) + \sqrt{\frac{2\log t}{N_i(t-1)}} \right)$$</p>

          <p>The confidence radius $\sqrt{\frac{2\log t}{N_i(t-1)}}$ decreases as more data is collected for arm $i$, but increases with time to ensure sufficient exploration of all arms when evidence is limited.</p>

          <div class="algorithm-box">
            <h4>UCB Algorithm</h4>
            <p><strong>Initialization:</strong> Pull each arm once</p>
            <p><strong>For rounds $t = K+1, K+2, \ldots, n$:</strong></p>
            <ol>
              <li>Compute UCB for each arm: $UCB_i(t) = \hat{\mu}_i(t-1) + \sqrt{\frac{2\log t}{N_i(t-1)}}$</li>
              <li>Select arm: $I_t = \arg\max_i UCB_i(t)$</li>
              <li>Observe reward and update estimates</li>
            </ol>
          </div>

          <p>UCB's regret analysis relies on optimism under uncertainty. The algorithm maintains confidence intervals such that with high probability, the true arm means lie within these intervals. When sufficient evidence accumulates (tight confidence intervals), the optimal arm will have the highest UCB and be selected most often.</p>

          <p>The key lemma bounds how often UCB can select suboptimal arms:</p>

          <p><strong>Lemma:</strong> For any suboptimal arm $i$ with gap $\Delta_i > 0$, the number of times arm $i$ is selected satisfies:</p>

          <p>$$\mathbb{E}[N_i(n)] \leq \frac{8\log n}{\Delta_i^2} + 1 + \frac{\pi^2}{3}$$</p>

          <p>This leads to UCB's regret bound:</p>

          <p>$$\mathbb{E}[R_n] \leq \sum_{i: \Delta_i > 0} \Delta_i \left( \frac{8\log n}{\Delta_i^2} + 1 + \frac{\pi^2}{3} \right) = \sum_{i: \Delta_i > 0} \left( \frac{8\log n}{\Delta_i} + O(1) \right)$$</p>

          <p>Crucially, this scales as $O(\log n / \Delta_i)$ for each suboptimal arm, automatically adapting to problem difficulty. Arms with large gaps are quickly identified and avoided, while arms with small gaps are selected more often - exactly the desired behavior when arms have similar performance.</p>

          <pre><code>def ucb(rewards, T):
    """
    Upper Confidence Bound algorithm

    Args:
        rewards: K x T matrix of rewards
        T: time horizon

    Returns:
        cumulative_regret: array of regret at each time step
    """
    K = rewards.shape[0]

    # Initialize
    empirical_means = np.zeros(K)
    num_pulls = np.zeros(K)
    cumulative_regret = np.zeros(T)
    optimal_reward = np.max(np.mean(rewards, axis=1))

    # Pull each arm once
    for k in range(K):
        reward = rewards[k, k]
        empirical_means[k] = reward
        num_pulls[k] = 1
        regret = optimal_reward - reward
        cumulative_regret[k] = cumulative_regret[k-1] + regret if k > 0 else regret

    # UCB selection
    for t in range(K, T):
        # Compute UCB for each arm
        ucb_values = empirical_means + np.sqrt(2 * np.log(t) / num_pulls)

        # Select arm with highest UCB
        arm = np.argmax(ucb_values)

        # Observe reward
        reward = rewards[arm, int(num_pulls[arm])]

        # Update statistics
        num_pulls[arm] += 1
        empirical_means[arm] += (reward - empirical_means[arm]) / num_pulls[arm]

        # Track regret
        regret = optimal_reward - reward
        cumulative_regret[t] = cumulative_regret[t-1] + regret

    return cumulative_regret</code></pre>
        </section>

        <section id="optimality">
          <h2>Optimality Concepts and Information Theory</h2>

          <p>To understand whether UCB is optimal, we need information-theoretic lower bounds. The fundamental question: what is the minimum regret any algorithm must accept when balancing exploration with exploitation?</p>

          <p>The key insight comes from statistical hypothesis testing. Consider two bandit instances that are statistically "close" but have different optimal arms. An algorithm must distinguish between these instances, requiring sufficient data from each arm. This fundamental statistical requirement leads to unavoidable limits on learning efficiency.</p>

          <p>Let $\mathcal{E} = \{\nu_1, \ldots, \nu_K\}$ be a bandit instance. The Lai–Robbins instance-dependent lower bound states that for any consistent algorithm and each suboptimal arm $i$:</p>

          <p>$$\liminf_{n \to \infty} \frac{\mathbb{E}[N_i(n)]}{\log n} \geq \frac{1}{\text{KL}(\nu_i, \nu_{i^*})}.$$</p>

          <p>For Gaussian rewards with unit variance, $\text{KL}(\mathcal{N}(\mu_i,1),\mathcal{N}(\mu^*,1)) = \Delta_i^2/2$, so asymptotically $\mathbb{E}[N_i(n)] \gtrsim \frac{2\log n}{\Delta_i^2}$. This quantifies instance difficulty via KL; problems with many small gaps are inherently harder.</p>

          <p>The information-theoretic approach uses the Kullback-Leibler (KL) divergence between reward distributions. For distributions $P$ and $Q$:</p>

          <p>$$\text{KL}(P, Q) = \int p(x) \log \frac{p(x)}{q(x)} dx$$</p>

          <p>KL divergence measures the difficulty of distinguishing between distributions. Smaller KL divergence means harder discrimination, requiring more samples.</p>

          <p>The change-of-measure technique relates regret to KL divergence. Consider an algorithm $\pi$ and two bandit instances $\mathcal{E}$ and $\mathcal{E}'$ differing only in arms $i$ and $j$. If instance $\mathcal{E}$ has optimal arm $i$ and instance $\mathcal{E}'$ has optimal arm $j$, then:</p>

          <p>$$\mathbb{E}_{\mathcal{E}}[N_j(n)] \cdot \text{KL}(\nu_j, \nu_j') + \mathbb{E}_{\mathcal{E}}[N_i(n)] \cdot \text{KL}(\nu_i, \nu_i') \geq \text{kl}(\epsilon, 1-\epsilon)$$</p>

          <p>where $\epsilon$ is the probability of misidentifying the optimal arm, and $\text{kl}(p,q) = p\log(p/q) + (1-p)\log((1-p)/(1-q))$ is the binary KL divergence.</p>

          <p>This inequality captures the fundamental statistical trade-off: to achieve low error probability $\epsilon$ in arm identification, we must pull arm $i$ many times (if $\text{KL}(\nu_i, \nu_i')$ is small) or pull arm $j$ many times (if $\text{KL}(\nu_j, \nu_j')$ is small). This mathematical constraint reflects the statistical requirements for reliable learning.</p>
        </section>

        <section id="minimax">
          <h2>More Information Theory and Minimax Lower Bounds</h2>

          <p>The minimax lower bound provides the worst-case regret any algorithm must suffer. It's derived by constructing a particularly challenging family of bandit instances.</p>

          <p><strong>Theorem (Minimax Lower Bound):</strong> For any algorithm and horizon $n \geq K$:</p>

          <p>$$\inf_{\pi} \sup_{\mathcal{E}} \mathbb{E}_{\mathcal{E}}^{\pi}[R_n] \geq c\,\sqrt{K n}$$</p>

          <p>for a universal constant $c>0$. One standard construction takes $K-1$ arms with mean $1/2 - \Delta$ and one arm with mean $1/2 + \Delta$, with $\Delta$ on the order of $\sqrt{K/n}$.</p>

          <p>The proof uses Fano's inequality, a fundamental result in information theory. For a parameter estimation problem with $M$ hypotheses:</p>

          <p>$$\mathbb{P}[\hat{\theta} \neq \theta] \geq 1 - \frac{I(X; \theta) + \log 2}{\log M}$$</p>

          <p>where $I(X; \theta)$ is the mutual information between observations $X$ and parameter $\theta$.</p>

          <p>Applied to bandits, Fano's inequality lower bounds the probability of misidentifying the optimal arm, which directly translates to regret. The construction ensures that:</p>
          <ol>
            <li>The arms are sufficiently similar that many pulls are needed to distinguish them</li>
            <li>The gaps are large enough that mistakes incur significant regret</li>
            <li>The time horizon is short enough that perfect identification is impossible</li>
          </ol>

          <p>Comparing with UCB's minimax regret bound of $O(\sqrt{K n \log n})$, we see that UCB is near-minimax optimal, suffering only an extra $\sqrt{\log n}$ factor. Algorithms like MOSS achieve minimax-optimal $O(\sqrt{K n})$ rates (up to constants), and KL-UCB attains optimal instance-dependent constants.</p>

          <div class="algorithm-box">
            <h4>Key Results Summary</h4>
            <ul>
              <li><strong>ETC:</strong> $O\!\left(\frac{K\log n}{\Delta_{\min}}\right)$ regret (requires gap knowledge)</li>
              <li><strong>UCB1 (Hoeffding):</strong> $O\!\left(\sum_{i:\Delta_i>0} \frac{\log n}{\Delta_i}\right)$ regret</li>
              <li><strong>Minimax lower bound:</strong> $\Omega(\sqrt{K n})$</li>
              <li><strong>UCB1 minimax regret:</strong> $O(\sqrt{K n \log n})$; MOSS achieves $O(\sqrt{K n})$</li>
            </ul>
          </div>
        </section>

        <section id="instance">
          <h2>Instance Dependent Lower Bounds</h2>

          <p>While minimax bounds characterize worst-case performance, instance-dependent bounds reveal that UCB is actually optimal for specific problem instances.</p>

          <p><strong>Theorem (Lai-Robbins Lower Bound):</strong> For any consistent algorithm (one whose regret grows sublinearly), and any bandit instance with gaps $\Delta_i > 0$:</p>

          <p>$$\liminf_{n \to \infty} \frac{\mathbb{E}[N_i(n)]}{\log n} \geq \frac{1}{\text{KL}(\nu_i, \nu_{i^*})}$$</p>

          <p>Here, consistency means $\mathbb{E}[R_n] = o(n^{\alpha})$ for all $\alpha > 0$. The bound states that any reasonable algorithm must pull suboptimal arm $i$ at least $\frac{\log n}{\text{KL}(\nu_i, \nu_{i^*})}$ times.</p>
          <p><span class="edit-green">This lower bound holds under standard regularity conditions (e.g., one-parameter exponential families with mild smoothness).</span></p>

          <p>For Gaussian rewards with variance 1, $\text{KL}(\mathcal{N}(\mu_i, 1), \mathcal{N}(\mu^*, 1)) = \frac{(\mu^* - \mu_i)^2}{2} = \frac{\Delta_i^2}{2}$. This gives:</p>

          <p>$$\liminf_{n \to \infty} \frac{\mathbb{E}[N_i(n)]}{\log n} \geq \frac{2}{\Delta_i^2}$$</p>

          <p>Comparing with UCB's bound $\mathbb{E}[N_i(n)] \leq \frac{8\log n}{\Delta_i^2} + O(1)$, we see UCB achieves the optimal rate up to constants!</p>

          <p>The Lai-Robbins proof employs the method of mixtures and change of measure. The key insight is that to distinguish between the true instance and a "confusing" alternative where arm $i$ is optimal, the algorithm must observe enough samples to resolve the statistical uncertainty.</p>

          <p>Specifically, consider two instances:</p>
          <ul>
            <li>$\mathcal{E}$: True instance with optimal arm $i^*$</li>
            <li>$\mathcal{E}'$: Modified instance where arm $i$ is optimal (swapping means of arms $i$ and $i^*$)</li>
          </ul>

          <p>If the algorithm pulls arm $i$ too few times, it cannot distinguish these instances, leading to suboptimal performance on one of them. The number of required samples is determined by the KL divergence between the arm distributions.</p>

          <p>This establishes UCB as an optimal algorithm for the stochastic bandit problem, achieving both minimax optimality (up to log factors) and instance-dependent optimality.</p>
        </section>

        <section id="conclusion">
          <h2>Looking Ahead</h2>

          <p>We've established the foundational elements of bandit theory: the exploration-exploitation trade-off, algorithms like ETC and UCB for optimal arm selection, and the information-theoretic limits that govern learning. Key takeaways include:</p>

          <ul>
            <li>Regret decomposes as $\sum_i \Delta_i \mathbb{E}[N_i(n)]$, quantifying loss from selecting suboptimal arms</li>
            <li>UCB achieves optimal instance-dependent regret through adaptive confidence intervals, embodying optimism under uncertainty</li>
            <li>Information theory provides fundamental limits on how quickly we can identify optimal arms</li>
            <li>Instance-dependent regret can scale as $\sum_{i:\Delta_i>0} O(\log n/\Delta_i)$; worst-case minimax regret scales as $\Theta(\sqrt{K n})$.</li>
          </ul>

          <p>This foundation prepares us for advanced topics. In <a href="bandits-part2.html">Part 2</a>, we'll explore adversarial settings where rewards can adapt to our actions, contextual bandits that incorporate side information, and structured approaches where rewards have known relationships. In <a href="bandits-part3.html">Part 3</a>, we'll dive into modern theory including sparse linear bandits, refined statistical bounds, and the elegant duality between Bayesian and frequentist approaches.</p>

          <p>The mathematical machinery we've developed - concentration inequalities, confidence sets, information-theoretic arguments - forms the backbone of modern bandit algorithms, extending from simple comparisons to complex structured problems.</p>
        </section>

        <hr>

        <section>
          <h2>References</h2>
          <ul>
            <li>Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. <em>Biometrika</em>, 25(3/4), 285-294.</li>
            <li>Lai, T. L., & Robbins, H. (1985). Asymptotically efficient adaptive allocation rules. <em>Advances in Applied Mathematics</em>, 6(1), 4-22.</li>
            <li>Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. <em>Machine Learning</em>, 47(2-3), 235-256.</li>
            <li>Bubeck, S., & Cesa-Bianchi, N. (2012). Regret analysis of stochastic and nonstochastic multi-armed bandit problems. <em>Foundations and Trends in Machine Learning</em>, 5(1), 1-122.</li>
            <li>Lattimore, T., & Szepesvári, C. (2020). <em>Bandit Algorithms</em>. Cambridge University Press.</li>
            <li>Garivier, A., & Cappé, O. (2011). The KL-UCB algorithm for bounded stochastic bandits and beyond. <em>COLT</em>, 359-376.</li>
            <li>Berry, D. A., & Fristedt, B. (1985). <em>Bandit Problems: Sequential Allocation of Experiments</em>. Chapman and Hall.</li>
            <li>Thall, P. F., & Wathen, J. K. (2007). Practical Bayesian adaptive randomisation in clinical trials. <em>European Journal of Cancer</em>, 43(6), 859-866.</li>
          </ul>
        </section>
      </article>
    </div>

    <footer class="footer">Technical Blog · <a href="bandits-part2.html">Next: Advanced Algorithms →</a></footer>
  </main>

  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        delimiters: [
          {left: "$$", right: "$$", display: true},
          {left: "$", right: "$", display: false}
        ],
        ignoredTags: ["script", "noscript", "style", "textarea", "pre", "code"]
      });
    });
  </script>
</body>
</html>