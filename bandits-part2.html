<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Multi-Armed Bandits Part 2: Advanced Algorithms · Technical Blog</title>
  <link rel="stylesheet" href="styles.css">
  <link rel="stylesheet" href="post-styles.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <style>
    /* Override global page styles for mixed layout */
    .page{
      max-width: none;
      margin: 80px 0 64px 0;
      padding: 0;
    }
    
    .page-header {
      max-width: 800px;
      margin: 0 auto;
      padding: 0 32px;
    }
    
    @media (max-width: 768px) {
      .page {
        margin: 40px 0 32px 0;
      }
      .page-header {
        padding: 0 16px;
      }
    }
    
    .layout{
      display: grid;
      grid-template-columns: 300px 1fr;
      gap: 24px;
      max-width: 1400px;
      margin: 0 auto;
      padding: 0 32px;
    }
    @media(min-width: 1800px){
      .layout{
        max-width: 2200px;
        grid-template-columns: 600px minmax(760px, 900px) 600px;
        justify-content: center;
        padding: 0;
      }
      .toc{grid-column: 1}
      .post{grid-column: 2; max-width: 900px; margin: 0 auto}
    }
    .toc{
      position:sticky; 
      top:24px; 
      height:fit-content;
      border: none;
      border-right: 1px solid var(--border);
      border-radius: 0;
      padding: 0 16px 0 0;
      margin-right: 0;
      background: transparent;
    }
    @media(min-width: 1800px){
      .toc{
        padding: 0 24px 0 0;
        margin-right: 24px;
      }
    }
    .toc ul{list-style:none; padding-left:0}
    .toc li{margin:8px 0}
    .toc li a{
      display: flex;
      align-items: baseline;
      text-decoration: none;
      color: var(--text);
      font-size: 18px;
      line-height: 1.5;
    }
    .toc li a:hover{
      text-decoration: underline;
    }
    .toc-section-number{
      color: var(--text);
      font-weight: 400;
      margin-right: 8px;
      min-width: 20px;
      font-size: 18px;
    }
    @media(min-width: 1800px){
      .toc-section-number{
        margin-right: 16px;
        min-width: 28px;
      }
    }
    .toc-section-title{
      color: var(--text);
      font-weight: 400;
    }
    .series-nav {
      background: transparent;
      border: none;
      border-radius: 0;
      padding: 0;
      margin: 24px 0;
    }
    .series-nav h3 {
      margin: 0 0 12px 0;
      font-size: 18px;
    }
    .series-nav ul {
      margin: 0;
      padding-left: 20px;
    }
    .algorithm-box {
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 16px;
      margin: 16px 0;
      background: var(--code-bg);
    }
    .algorithm-box h4 {
      margin: 0 0 12px 0;
      font-size: 16px;
      font-weight: 700;
    }
    .theorem-box {
      border-left: 3px solid var(--accent);
      padding-left: 16px;
      margin: 16px 0;
      background: rgba(0,0,0,0.02);
      border-radius: 0 8px 8px 0;
    }
  </style>
</head>
<body>
  <main class="page">
    <div class="page-header">
      <nav class="nav">
        <a href="index.html" class="back-btn">← Back</a>
      </nav>

      <header>
        <h1 class="title">Multi-Armed Bandits Part 2: Advanced Algorithms</h1>
        <div class="meta">2025-09-17 · 20 min read</div>
        <hr>
      </header>

      <div class="series-nav">
        <h3>Multi-Armed Bandit Series</h3>
        <ul>
          <li><a href="bandits-part1.html">Part 1: Foundations</a></li>
          <li><strong>Part 2: Advanced Algorithms</strong> (current)</li>
          <li><a href="bandits-part3.html">Part 3: Modern Theory</a></li>
        </ul>
      </div>
    </div>

    <div class="layout">
      <aside class="toc">
        <h2>Contents</h2>
        <ul>
          <li><a href="#adversarial"><span class="toc-section-number">1</span><span class="toc-section-title">Adversarial Bandits</span></a></li>
          <li><a href="#exp3"><span class="toc-section-number">2</span><span class="toc-section-title">The EXP3 Algorithm</span></a></li>
          <li><a href="#high-prob"><span class="toc-section-number">3</span><span class="toc-section-title">High Probability Lower Bounds</span></a></li>
          <li><a href="#contextual"><span class="toc-section-number">4</span><span class="toc-section-title">Contextual Bandits and Experts</span></a></li>
          <li><a href="#exp4"><span class="toc-section-number">5</span><span class="toc-section-title">EXP4 Algorithm</span></a></li>
          <li><a href="#linear"><span class="toc-section-number">6</span><span class="toc-section-title">Stochastic Linear Bandits</span></a></li>
          <li><a href="#confidence"><span class="toc-section-number">7</span><span class="toc-section-title">Ellipsoidal Confidence Sets</span></a></li>
          <li><a href="#conclusion"><span class="toc-section-number">8</span><span class="toc-section-title">Integration and Outlook</span></a></li>
        </ul>
      </aside>

      <article class="post">
        <section id="adversarial">
          <h2>Adversarial Clinical Environments</h2>

          <p>Thompson's stochastic framework assumes treatment effects are drawn from fixed, unknown distributions. But what if diseases can adapt or environmental factors change treatment effectiveness over time? This leads to adversarial clinical settings, where we face evolving pathogens, drug resistance, or changing patient populations that challenge our treatment strategies.</p>

          <p>In adversarial clinical environments, we have treatment outcome sequences $\ell_{i,t}$ (losses, representing treatment failures) for each treatment $i$ and time $t$, determined by disease evolution or environmental changes. These factors may respond to treatment usage patterns (like antibiotic resistance) but cannot anticipate our specific randomized allocation decisions. Our goal is to minimize regret against the best fixed treatment strategy in hindsight:</p>

          <p>$$R_T = \sum_{t=1}^T \ell_{I_t,t} - \min_{i \in [K]} \sum_{t=1}^T \ell_{i,t}$$</p>

          <p>This framework captures scenarios like adaptive pathogens (HIV, influenza), drug resistance development, or changing disease epidemiology that responds to population-level treatment patterns. Unlike stable clinical environments where $O(\log T)$ regret is achievable, adversarial settings have fundamental $\Omega(\sqrt{T})$ regret, reflecting the inherent difficulty of treating evolving diseases.</p>

          <p>The key challenge is the clinical information bottleneck: we only observe outcomes for administered treatments, not the counterfactual outcomes patients would have experienced under alternative therapies. This partial feedback constraint - fundamental to medical ethics - makes adaptive treatment selection significantly harder than laboratory studies with full information.</p>

          <div class="theorem-box">
            <h4>Adversarial Lower Bound</h4>
            <p>For any algorithm and adversarial sequence, there exists a loss sequence such that:</p>
            <p>$$\mathbb{E}[R_T] \geq \frac{1}{2}\sqrt{\frac{K-1}{2}T}$$</p>
          </div>

          <p>The proof constructs a pathogen that evolves randomly until the treatment protocol reveals which therapy is being avoided, then develops resistance to alternatives. This forces clinical algorithms to maintain sufficient exploration of all available treatments - a principle that aligns with antimicrobial stewardship guidelines.</p>
        </section>

        <section id="exp3">
          <h2>The EXP3 Algorithm</h2>

          <p>EXP3 (Exponential-weight algorithm for Exploration and Exploitation) elegantly handles adversarial bandits using importance-weighted estimation and exponential weights. The algorithm maintains weights for each arm and selects arms according to these weights.</p>

          <div class="algorithm-box">
            <h4>EXP3 Algorithm</h4>
            <p><strong>Parameters:</strong> Learning rate $\gamma \in (0,1]$</p>
            <p><strong>Initialize:</strong> $w_i(1) = 1$ for all $i \in [K]$</p>
            <p><strong>For $t = 1, 2, \ldots, T$:</strong></p>
            <ol>
              <li>Set probabilities: $p_i(t) = (1-\gamma)\frac{w_i(t)}{\sum_j w_j(t)} + \frac{\gamma}{K}$</li>
              <li>Sample arm: $I_t \sim p(t)$</li>
              <li>Observe loss: $\ell_{I_t,t}$</li>
              <li>Estimate full loss vector: $\hat{\ell}_i(t) = \frac{\ell_{i,t} \mathbb{1}_{I_t = i}}{p_i(t)}$</li>
              <li>Update weights: $w_i(t+1) = w_i(t) \exp(-\gamma \hat{\ell}_i(t))$</li>
            </ol>
          </div>

          <p>The importance-weighted estimator $\hat{\ell}_i(t)$ is unbiased: $\mathbb{E}[\hat{\ell}_i(t)] = \ell_{i,t}$, but has high variance $\text{Var}[\hat{\ell}_i(t)] = \ell_{i,t}^2 \frac{1-p_i(t)}{p_i(t)}$. The exploration probability $\frac{\gamma}{K}$ ensures all arms have minimum selection probability, controlling this variance.</p>

          <p>EXP3's regret analysis combines the potential function method with careful variance control:</p>

          <div class="theorem-box">
            <h4>EXP3 Regret Bound</h4>
            <p>With $\gamma = \sqrt{\frac{2\log K}{KT}}$, EXP3 achieves:</p>
            <p>$$\mathbb{E}[R_T] \leq 2\sqrt{2KT\log K}$$</p>
          </div>

          <p>The proof tracks the potential $\Phi_t = \sum_i w_i(t)$ and shows that its expected decrease relates to regret. The key insight is that the exponential weights algorithm approximately minimizes regret if the loss estimates were perfect, and importance weighting provides unbiased estimates at the cost of increased variance.</p>

          <pre><code>import numpy as np

def exp3(losses, gamma=None, T=None):
    """
    EXP3 algorithm for adversarial bandits

    Args:
        losses: T x K matrix of losses (revealed online)
        gamma: exploration parameter
        T: time horizon (for default gamma)

    Returns:
        cumulative_regret: array of regret at each time step
    """
    T, K = losses.shape

    if gamma is None:
        gamma = np.sqrt(2 * np.log(K) / (K * T))

    # Initialize weights
    weights = np.ones(K)
    cumulative_regret = np.zeros(T)
    best_arm_losses = np.min(np.sum(losses, axis=0))

    for t in range(T):
        # Compute probabilities
        weight_sum = np.sum(weights)
        probs = (1 - gamma) * weights / weight_sum + gamma / K

        # Sample arm
        arm = np.random.choice(K, p=probs)

        # Observe loss
        loss = losses[t, arm]

        # Compute regret
        regret = loss - best_arm_losses / T
        cumulative_regret[t] = cumulative_regret[t-1] + regret if t > 0 else regret

        # Update weights using importance-weighted loss estimate
        loss_estimate = loss / probs[arm]
        weights[arm] *= np.exp(-gamma * loss_estimate)

    return cumulative_regret

# Alternative: Full-information EXP3 for comparison
def exp3_full_info(losses, gamma=None):
    """EXP3 with full information (for comparison)"""
    T, K = losses.shape

    if gamma is None:
        gamma = np.sqrt(2 * np.log(K) / T)

    weights = np.ones(K)
    cumulative_regret = np.zeros(T)
    best_arm_losses = np.min(np.sum(losses, axis=0))

    for t in range(T):
        # Compute probabilities
        weight_sum = np.sum(weights)
        probs = weights / weight_sum

        # Sample arm
        arm = np.random.choice(K, p=probs)

        # Observe loss
        loss = losses[t, arm]

        # Compute regret
        regret = loss - best_arm_losses / T
        cumulative_regret[t] = cumulative_regret[t-1] + regret if t > 0 else regret

        # Update weights using true losses (full information)
        weights *= np.exp(-gamma * losses[t])

    return cumulative_regret</code></pre>
        </section>

        <section id="high-prob">
          <h2>High Probability Lower Bounds</h2>

          <p>Beyond expected regret, we often need high-probability guarantees. For adversarial bandits, concentration becomes more challenging due to the adaptive nature of the algorithm and the adversarial rewards.</p>

          <p>The high-probability analysis of EXP3 requires careful treatment of the martingale structure. Define the martingale:</p>

          <p>$$M_t = \sum_{s=1}^t \left( \hat{\ell}_{I_s,s} - \ell_{I_s,s} \right)$$</p>

          <p>This captures the difference between importance-weighted estimates and true losses. Since $\mathbb{E}[\hat{\ell}_{I_s,s} | \mathcal{F}_{s-1}] = \ell_{I_s,s}$, where $\mathcal{F}_{s-1}$ is the history up to time $s-1$, we have $\mathbb{E}[M_t] = 0$.</p>

          <p>Using Azuma's inequality for bounded martingale differences:</p>

          <div class="theorem-box">
            <h4>High-Probability Bound for EXP3</h4>
            <p>With probability at least $1-\delta$, EXP3 with $\gamma = \sqrt{\frac{2\log K}{KT}}$ achieves:</p>
            <p>$$R_T \leq 2\sqrt{2KT\log K} + \sqrt{\frac{KT\log(1/\delta)}{2}}$$</p>
          </div>

          <p>The proof bounds the martingale $M_t$ using the fact that importance-weighted losses are bounded by $1/\gamma$. The second term comes from the concentration inequality and grows as $\sqrt{T}$, matching the leading term's dependence.</p>

          <p>For stochastic bandits, high-probability bounds are tighter because we can use concentration of sums of independent random variables. The adversarial setting's adaptive nature makes sharp concentration more difficult to achieve.</p>
        </section>

        <section id="contextual">
          <h2>Contextual Bandits, Prediction with Expert Advice and EXP4</h2>

          <p>Contextual clinical trials extend Thompson's framework by incorporating patient characteristics. Before treating each patient, we observe their context $x_t$ (biomarkers, demographics, medical history). Treatment effectiveness depends on both patient characteristics and the chosen therapy: $r_t = r(x_t, I_t)$.</p>

          <p>This setting interpolates between controlled laboratory studies (where we could test all treatments on identical subjects) and population-level trials (treating all patients identically). Applications include personalized medicine, biomarker-guided therapy selection, and adaptive dosing protocols.</p>

          <p>The challenge is clinical generalization: we must learn effective treatments for new patients based on limited outcome data from previous patients and therapies. This requires leveraging structure in how treatment effects depend on patient characteristics and therapeutic mechanisms.</p>

          <h3>Prediction with Expert Advice</h3>

          <p>A key stepping stone is prediction with expert advice. We have $N$ experts, each providing predictions $f_i(x_t)$ for context $x_t$. Our goal is to perform nearly as well as the best expert in hindsight.</p>

          <p>The Weighted Majority algorithm maintains weights on experts and predicts according to weighted averages. For convex loss functions, this achieves regret $O(\sqrt{T \log N})$, optimal up to constants.</p>

          <p>The connection to bandits is that each expert corresponds to a policy mapping contexts to arms. The challenge is that we can't evaluate all expert predictions - only the one corresponding to our chosen arm.</p>

          <h3>Linear Contextual Bandits</h3>

          <p>A tractable special case assumes linear rewards: the expected reward for arm $i$ in context $x_t$ is $\langle \theta_i, x_t \rangle$ for unknown parameters $\theta_i \in \mathbb{R}^d$.</p>

          <p>This setting enables efficient learning because:</p>
          <ol>
            <li>The problem has finite-dimensional parametrization</li>
            <li>Confidence sets for $\theta_i$ can be computed using least-squares</li>
            <li>Optimism over confidence sets yields effective algorithms</li>
          </ol>

          <p>The key insight is that linear structure enables generalization across contexts - observing reward for context $x$ and arm $i$ provides information about rewards for other contexts through the shared parameter $\theta_i$.</p>
        </section>

        <section id="exp4">
          <h2>EXP4 Algorithm</h2>

          <p>EXP4 (EXP3 with expert advice) combines the robustness of EXP3 with the ability to leverage expert predictions. It maintains weights over expert-arm pairs and uses importance weighting to handle partial feedback.</p>

          <div class="algorithm-box">
            <h4>EXP4 Algorithm</h4>
            <p><strong>Input:</strong> $N$ experts, learning rate $\gamma$</p>
            <p><strong>Initialize:</strong> Weights $w_{i,j}(1) = 1$ for expert $i$, arm $j$</p>
            <p><strong>For $t = 1, 2, \ldots, T$:</strong></p>
            <ol>
              <li>Observe context $x_t$ and expert predictions $f_i(x_t)$ for $i \in [N]$</li>
              <li>Compute arm probabilities: $p_j(t) = \sum_i \frac{w_{i,j}(t)}{\sum_{k,\ell} w_{k,\ell}(t)} f_i(x_t)$</li>
              <li>Add exploration: $p_j(t) \leftarrow (1-\gamma)p_j(t) + \frac{\gamma}{K}$</li>
              <li>Select arm $I_t \sim p(t)$ and observe reward $r_t$</li>
              <li>Update weights: $w_{i,j}(t+1) = w_{i,j}(t) \exp\left(\gamma \frac{r_t f_i(x_t) \mathbb{1}_{I_t = j}}{p_j(t)}\right)$</li>
            </ol>
          </div>

          <p>EXP4's regret bound depends on the diversity of expert predictions:</p>

          <div class="theorem-box">
            <h4>EXP4 Regret Bound</h4>
            <p>EXP4 achieves regret against the best expert:</p>
            <p>$$\mathbb{E}[R_T] \leq O\left(\sqrt{KT\log(NK)/N}\right)$$</p>
          </div>

          <p>When $N$ is large and experts are diverse, this can be much better than standard EXP3. The algorithm effectively uses expert advice to guide exploration while maintaining robustness through importance weighting.</p>

          <p>The weight update rule rewards experts whose predictions align with good outcomes. The importance weighting $\frac{f_i(x_t) \mathbb{1}_{I_t = j}}{p_j(t)}$ ensures unbiased estimation while accounting for the expert's influence on the action probability.</p>

          <pre><code>def exp4(contexts, experts, rewards, gamma=None, T=None):
    """
    EXP4 algorithm for contextual bandits with expert advice

    Args:
        contexts: T x d matrix of contexts
        experts: N x T x K tensor of expert predictions
        rewards: T x K matrix of rewards
        gamma: exploration parameter

    Returns:
        cumulative_regret: regret at each time step
    """
    T, d = contexts.shape
    N, _, K = experts.shape

    if gamma is None:
        gamma = np.sqrt(2 * np.log(N * K) / (K * T))

    # Initialize expert-arm weights
    weights = np.ones((N, K))
    cumulative_regret = np.zeros(T)

    # Compute best expert regret for comparison
    expert_rewards = np.sum([
        np.sum([rewards[t, np.argmax(experts[i, t])] for t in range(T)])
        for i in range(N)
    ])
    best_expert_reward = np.max(expert_rewards)

    for t in range(T):
        context = contexts[t]
        expert_preds = experts[:, t, :]  # N x K

        # Compute arm probabilities weighted by experts
        total_weight = np.sum(weights)
        arm_probs = np.zeros(K)

        for j in range(K):
            for i in range(N):
                arm_probs[j] += (weights[i, j] / total_weight) * expert_preds[i, j]

        # Add exploration
        arm_probs = (1 - gamma) * arm_probs + gamma / K

        # Sample arm
        arm = np.random.choice(K, p=arm_probs)
        reward = rewards[t, arm]

        # Compute regret
        regret = best_expert_reward / T - reward
        cumulative_regret[t] = cumulative_regret[t-1] + regret if t > 0 else regret

        # Update weights
        for i in range(N):
            importance_weight = expert_preds[i, arm] / arm_probs[arm]
            weights[i, arm] *= np.exp(gamma * reward * importance_weight)

    return cumulative_regret</code></pre>
        </section>

        <section id="linear">
          <h2>Stochastic Linear Bandits and UCB</h2>

          <p>Linear clinical response models assume treatment outcomes follow $r_t = \langle \theta^*, x_{I_t} \rangle + \eta_t$, where $x_i \in \mathbb{R}^d$ represents the therapeutic profile of treatment $i$ (dosage, mechanism, pharmacokinetics), $\theta^* \in \mathbb{R}^d$ captures patient-specific response parameters, and $\eta_t$ represents individual variation.</p>

          <p>This setting is much richer than comparing fixed treatments - we can consider infinitely many dosing regimens or combination therapies as long as their therapeutic profiles are characterized. The algorithm must learn patient response patterns $\theta^*$ while optimizing treatment selection.</p>

          <p>LinUCB extends the therapeutic optimism principle to linear dose-response relationships using confidence ellipsoids around least-squares estimates of patient response parameters $\theta^*$.</p>

          <div class="algorithm-box">
            <h4>LinUCB Algorithm</h4>
            <p><strong>Initialize:</strong> Design matrix $V_0 = \lambda I_d$, response vector $b_0 = 0 \in \mathbb{R}^d$</p>
            <p><strong>For patient $t = 1, 2, \ldots, T$:</strong></p>
            <ol>
              <li>Estimate response parameters: $\hat{\theta}_t = V_{t-1}^{-1} b_{t-1}$</li>
              <li>Select treatment: $I_t = \arg\max_i \langle \hat{\theta}_t, x_i \rangle + \beta_t \|x_i\|_{V_{t-1}^{-1}}$</li>
              <li>Observe patient outcome $r_t$ and update:</li>
              <ul>
                <li>$V_t = V_{t-1} + x_{I_t} x_{I_t}^T$ (update design matrix)</li>
                <li>$b_t = b_{t-1} + r_t x_{I_t}$ (update response data)</li>
              </ul>
            </ol>
          </div>

          <p>The confidence width $\beta_t \|x_i\|_{V_{t-1}^{-1}}$ captures uncertainty in treatment $i$'s therapeutic profile direction. Treatments with profiles in poorly-explored therapeutic space receive larger confidence bonuses, encouraging investigation of novel mechanisms or dosing regimens.</p>

          <p>The regret analysis leverages the elliptic potential lemma, which bounds the sum of confidence widths:</p>

          <div class="theorem-box">
            <h4>LinUCB Regret Bound</h4>
            <p>With appropriate choice of $\beta_t$, LinUCB achieves:</p>
            <p>$$\mathbb{E}[R_T] = O(d\sqrt{T \log T})$$</p>
          </div>

          <p>This $\sqrt{T}$ dependence reflects the difficulty of personalizing medicine in high-dimensional therapeutic space, but the linear dependence on dimension $d$ enables practical application to multi-parameter treatment optimization - much better than testing all possible combinations independently.</p>

          <p>The key insight is that the confidence ellipsoid shrinks in therapeutic directions where we've treated many similar patients, enabling generalization across the patient population. The algorithm automatically balances exploration of uncertain therapeutic mechanisms with exploitation of established effective treatments.</p>
        </section>

        <section id="confidence">
          <h2>Ellipsoidal Confidence Sets for Least-Squares Estimators</h2>

          <p>The foundation of LinUCB is the construction of confidence sets for the least-squares estimator. This requires careful analysis of the regularized least-squares problem.</p>

          <p>Given observations $(x_1, r_1), \ldots, (x_t, r_t)$ with $r_s = \langle \theta^*, x_s \rangle + \eta_s$, the regularized least-squares estimator is:</p>

          <p>$$\hat{\theta}_t = \arg\min_{\theta} \left\{ \sum_{s=1}^t (r_s - \langle \theta, x_s \rangle)^2 + \lambda \|\theta\|^2 \right\}$$</p>

          <p>This has closed form $\hat{\theta}_t = V_t^{-1} b_t$ where:</p>
          <ul>
            <li>$V_t = \lambda I + \sum_{s=1}^t x_s x_s^T$ (design matrix)</li>
            <li>$b_t = \sum_{s=1}^t r_s x_s$ (data vector)</li>
          </ul>

          <p>The confidence ellipsoid is based on the concentration of $\hat{\theta}_t$ around $\theta^*$. For Gaussian noise, we can show:</p>

          <p>$$\|\hat{\theta}_t - \theta^*\|_{V_t} \leq \beta_t$$</p>

          <p>with high probability, where $\|\theta\|_{V_t} = \sqrt{\theta^T V_t \theta}$ and $\beta_t = O(\sqrt{d \log t})$.</p>

          <div class="theorem-box">
            <h4>Confidence Ellipsoid</h4>
            <p>With probability at least $1-\delta$:</p>
            <p>$$\theta^* \in \{\theta : \|\theta - \hat{\theta}_t\|_{V_t} \leq \beta_t(\delta)\}$$</p>
            <p>where $\beta_t(\delta) = \sqrt{d \log((1+t/\lambda)/\delta) + \lambda^{1/2} S}$ and $S$ bounds $\|\theta^*\|$.</p>
          </div>

          <p>This ellipsoid has several important properties:</p>
          <ol>
            <li><strong>Adaptive shape:</strong> Narrow in directions with many observations, wide in unexplored directions</li>
            <li><strong>Shrinking volume:</strong> $\text{Vol}(\text{ellipsoid}) \propto \det(V_t)^{-1/2}$ decreases over time</li>
            <li><strong>Optimal exploration:</strong> Maximizing $\|x\|_{V_t^{-1}}$ minimizes ellipsoid volume most quickly</li>
          </ol>

          <p>The UCB selection rule $\max_i \langle \hat{\theta}_t, x_i \rangle + \beta_t \|x_i\|_{V_t^{-1}}$ implements optimism over this confidence set. It selects the arm with highest upper confidence bound on expected reward.</p>

          <pre><code>import numpy as np
from numpy.linalg import inv, norm

def linucb(features, rewards, lam=1.0, delta=0.1):
    """
    Linear UCB algorithm

    Args:
        features: T x K x d tensor of arm features
        rewards: T x K matrix of rewards (observed online)
        lam: regularization parameter
        delta: confidence parameter

    Returns:
        cumulative_regret: regret at each time step
    """
    T, K, d = features.shape

    # Initialize
    V = lam * np.eye(d)
    b = np.zeros(d)
    cumulative_regret = np.zeros(T)

    # Compute best arm regret for comparison
    true_theta = np.random.randn(d)  # Unknown in practice
    true_rewards = np.array([[np.dot(true_theta, features[t, k]) for k in range(K)] for t in range(T)])
    best_rewards = np.max(true_rewards, axis=1)

    for t in range(T):
        # Compute least-squares estimate
        theta_hat = inv(V) @ b

        # Compute confidence width
        beta = np.sqrt(d * np.log((1 + t/lam) / delta) + np.sqrt(lam) * norm(theta_hat))

        # Compute UCB for each arm
        ucb_values = np.zeros(K)
        for k in range(K):
            x = features[t, k]
            mean_reward = np.dot(theta_hat, x)
            confidence_width = beta * np.sqrt(x.T @ inv(V) @ x)
            ucb_values[k] = mean_reward + confidence_width

        # Select arm with highest UCB
        arm = np.argmax(ucb_values)
        reward = rewards[t, arm]

        # Update design matrix and data vector
        x_chosen = features[t, arm]
        V += np.outer(x_chosen, x_chosen)
        b += reward * x_chosen

        # Compute regret
        regret = best_rewards[t] - reward
        cumulative_regret[t] = cumulative_regret[t-1] + regret if t > 0 else regret

    return cumulative_regret

# Ellipsoidal confidence set visualization (2D case)
def plot_confidence_ellipsoid(V, theta_hat, beta):
    """Plot confidence ellipsoid for 2D case"""
    theta = np.linspace(-2, 2, 100)
    ellipsoid_points = []

    for angle in np.linspace(0, 2*np.pi, 100):
        direction = np.array([np.cos(angle), np.sin(angle)])
        # Find radius in this direction
        radius = beta / np.sqrt(direction.T @ V @ direction)
        point = theta_hat + radius * direction
        ellipsoid_points.append(point)

    return np.array(ellipsoid_points)</code></pre>
        </section>

        <section id="conclusion">
          <h2>Integration and Outlook</h2>

          <p>We've explored how adaptive clinical trial algorithms handle increasingly complex medical environments: evolving diseases, patient heterogeneity, and structured therapeutic relationships. Key insights for modern clinical research include:</p>

          <ul>
            <li><strong>Robustness vs. Efficiency:</strong> Adversarial algorithms like EXP3 sacrifice statistical efficiency for worst-case protection against evolving pathogens</li>
            <li><strong>Patient Stratification:</strong> Contextual trials enable leveraging patient characteristics, requiring balanced exploration across patient-treatment combinations</li>
            <li><strong>Mechanistic Understanding:</strong> Linear response models achieve scalable personalization by exploiting therapeutic structure</li>
            <li><strong>Confidence-Based Allocation:</strong> Ellipsoidal confidence regions provide principled therapeutic optimism in high-dimensional treatment spaces</li>
          </ul>

          <p>The progression from simple drug comparisons to linear dose-response models illustrates a fundamental theme in clinical research: how mechanistic understanding enables efficient personalization. Without therapeutic structure, we face the curse of dimensionality in treatment space; with appropriate pharmacological assumptions, we can achieve learning rates that scale with intrinsic therapeutic dimensions rather than all possible combinations.</p>

          <p>Looking ahead to <a href="bandits-part3.html">Part 3</a>, we'll explore the frontiers of adaptive clinical research: sparse treatment effect models that exploit biological pathway structure, refined statistical bounds that reveal when adaptive designs improve over traditional RCTs, and the elegant duality between Bayesian and frequentist approaches that provides deep insights into the nature of evidence-based medicine.</p>

          <p>The mathematical tools developed here - importance weighting for unbiased estimation, ellipsoidal confidence sets for therapeutic optimism, potential function analysis for regret bounds - form the foundation for modern adaptive trial methodology, extending from simple drug comparisons to complex personalized medicine algorithms and sequential biomarker-driven protocols.</p>
        </section>

        <hr>

        <section>
          <h2>References</h2>
          <ul>
            <li>Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. <em>Biometrika</em>, 25(3/4), 285-294.</li>
            <li>Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. E. (2002). The nonstochastic multiarmed bandit problem. <em>SIAM Journal on Computing</em>, 32(1), 48-77.</li>
            <li>Li, L., Chu, W., Langford, J., & Schapire, R. E. (2010). A contextual-bandit approach to personalized news article recommendation. <em>WWW</em>, 661-670.</li>
            <li>Dani, V., Hayes, T. P., & Kakade, S. M. (2008). Stochastic linear optimization under bandit feedback. <em>COLT</em>, 355-366.</li>
            <li>Abbasi-Yadkori, Y., Pál, D., & Szepesvári, C. (2011). Improved algorithms for linear stochastic bandits. <em>NIPS</em>, 2312-2320.</li>
            <li>Agarwal, A., Hsu, D., Kale, S., Langford, J., Li, L., & Schapire, R. (2014). Taming the monster: A fast and simple algorithm for contextual bandits. <em>ICML</em>, 1638-1646.</li>
            <li>Cesa-Bianchi, N., & Lugosi, G. (2006). <em>Prediction, Learning, and Games</em>. Cambridge University Press.</li>
            <li>Villar, S. S., Bowden, J., & Wason, J. (2015). Multi-armed bandit models for the optimal design of clinical trials: benefits and challenges. <em>Statistical Science</em>, 30(2), 199-215.</li>
            <li>Aziz, M., Kaufmann, E., & Riviere, M. K. (2021). On multi-armed bandit designs for dose-finding clinical trials. <em>Journal of Machine Learning Research</em>, 22(1), 685-723.</li>
          </ul>
        </section>
      </article>
    </div>

    <footer class="footer">
      <a href="bandits-part1.html">← Previous: Foundations</a> ·
      <a href="bandits-part3.html">Next: Modern Theory →</a>
    </footer>
  </main>

  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        delimiters: [
          {left: "$$", right: "$$", display: true},
          {left: "$", right: "$", display: false}
        ]
      });
    });
  </script>
</body>
</html>