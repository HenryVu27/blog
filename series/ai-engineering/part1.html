<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Context Engineering Fundamentals · Technical Blog</title>
  <link rel="stylesheet" href="../../css/styles.css">
  <link rel="stylesheet" href="../../css/post-styles.css">
  <link rel="stylesheet" href="../../css/deep-dive-layout.css">
  <link rel="stylesheet" href="../../css/components.css">
  <link rel="stylesheet" href="../../css/mode-toggle.css">
  <script>try{if(localStorage.getItem('blog-reading-mode')==='deep-dive')document.documentElement.classList.add('deep-dive-mode')}catch(e){}</script>
  <!-- KaTeX for Deep Dive math -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
</head>
<body>
  <main class="page">
    <div class="page-header">
      <nav class="nav">
        <a href="../../index.html" class="back-btn">← Back</a>
      </nav>

      <header>
        <h1 class="title">Context Engineering Fundamentals</h1>
        <div class="meta">
          2026-02-23 · 20 min read
          <span class="mode-toggle">
            <button data-mode="builder" class="active">Builder</button>
            <button data-mode="deep-dive">Deep Dive</button>
          </span>
        </div>
        <hr>
      </header>

      <div class="series-nav">
        <h3>AI Engineering Series</h3>
        <ul>
          <li><strong>Part 1: Context Engineering Fundamentals</strong> (current)</li>
          <li>Part 2: RAG, From Naive to Agentic (coming soon)</li>
          <li>Part 3: Memory Engineering (coming soon)</li>
          <li>Part 4: Building AI Agents (coming soon)</li>
          <li>Part 5: Multi-Agent Systems (coming soon)</li>
        </ul>
      </div>
    </div>

    <div class="content-divider"></div>

    <div class="layout">
      <aside class="toc">
        <h2>Contents</h2>
        <ul>
          <li><a href="#from-prompts-to-context"><span class="toc-section-number">1</span><span class="toc-section-title">From Prompts to Context</span></a></li>
          <li><a href="#seven-components"><span class="toc-section-number">2</span><span class="toc-section-title">The Seven Components</span></a></li>
          <li><a href="#four-strategies"><span class="toc-section-number">3</span><span class="toc-section-title">Write / Select / Compress / Isolate</span></a></li>
          <li><a href="#failure-modes"><span class="toc-section-number">4</span><span class="toc-section-title">How Context Fails</span></a></li>
          <li><a href="#token-budget"><span class="toc-section-number">5</span><span class="toc-section-title">A Real Token Budget</span></a></li>
          <li><a href="#what-i-learned"><span class="toc-section-number">6</span><span class="toc-section-title">What I Learned</span></a></li>
          <li><a href="#references"><span class="toc-section-number">7</span><span class="toc-section-title">References</span></a></li>
        </ul>
      </aside>

      <article class="post">

        <!-- ============================================================ -->
        <!-- SECTION 1: From Prompts to Context                           -->
        <!-- ============================================================ -->
        <section id="from-prompts-to-context">
          <h2>From Prompts to Context</h2>

          <p>Sometime around mid-2025, the AI engineering community started calling what we do "context engineering" instead of "prompt engineering." The label shift wasn't cosmetic. It reflected something I'd been feeling for a while: the hard part of building LLM systems isn't phrasing instructions. It's deciding what information the model gets to see.</p>

          <p>Andrej Karpathy called it "the delicate art and science of filling the context window with just the right information for the next step." Philipp Schmid defined it as building "dynamic systems that provide the right information and tools, in the right format, at the right time." Both are saying the same thing. This is systems design work, not copywriting.</p>

          <p>Why does the distinction matter? Because the failure mode changed. In 2023, most LLM failures were model failures: it couldn't reason well enough, or it hallucinated, or it refused. By 2025, Schmid argues that most agent failures are context failures. The model has the capability. It just doesn't have the right information when it needs it. And he claims proper context engineering can push agent success rates from ~30% to over 90%.</p>

          <p>I think the clearest way to see the difference is this:</p>

          <table>
            <thead>
              <tr><th></th><th>Prompt Engineering</th><th>Context Engineering</th></tr>
            </thead>
            <tbody>
              <tr><td>Focus</td><td>How you phrase the instruction</td><td>What information fills the window</td></tr>
              <tr><td>Scope</td><td>Single text string</td><td>Entire information architecture</td></tr>
              <tr><td>Nature</td><td>Static, creative</td><td>Dynamic, systems design</td></tr>
              <tr><td>Core question</td><td>"How should I phrase this?"</td><td>"What does the model need right now?"</td></tr>
              <tr><td>Dominates when</td><td>Self-contained tasks</td><td>Multi-step, agentic systems</td></tr>
            </tbody>
          </table>

          <p>Context engineering is a superset of prompt engineering. The system prompt is one of seven things that fill the context window. For a simple classification task, prompt engineering is enough. But the moment you add retrieval, tools, multi-step reasoning, or agent workflows, you're doing context engineering whether you call it that or not.</p>

          <p>This post covers what those seven components are, the strategies for managing them, how they fail, and what a real token budget looks like in a production agent I built. In later parts, I'll go deeper on RAG (Part 2), memory (Part 3), agents (Part 4), and multi-agent coordination (Part 5).</p>

          <div class="theory-only">
            <div class="callout">
              <p><span class="mode-label">Deep Dive</span></p>
              <p>Mei et al.'s 166-page survey (arXiv:2507.13334, analyzing 1,411 papers) provides the first mathematical formalization. They model context as a structured assembly of six typed components:</p>
              <p>$$C = A(c_\text{instr},\; c_\text{know},\; c_\text{tools},\; c_\text{mem},\; c_\text{state},\; c_\text{query})$$</p>
              <p>Context engineering then becomes a constrained optimization problem: find the assembly function $F$ that maximizes expected reward across tasks, subject to a hard window size limit:</p>
              <p>$$F^* = \arg\max_F \; \mathbb{E}_{\tau \sim \mathcal{T}} \left[\text{Reward}\!\left(P_\theta(Y \mid C_{F}(\tau)),\; Y^*_\tau\right)\right] \quad \text{s.t.} \;\; |C| \leq L_\text{max}$$</p>
              <p>I find this framing useful because it makes the tradeoffs explicit. You're optimizing under a constraint. Every token you spend on tool definitions is a token you can't spend on conversation history. The math makes that concrete.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 2: The Seven Components of Context                   -->
        <!-- ============================================================ -->
        <section id="seven-components">
          <h2>The Seven Components</h2>

          <p>Every LLM call consumes a context window: a fixed-size buffer of tokens containing everything the model can see. What you put in that buffer determines what the model can do. Based on Philipp Schmid's framework (corroborated by Anthropic, LangChain, and Google), context decomposes into seven components. I'll walk through each one and share what I've found matters in practice.</p>

          <p>The <strong>system prompt</strong> (or instructions) is the DNA of your agent, setting behavioral guidelines, role definitions, and rules. Anthropic recommends finding the "right altitude" between specificity and flexibility: start minimal, then iteratively add instructions based on observed failures. A counterintuitive finding from Spotify's engineering team is that larger, static, version-controlled prompts proved more predictable than dynamic tool-based approaches, which suggests you should treat your system prompt as code: versioned, reviewed, tested.</p>

          <p>The <strong>user prompt</strong> is the immediate message from the human, and it's the one component you don't control.</p>

          <p>Then there's <strong>state and short-term history</strong>, meaning the current conversation turns and prior exchanges that serve as the working memory of the system. One finding that surprised me came from FlowHunt: a focused 300-token context often outperforms an unfocused 113,000-token context. What you remove matters as much as what you keep, so don't send raw histories verbatim since greetings, acknowledgments, and off-topic content waste tokens that could go toward actual task-relevant information.</p>

          <p><strong>Long-term memory</strong> is persistent knowledge across conversations: user preferences, facts, summaries, learned patterns. It's complex enough that I'm dedicating Part 3 entirely to it.</p>

          <p><strong>Retrieved information</strong> (RAG) means external knowledge from documents, databases, and APIs, injected on-demand. The key design decision I keep coming back to: pull, don't push. Let the AI determine what context it needs via tool calls rather than front-loading everything. Inngest found this produces better results than pre-loading, and it matches my experience too. Part 2 goes deep on retrieval.</p>

          <p><strong>Tool definitions</strong> are the function signatures the system can invoke, and they consume context tokens whether they're used or not. Two production rules worth knowing: Inngest recommends removing any tool used less than 10% of the time because performance degrades as tool count grows, but Manus found that dynamically loading and removing tools breaks KV-cache (a 10x cost difference between cached and uncached tokens), so their solution is logits masking where tools stay in context but get suppressed at the decoding level through a state machine.</p>

          <p>Finally, <strong>structured output</strong> specifications: JSON schemas, type definitions, output constraints. They guide generation but also consume tokens.</p>

          <p>So what does a rough token allocation look like? In a typical agentic system, the system instructions take 500-2,000 tokens (static per session), conversation history grows from 500-5,000 and needs trimming, RAG results swing from 0-10,000 on-demand, and tool definitions sit at 500-3,000 tokens permanently. The total has to fit the model's window (8k-200k depending on provider), and the art is maximizing signal per token.</p>

          <div class="theory-only">
            <div class="callout">
              <p><span class="mode-label">Deep Dive</span></p>
              <p>The survey formalizes optimal retrieval as an information-theoretic problem. The best retrieval function maximizes mutual information between the target answer and retrieved knowledge, conditioned on the query:</p>
              <p>$$\text{Retrieve}^* = \arg\max_{\text{Retrieve}} \; I(Y^*;\; c_\text{know} \mid c_\text{query})$$</p>
              <p>Context assembly can also be framed as Bayesian posterior inference, combining the likelihood of the query given a context with the prior over contexts given interaction history:</p>
              <p>$$P(C \mid c_\text{query}, \ldots) \propto P(c_\text{query} \mid C) \cdot P(C \mid \text{History}, \text{World})$$</p>
              <p>These formalizations connect what feels like intuitive craft to optimization theory and information theory. I don't think most practitioners need to think in these terms day-to-day, but they're useful for understanding why certain techniques work.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 3: Write / Select / Compress / Isolate               -->
        <!-- ============================================================ -->
        <section id="four-strategies">
          <h2>Write / Select / Compress / Isolate</h2>

          <p>LangChain's framework identifies four strategies for managing context. I find it useful because every production system I've seen uses some combination of these four, and knowing the vocabulary helps you think about what's missing from your own setup.</p>

          <p><strong>Write</strong> means storing information outside the context window for later retrieval. Scratchpads let the agent write intermediate notes during a session: observations, partial results, plans. These persist via tool calls or state objects, available for later steps without occupying the window continuously. Memories go further, enabling cross-session retention by extracting reflections or facts and storing them in a persistent backend. The Manus team uses a concrete version of this: agents maintain a <code>todo.md</code> file during complex tasks, writing and re-reading their plan to counteract the "lost-in-the-middle" problem across ~50 average tool calls.</p>

          <p><strong>Select</strong> means retrieving relevant information when needed. This includes reading back scratchpad notes from earlier steps, querying stored memories using embeddings or keyword search, and full RAG pipelines over documents or code. One specific pattern worth highlighting: when you have many tools, use RAG over tool descriptions to select the right one. This improves selection accuracy 3x compared to exposing all tools at once.</p>

          <p><strong>Compress</strong> means reducing tokens while maintaining task performance. Summarization replaces older conversation history with a condensed version; Claude Code applies auto-compact at 95% context utilization. Trimming removes older messages using heuristics (drop oldest N) or trained pruners. The Provence model is a learned context pruner that identifies which messages contribute least to task performance. One important finding from recurrent context compression research: compressing instructions and context simultaneously degrades responses. Compress the data, but preserve the instructions separately.</p>

          <p><strong>Isolate</strong> means splitting information across separate processing units. Multi-agent systems give each sub-agent its own clean context window, focused on a specific subtask, returning a condensed summary (1,000-2,000 tokens) to the lead agent. HuggingFace's CodeAgent isolates token-heavy objects in sandbox environments, keeping only references in the main context. And you can separate LLM-exposed fields from auxiliary context storage in your state schema: not everything the system knows needs to be in the window.</p>

          <p>Most production systems use all four strategies. The question is the mix, and that depends on your task profile, model, and latency budget.</p>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 4: How Context Fails                                 -->
        <!-- ============================================================ -->
        <section id="failure-modes">
          <h2>How Context Fails</h2>

          <p>Drew Breunig identified four failure modes that I think explain most context-related degradation. Understanding them matters because each has a different fix. Applying the wrong mitigation wastes effort.</p>

          <p><strong>Context poisoning</strong> happens when a hallucination or error enters the context and gets repeatedly referenced, compounding mistakes over time. Once a wrong fact lands in the conversation history, the model treats it as ground truth and builds on it. Google DeepMind's Gemini 2.5 technical report confirmed this: if the "goals" section of an agent's context was poisoned, agents developed completely nonsensical strategies downstream. The fix is to validate information before writing to long-term memory, using structured schemas with field-level validation and treating memory writes like database writes where you check constraints before committing.</p>

          <p><strong>Context distraction</strong> happens when the context grows so long that the model over-focuses on accumulated history and neglects what it learned during training. Beyond ~100k tokens, agents tend toward repeating actions from history rather than synthesizing novel plans, so aggressive trimming and summarization help, along with actively removing completed or irrelevant sections.</p>

          <p><strong>Context confusion</strong> is when superfluous information gets treated as signal because the model can't distinguish noise from relevant information when everything is dumped in together. Anthropic's guiding principle applies here: "find the smallest set of high-signal tokens that maximize the likelihood of your desired outcome." Every token should earn its place.</p>

          <p><strong>Context clash</strong> occurs when new information conflicts with existing information already in the prompt. Contradictory instructions produce unpredictable behavior. The fix is establishing a hierarchy of instructions with clear precedence rules and testing for contradictions when updating context.</p>

          <p>I think of these four modes as debugging categories. When an agent misbehaves, the first question is: which failure mode is active? The mitigations are different for each.</p>

          <div class="theory-only">
            <div class="callout">
              <p><span class="mode-label">Deep Dive</span></p>
              <p>There's a deeper mechanism behind context distraction that Anthropic calls <strong>context rot</strong>. Recall accuracy diminishes as token counts increase because the number of pairwise token relationships in self-attention scales as $O(n^2)$. At 100k+ tokens, the model's attention budget is spread thin across billions of pair interactions. Critical information gets drowned out.</p>
              <p>The survey quantifies this with the <strong>lost-in-the-middle</strong> finding: performance degrades by up to 73% when relevant information sits in the middle of long contexts, compared to the beginning or end. This isn't a bug you can fix. It's an architectural property of self-attention. The practical implication: position matters. Put your most important context at the beginning and end of the window, not the middle.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 5: A Real Token Budget                               -->
        <!-- ============================================================ -->
        <section id="token-budget">
          <h2>A Real Token Budget</h2>

          <p>I built an ADHD coaching assistant as a ReAct agent with LangGraph, and looking at its token budget is probably the most concrete way to see context engineering in action. The system prompt ranges from ~2,000 to ~4,050 tokens depending on session maturity, assembled dynamically from several blocks.</p>

          <p>There's a conversation state block (50-100 tokens, updated every turn) carrying the turn count, phase, datetime, and active topic. A static identity block (~350 tokens) defines the coach's personality and voice. A boundaries block (~280 tokens) encodes five hard scope rules. Then a dynamic family context block (80-700 tokens) contains the user profile, session summary, and goals. Static blocks for approach, tools, and response guides add ~470 tokens. Six few-shot examples take ~550 tokens. And conditionally, the last 3 RAG results from the knowledge base add up to 1,500 tokens.</p>

          <p>On top of all that, the conversation window holds 6 turns max, char-budgeted at 120,000 characters. Messages from turns already covered by the rolling summary are excluded. Tool results add 0-10,000 tokens on-demand when the agent calls <code>search_knowledge_base</code>.</p>

          <p>What I find interesting is how many context engineering patterns end up in a single system. This agent uses XML-tagged sections for structure, rolling summaries (every 5 turns via Gemini Flash) for compression, history trimming with a 6-turn window, on-demand hybrid RAG (dense + sparse + RRF fusion + cross-encoder reranking), semantic memory via a structured family profile, episodic memory via outcome events, and isolated guardrail contexts so input/output safety gates don't contaminate the main conversation.</p>

          <p>But even with all that, there are gaps. The system has no procedural memory: the system prompt never evolves based on what works. There's no consolidation from episodic to semantic memory, so five positive outcomes for the same strategy stay as five separate records instead of becoming one fact. Memories are retrieved by recency only, with no importance scoring. Old data never expires. And the volatile conversation state block sits at the start of the prompt, breaking the KV-cache prefix on every turn (a 10x cost difference according to Manus).</p>

          <p>This analysis taught me that context engineering is deeply iterative. You build something, observe the failure modes, and add techniques. No system gets it right on the first pass. Or the second.</p>

          <div class="theory-only">
            <div class="callout">
              <p><span class="mode-label">Deep Dive</span></p>
              <p>How much does context engineering actually matter in aggregate? The GAIA benchmark provides the starkest number: human accuracy is 92%, GPT-4 accuracy is 15%. A 77-point gap. On the GTA benchmark, GPT-4 completes fewer than 50% of tasks. On WebArena, the top agent (IBM CUGA) reaches only 61.7%. These benchmarks all require integrating information from multiple sources, using tools, and maintaining state across steps. Exactly the problems context engineering addresses.</p>
              <p>Memory systems fare poorly too. LongMemEval (500 curated questions) finds 30% accuracy degradation in commercial assistants during extended interactions. GPT-4, Claude, and Llama 3.1 all struggle with episodic memory involving interconnected events, even in brief contexts. The gap between model capability on narrow benchmarks and system capability on realistic tasks is, I think, the context engineering gap.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 6: What I Learned                                    -->
        <!-- ============================================================ -->
        <section id="what-i-learned">
          <h2>What I Learned</h2>

          <p>After building that agent, reading through the research, and looking at what teams like Manus, Spotify, and Inngest have published, a few principles feel solid enough to share.</p>

          <p>Start minimal and iterate on failures, adding instructions only when you observe a specific failure mode. Both Anthropic and Spotify converge on this. The instinct is to dump everything in, but less is genuinely more: FlowHunt showed that 300 focused tokens beat 113,000 unfocused ones.</p>

          <p>Structure your context with XML tags, Markdown headers, or clear delimiters since LLMs respond measurably better to structured input than unstructured dumps. And version control your prompts; Spotify found that static, versioned prompts are more reliable than dynamic ones.</p>

          <p>On the cost side, preserve cache stability. Manus reports that cached tokens on Claude Sonnet cost $0.30/MTok vs $3/MTok uncached, so keeping prompt prefixes stable and append-only matters because any mutation invalidates the cache.</p>

          <p>Minimize your tool count by removing tools used less than 10% of the time since fewer tools mean fewer dimensions of unpredictability. Spotify found, counterintuitively, that deliberately restricting tool access improved reliability.</p>

          <p>For agents specifically, isolate with sub-agents rather than maintaining one massive shared context, and keep failure traces in the context; Manus found that models seeing their own errors implicitly update beliefs and reduce mistake repetition. Build observability from day one too, logging every tool call with inputs, outputs, and timing, because you'll need that data for evaluation and iteration.</p>

          <p>One last thing: match your architecture to your model. Different models have different strengths, and what works for Claude might not work for GPT-4 might not work for Gemini. Context strategy isn't portable across providers without testing.</p>

          <p>In Part 2, I'll go deep on retrieval-augmented generation: how to get the right external information into the context at the right time, from naive RAG pipelines all the way to agentic search systems that decide for themselves when and what to retrieve.</p>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 7: References                                        -->
        <!-- ============================================================ -->
        <section id="references">
          <h2>References</h2>

          <ol>
            <li>Karpathy, A. "Context Engineering." <a href="https://x.com/karpathy/status/1937902205765607626">X/Twitter</a>, June 2025.</li>
            <li>Lutke, T. "Context Engineering over Prompt Engineering." <a href="https://x.com/tobi/status/1935533422589399127">X/Twitter</a>, June 2025.</li>
            <li>Schmid, P. "Context Engineering." <a href="https://www.philschmid.de/context-engineering">philschmid.de</a>, 2025.</li>
            <li>Rajasekaran, P. et al. "Effective Context Engineering for AI Agents." <a href="https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents">Anthropic Engineering</a>, September 2025.</li>
            <li>Martin, L. "Context Engineering for Agents." <a href="https://blog.langchain.com/context-engineering-for-agents/">LangChain Blog</a>, July 2025.</li>
            <li>Breunig, D. "How Contexts Fail and How to Fix Them." <a href="https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html">dbreunig.com</a>, June 2025.</li>
            <li>Ji, Y. "Context Engineering for AI Agents: Lessons from Building Manus." <a href="https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus">manus.im</a>, July 2025.</li>
            <li>Spotify Engineering. "Context Engineering: Background Coding Agents Part 2." <a href="https://engineering.atspotify.com/2025/11/context-engineering-background-coding-agents-part-2">engineering.atspotify.com</a>, November 2025.</li>
            <li>Inngest. "Five Critical Lessons for Context Engineering." <a href="https://www.inngest.com/blog/five-lessons-for-context-engineering">inngest.com</a>, 2025.</li>
            <li>Mei, Z. et al. "A Survey of Context Engineering for Large Language Models." <a href="https://arxiv.org/abs/2507.13334">arXiv:2507.13334</a>, July 2025.</li>
            <li>Willison, S. "Context Engineering." <a href="https://simonwillison.net/2025/Jun/27/context-engineering/">simonwillison.net</a>, June 2025.</li>
            <li>Osmani, A. "Context Engineering: Bringing Engineering Discipline to AI." <a href="https://addyo.substack.com/p/context-engineering-bringing-engineering">Substack</a>, 2025.</li>
            <li>FlowHunt. "Context Engineering: The Definitive Guide." <a href="https://www.flowhunt.io/blog/context-engineering/">flowhunt.io</a>, 2025.</li>
            <li>Fowler, M. "Context Engineering for Coding Agents." <a href="https://martinfowler.com/articles/exploring-gen-ai/context-engineering-coding-agents.html">martinfowler.com</a>, 2025.</li>
          </ol>
        </section>

      </article>
    </div>

    <footer class="footer">© Henry Vu</footer>
  </main>

  <script src="../../js/katex-init.js"></script>
  <script src="../../js/mode-toggle.js"></script>
</body>
</html>
