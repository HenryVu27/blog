<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>What Fills the Context Window · Technical Blog</title>
  <link rel="stylesheet" href="../../css/styles.css">
  <link rel="stylesheet" href="../../css/post-styles.css">
  <link rel="stylesheet" href="../../css/deep-dive-layout.css">
  <link rel="stylesheet" href="../../css/components.css">
  <link rel="stylesheet" href="../../css/mode-toggle.css">
  <script>try{if(localStorage.getItem('blog-reading-mode')==='deep-dive')document.documentElement.classList.add('deep-dive-mode')}catch(e){}</script>
  <!-- KaTeX for Deep Dive math -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
</head>
<body>
  <main class="page">
    <div class="page-header">
      <nav class="nav">
        <a href="../../index.html" class="back-btn">← Back</a>
      </nav>

      <header>
        <h1 class="title">What Fills the Context Window</h1>
        <div class="meta">
          2026-02-23 · 20 min read
        </div>
        <div class="mode-toggle">
          <button data-mode="deep-dive">Deep Dive</button>
          <button data-mode="builder" class="active">Builder</button>
        </div>
        <hr>
      </header>

      <div class="series-nav">
        <h3>AI Engineering Series</h3>
        <ul>
          <li>Part 1: What Fills the Context Window (current)</li>
          <li>Part 2: RAG, From Naive to Agentic (coming soon)</li>
          <li>Part 3: Memory Engineering (coming soon)</li>
          <li>Part 4: Building AI Agents (coming soon)</li>
          <li>Part 5: Multi-Agent Systems (coming soon)</li>
        </ul>
      </div>
    </div>

    <div class="content-divider"></div>

    <div class="layout">
      <aside class="toc">
        <div class="toc-sticky">
          <h2>Contents</h2>
          <ul>
            <li><a href="#from-prompts-to-context"><span class="toc-section-number">1</span><span class="toc-section-title">From Prompts to Context</span></a></li>
            <li><a href="#seven-components"><span class="toc-section-number">2</span><span class="toc-section-title">The Seven Components</span></a></li>
            <li><a href="#four-strategies"><span class="toc-section-number">3</span><span class="toc-section-title">Write / Select / Compress / Isolate</span></a></li>
            <li><a href="#failure-modes"><span class="toc-section-number">4</span><span class="toc-section-title">How Context Fails</span></a></li>
            <li><a href="#token-budget"><span class="toc-section-number">5</span><span class="toc-section-title">A Real Token Budget</span></a></li>
            <li><a href="#what-i-learned"><span class="toc-section-number">6</span><span class="toc-section-title">What I Learned</span></a></li>
            <li><a href="#references"><span class="toc-section-number">7</span><span class="toc-section-title">References</span></a></li>
          </ul>
        </div>
      </aside>

      <article class="post">

        <!-- ============================================================ -->
        <!-- SECTION 1: From Prompts to Context                           -->
        <!-- ============================================================ -->
        <section id="from-prompts-to-context">
          <h2>From Prompts to Context</h2>

          <p>Sometime around mid-2025, the AI engineering community started calling what we do "context engineering" instead of "prompt engineering," and the rename stuck because it described what I was actually spending my time on. Not crafting the perfect instruction phrasing. Figuring out what information the model should see at each step, and building the infrastructure to assemble it reliably.</p>

          <p><a href="#ref-karpathy">Andrej Karpathy</a> called it "the delicate art and science of filling the context window with just the right information for the next step." I like that framing because it puts the emphasis on information selection, not wordsmithing.</p>

          <p>What makes the rename more than cosmetic is that the dominant failure mode shifted. In 2023, most LLM failures were model failures. The model couldn't reason well enough, or it hallucinated, or it refused. By 2025, most agent failures I encounter are context failures, where the model is perfectly capable of doing what I need but doesn't have the right information when it needs it. I've debugged more broken agents by fixing what goes *into* the prompt than by fixing the prompt itself.</p>

          <p>The system prompt is one of seven things that fill the context window, and for a simple classification task, careful prompt engineering is all you need. But the moment you add retrieval, tools, multi-step reasoning, or agent workflows, you're doing context engineering whether you call it that or not. Context engineering is the bigger tent that prompt engineering lives inside.</p>

          <p>This post walks through those seven components, the strategies for managing them, how they fail, and what a real token budget looks like in an ADHD coaching agent I built with LangGraph (with code). I'll go deeper on RAG in Part 2, memory in Part 3, agents in Part 4, and multi-agent coordination in Part 5.</p>

          <div class="theory-only">
            <div class="callout">
              <p><span class="mode-label">Deep Dive</span></p>
              <p><a href="#ref-mei">Mei et al.'s 166-page survey</a> (arXiv:2507.13334, analyzing 1,411 papers) provides the first mathematical formalization. They model context as a structured assembly of six typed components:</p>
              <p>$$C = A(c_\text{instr},\; c_\text{know},\; c_\text{tools},\; c_\text{mem},\; c_\text{state},\; c_\text{query})$$</p>
              <p>Context engineering then becomes a constrained optimization problem, where you find the assembly function $F$ that maximizes expected reward across tasks, subject to a hard window size limit:</p>
              <p>$$F^* = \arg\max_F \; \mathbb{E}_{\tau \sim \mathcal{T}} \left[\text{Reward}\!\left(P_\theta(Y \mid C_{F}(\tau)),\; Y^*_\tau\right)\right] \quad \text{s.t.} \;\; |C| \leq L_\text{max}$$</p>
              <p>Now, I'll be honest, my first reaction to this was that it's a classic case of ML researchers dressing up practitioner intuition in math to make it paper-worthy. "Maximize reward subject to a token budget" is just "put good stuff in, leave bad stuff out" wearing a tuxedo. But the formalization actually helped me think more clearly about one thing: every token you spend on tool definitions is a token you can't spend on conversation history, and that tradeoff is zero-sum in a way my gut wasn't fully appreciating before I saw it written as a constraint.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 2: The Seven Components of Context                   -->
        <!-- ============================================================ -->
        <section id="seven-components">
          <h2>The Seven Components</h2>

          <p>Every LLM call consumes a context window, a fixed-size buffer of tokens containing everything the model can see. What you put in that buffer determines what the model can do. Several people have converged on roughly the same decomposition (<a href="#ref-schmid">Philipp Schmid</a>, <a href="#ref-anthropic">Anthropic</a>, <a href="#ref-langchain">LangChain</a>, Google), and I've found seven components to be the right granularity. I'll walk through each one with code from my ADHD coaching agent, but what I want to emphasize is how unevenly they matter in practice.</p>

          <div style="border:1px solid var(--border); border-radius:8px; padding:20px 24px; margin:24px 0; background:#fafafa;">
            <div style="display:flex; align-items:center; gap:8px; margin-bottom:14px;">
              <span style="font-size:12px; font-weight:500; text-transform:uppercase; letter-spacing:0.05em; color:var(--muted);">Context Window</span>
              <span style="flex:1; border-top:1px solid var(--border);"></span>
              <span style="font-size:12px; color:var(--muted);">8K-200K tokens</span>
            </div>
            <div style="display:flex; flex-direction:column; gap:3px; font-size:13px;">
              <div style="display:flex; align-items:center; background:#eeedea; border-radius:4px; padding:8px 12px;">
                <span style="flex:1;">System prompt</span>
                <span style="color:var(--muted); font-size:12px;">500-2,000</span>
              </div>
              <div style="display:flex; align-items:center; background:#eeedea; border-radius:4px; padding:6px 12px;">
                <span style="flex:1;">User prompt</span>
                <span style="color:var(--muted); font-size:12px;">variable</span>
              </div>
              <div style="display:flex; align-items:center; background:#eeedea; border-radius:4px; padding:14px 12px;">
                <span style="flex:1;">State + conversation history</span>
                <span style="color:var(--muted); font-size:12px;">500-5,000</span>
              </div>
              <div style="display:flex; align-items:center; background:#eeedea; border-radius:4px; padding:8px 12px;">
                <span style="flex:1;">Long-term memory</span>
                <span style="color:var(--muted); font-size:12px;">variable</span>
              </div>
              <div style="display:flex; align-items:center; background:#eeedea; border-radius:4px; padding:20px 12px;">
                <span style="flex:1;">Retrieved information (RAG)</span>
                <span style="color:var(--muted); font-size:12px;">0-10,000</span>
              </div>
              <div style="display:flex; align-items:center; background:#eeedea; border-radius:4px; padding:10px 12px;">
                <span style="flex:1;">Tool definitions</span>
                <span style="color:var(--muted); font-size:12px;">500-3,000</span>
              </div>
              <div style="display:flex; align-items:center; background:#eeedea; border-radius:4px; padding:6px 12px;">
                <span style="flex:1;">Structured output</span>
                <span style="color:var(--muted); font-size:12px;">50-500</span>
              </div>
            </div>
          </div>

          <p>Of the seven, the <strong>system prompt</strong> is where most people start, and rightfully so. It sets behavioral guidelines, role definitions, and rules for your agent. What I didn't expect when I started building agents is how much the system prompt wants to grow. Every failure mode you encounter tempts you to add another instruction, and before long you're at 4,000 tokens of rules that sometimes contradict each other. I've learned (the hard way) to start minimal and iteratively add instructions based on observed failures, which is essentially what <a href="#ref-anthropic">Anthropic</a> recommends as finding the "right altitude" between specificity and flexibility. <a href="#ref-spotify">Spotify's engineering team</a> went even further and found that larger, static, version-controlled prompts proved more predictable than dynamic tool-based approaches. I treat my system prompts as code now: versioned, reviewed, tested.</p>

          <p>My agent's system prompt is broken into XML-tagged sections, each responsible for a distinct behavioral concern:</p>

<pre><code>&lt;identity&gt;
  Personality, voice, coach name
&lt;/identity&gt;

&lt;boundaries&gt;
  5 hard scope rules + output gate awareness
  "If your response recommends specific medications...
   the entire response will be discarded."
&lt;/boundaries&gt;

&lt;family-context&gt;
  Structured profile, session summary, goals
&lt;/family-context&gt;

&lt;approach&gt;
  Progressive profiling, evidence-based guidance
&lt;/approach&gt;

&lt;tools&gt;
  When to search, when to update profile
&lt;/tools&gt;

&lt;response-guide&gt;
  Adaptive length, tone, situation matching
&lt;/response-guide&gt;

&lt;examples&gt;
  6 few-shot coaching conversations
&lt;/examples&gt;</code></pre>

          <p>I started using XML tags mostly for my own sanity (it's easier to review a prompt when you can collapse sections), but it turns out LLMs respond measurably better to structured input than unstructured dumps. Both <a href="#ref-anthropic">Anthropic</a> and Google recommend structured delimiters for this reason. The tags also give you a natural unit for version control diffs, which matters more than you'd think once your prompt is 2,000+ tokens and three people are editing it.</p>

          <p>I spend the least time worrying about the <strong>user prompt</strong> because it's the one thing I don't control. The immediate message from the human can be anything, from well-structured to incoherent, concise to rambling. The rest of your context engineering has to be robust enough to handle whatever arrives.</p>

          <p>Where I've seen the most waste is <strong>state and short-term history</strong>, the current conversation turns and prior exchanges that serve as the working memory of your system. Most implementations (my first version included) just dump the raw history in verbatim, greetings, acknowledgments, off-topic tangents, all of it. I was shocked when I read that <a href="#ref-flowhunt">FlowHunt</a> measured a focused 300-token context outperforming an unfocused 113,000-token context, but in hindsight it makes sense. What you remove from history matters at least as much as what you keep.</p>

          <p>My agent injects a compact state block that gives the model temporal awareness and continuity across the ReAct loop:</p>

<pre><code>def build_conversation_state(turn, phase, recent_tool_calls, active_topic, current_datetime):
    lines = [f"  &lt;turn&gt;{turn}&lt;/turn&gt;", f"  &lt;phase&gt;{phase}&lt;/phase&gt;"]
    if current_datetime:
        day_name = current_datetime.strftime("%A")
        time_of_day = "morning" if hour &lt; 12 else "afternoon" if hour &lt; 17 else "evening"
        lines.append(f"  &lt;datetime&gt;{day_name} {time_of_day}&lt;/datetime&gt;")
    if recent_tool_calls:
        lines.append(f"  &lt;last_tools&gt;{', '.join(recent_tool_calls)}&lt;/last_tools&gt;")
    if active_topic:
        lines.append(f"  &lt;focus&gt;{active_topic}&lt;/focus&gt;")
    return "&lt;conversation_state&gt;\n" + "\n".join(lines) + "\n&lt;/conversation_state&gt;"</code></pre>

          <p>This 50-100 token block tells the model what turn it's on, what phase the conversation is in, what time of day it is, and which tools it already called. Without it, the agent re-calls tools it used two turns ago or asks "how can I help you today?" on turn 8.</p>

          <p>I'm dedicating Part 3 entirely to <strong>long-term memory</strong> (persistent knowledge across conversations, things like user preferences, facts, summaries, learned patterns) because it's complex enough to need its own treatment.</p>

          <p>I spend the most engineering time on <strong>retrieved information</strong>, the RAG layer. External knowledge from documents, databases, and APIs gets injected on-demand, and the design decision I keep coming back to is to let the model pull what it needs via tool calls rather than front-loading everything. I tried the front-loading approach first (stuffing all potentially relevant docs into context at the start of each turn) and it worked terribly. <a href="#ref-inngest">Inngest</a> measured the same thing; pull consistently beats push. Part 2 goes deep on retrieval.</p>

          <p>In my agent, the retrieval tool automatically enriches queries with what it already knows about the family:</p>

<pre><code>@tool
async def search_knowledge_base(query, document_type=None, tags=None, age_range=None):
    state = await session_store.get(session_id)

    # Auto-apply age filter from family profile
    if not age_range and state.family_profile.child_age:
        age_range = _age_to_range(state.family_profile.child_age)

    filters = RetrievalFilters(document_type=document_type, tags=tags, age_range=age_range)
    response = await retriever.retrieve(query=query, filters=filters, state=state)</code></pre>

          <p>The agent decides when to search (pull, not push), and the tool enriches the query behind the scenes. The age filter means a parent of a 6-year-old never sees adolescent strategies, without the parent or the model needing to specify that constraint explicitly.</p>

          <p><strong>Tool definitions</strong> are the sneaky budget item. Function signatures the system can invoke consume context tokens whether they're used or not, and there's a genuine tension in how to manage them. <a href="#ref-inngest">Inngest</a> recommends removing any tool used less than 10% of the time because performance degrades as tool count grows. But <a href="#ref-manus">Manus</a> found that dynamically loading and removing tools breaks KV-cache (a 10x cost difference between cached and uncached tokens), so their solution is logits masking, where tools stay in context but get suppressed at the decoding level through a state machine. I don't think there's a clean answer here yet. I'm currently leaning toward keeping tool counts low rather than worrying about cache-aware masking, mostly because my agents have 4-6 tools, not 40. If you're at Manus's scale with dozens of tools, the cache math probably dominates.</p>

          <p><strong>Structured output</strong> specifications (JSON schemas, type definitions, output constraints) are easy to overlook in a token budget because they feel like "free" structure. They're not. At scale they add up, and I've seen schemas that consume 300+ tokens before the model even starts generating.</p>

          <div class="algorithm-box">
            <h4>Typical Token Allocation (Agentic System)</h4>
            <table>
              <thead>
                <tr><th>Component</th><th>Tokens</th><th>Behavior</th></tr>
              </thead>
              <tbody>
                <tr><td>System instructions</td><td>500-2,000</td><td>Static per session</td></tr>
                <tr><td>Conversation history</td><td>500-5,000</td><td>Grows, needs trimming</td></tr>
                <tr><td>RAG results</td><td>0-10,000</td><td>On-demand, per tool call</td></tr>
                <tr><td>Tool definitions</td><td>500-3,000</td><td>Permanent, always present</td></tr>
                <tr><td>State / structured output</td><td>50-500</td><td>Dynamic per turn</td></tr>
              </tbody>
            </table>
            <p>Total must fit the model's window (8k-200k depending on provider). Getting this budget right is less about cramming in more and more about being ruthless with what you leave out.</p>
          </div>

          <div class="theory-only">
            <div class="callout">
              <p><span class="mode-label">Deep Dive</span></p>
              <p>The survey also formalizes optimal retrieval as an information-theoretic problem. The best retrieval function maximizes mutual information between the target answer and retrieved knowledge, conditioned on the query:</p>
              <p>$$\text{Retrieve}^* = \arg\max_{\text{Retrieve}} \; I(Y^*;\; c_\text{know} \mid c_\text{query})$$</p>
              <p>And context assembly can be framed as Bayesian posterior inference, combining the likelihood of the query given a context with the prior over contexts given interaction history:</p>
              <p>$$P(C \mid c_\text{query}, \ldots) \propto P(c_\text{query} \mid C) \cdot P(C \mid \text{History}, \text{World})$$</p>
              <p>Again, more math than most practitioners will ever need day-to-day. But I found the mutual information framing genuinely clarifying for one specific question: when two retrieval approaches return different documents, the one that tells the model more *new* information (higher mutual information with the answer, conditional on the query) is the better one. That helped me reason about why hybrid retrieval beats pure semantic search in my own system.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 3: Write / Select / Compress / Isolate               -->
        <!-- ============================================================ -->
        <section id="four-strategies">
          <h2>Write / Select / Compress / Isolate</h2>

          <p>Every production context system I've seen uses some combination of four strategies, and <a href="#ref-langchain">LangChain's framework</a> gives them clean names. I find the vocabulary useful not because the categories are surprising, but because asking "which of these four am I underinvesting in?" is usually the fastest way to improve a system.</p>

          <p><strong>Writing</strong> means getting information out of the context window and into external storage for later retrieval. Scratchpads let the agent write intermediate notes during a session (observations, partial results, plans) that persist via tool calls or state objects without occupying the window continuously. Memories go further, enabling cross-session retention by extracting reflections or facts and storing them in a persistent backend. The <a href="#ref-manus">Manus</a> team has a concrete version of this that I really like: their agents maintain a <code>todo.md</code> file during complex tasks, writing and re-reading their plan to counteract the "lost-in-the-middle" problem across ~50 average tool calls. It's charmingly simple for a state-of-the-art agent.</p>

          <p>My agent writes rolling summaries every 5 turns, using a cheap Gemini Flash call to compress the conversation into 2-4 sentences:</p>

<pre><code>async def _update_summary(self, session_id, current_turn):
    existing = await self._store.get_latest_summary(session_id)
    start_turn = (existing.covers_through_turn + 1) if existing else 1
    messages = await self._store.get_messages_range(session_id, start_turn, current_turn)

    conversation_text = "\n".join(f"{m['role'].upper()}: {m['content']}" for m in messages)

    prompt = f"""Summarize this coaching conversation concisely. Focus on:
    - Key family information shared
    - Strategies discussed or recommended
    - Parent's emotional state and concerns
    Omit: greetings, small talk, generic acknowledgments.

    Previous summary: {existing.summary if existing else ""}
    New conversation: {conversation_text}

    Write a concise summary (2-4 sentences)."""

    summary = await self._gemini.generate(prompt, temperature=0.0)</code></pre>

          <p>The summary replaces the turns it covers, so the context window never accumulates unbounded history. On turn 20, the model sees a summary covering turns 1-15 plus the raw last 5 turns, rather than all 20 turns verbatim.</p>

          <p><strong>Selecting</strong> is the complement, retrieving relevant information back when needed. This includes reading back scratchpad notes from earlier steps, querying stored memories using embeddings or keyword search, and full RAG pipelines over documents or code. One pattern worth calling out specifically: if you have many tools, you can use RAG over tool descriptions to select the right one. This improves selection accuracy 3x compared to exposing all tools at once, which I find somewhat funny because it means the agent needs a search engine just to find its own capabilities.</p>

          <p>Selection quality depends heavily on query quality. My agent rewrites the user's message into a self-contained retrieval query before searching:</p>

<pre><code># "What do I do when he won't stop?"
# -&gt; "strategies for 8 year old ADHD homework meltdowns refusing to stop"

async def rewrite(self, query, conversation_history, family_profile):
    recent_turns = conversation_history[-3:]
    prompt = f"""Rewrite the search query to be self-contained.
    Resolve pronouns, add implicit context from conversation.
    Known: child_name={profile.child_name}, age={profile.child_age}

    Rules:
    - Keep it concise (under 30 words)
    - Resolve pronouns ("he" -&gt; the child's name)
    - Strip emotional language, focus on the information need"""</code></pre>

          <p>The comment at the top shows why this matters. "What do I do when he won't stop?" has zero retrieval value as-is: no subject, no age, no context. The rewriter resolves "he" to the child, infers "homework meltdowns" from the last 3 turns, and produces a query that actually hits relevant documents.</p>

          <p><strong>Compression</strong> reduces tokens while maintaining task performance. Summarization replaces older conversation history with a condensed version (Claude Code applies auto-compact at 95% context utilization). Trimming removes older messages using heuristics (drop oldest N) or trained pruners like the Provence model, which identifies which messages contribute least to task performance. One finding I keep coming back to from the recurrent context compression research, because I violated it in my first attempt: compressing instructions and context simultaneously degrades responses. You need to compress the data but preserve the instructions separately. I was summarizing entire turns including the system-injected guidance, and the model started ignoring its own rules.</p>

          <p>My agent uses a two-pass trimming strategy. First a hard message count cap, then a character budget that accounts for the system prompt size.</p>

<pre><code># Pass 1: Message count cap (6 turns = 12 messages)
max_messages = CONTEXT_WINDOW_TURNS * 2  # 6 * 2 = 12
conversation = conversation[-max_messages:]

# Pass 2: Character budget (subtract system prompt from 120K char limit)
remaining = CONTEXT_MAX_CHARS - len(system_prompt)
while estimate_chars(conversation) &gt; remaining and len(conversation) &gt; 2:
    conversation = conversation[1:]  # Drop oldest</code></pre>

          <p>The two-pass approach handles two different problems. The message cap prevents the model from getting distracted by old turns, and the character budget prevents the total context from overflowing, accounting for the fact that system prompts vary in size depending on how much family context has been accumulated.</p>

          <p><strong>Isolation</strong> means splitting information across separate processing units so each one gets a clean, focused context window. Multi-agent systems give each sub-agent its own window focused on a specific subtask, returning a condensed summary (1,000-2,000 tokens) to the lead agent. HuggingFace's CodeAgent isolates token-heavy objects in sandbox environments, keeping only references in the main context. You can also separate LLM-exposed fields from auxiliary context storage in your state schema, because not everything the system knows needs to be in the window.</p>

          <p>My agent isolates safety classification into separate LLM calls so the guardrail context never contaminates the main coaching conversation:</p>

<pre><code># Each node runs in its own isolated LLM call with its own context
graph = StateGraph(CoachingState)

graph.add_node("input_gate", input_gate_node)       # Safety classifier
graph.add_node("pro_react_agent", pro_agent)         # Main agent (complex queries)
graph.add_node("flash_react_agent", flash_agent)     # Main agent (simple messages)
graph.add_node("output_gate", output_gate_node)      # Scope validator

graph.set_entry_point("input_gate")
graph.add_conditional_edges("input_gate", route_after_input_gate, {
    END: END,                                        # Blocked -&gt; stop
    "pro_react_agent": "pro_react_agent",            # Complex -&gt; Pro + thinking
    "flash_react_agent": "flash_react_agent",        # Simple -&gt; Flash
})
graph.add_edge("pro_react_agent", "output_gate")
graph.add_edge("flash_react_agent", "output_gate")
graph.add_edge("output_gate", END)</code></pre>

          <p>The input gate sees only the latest user message and a short classification prompt. The output gate sees only the agent's response and a scope-checking prompt. Neither gate's context (safety rules, classification examples) appears in the main agent's window, keeping the coaching conversation clean and focused.</p>

          <p>If you're building an agent and something feels off about the outputs, run through these four categories and ask which one you're neglecting. In my experience the answer is almost always compression or isolation; people over-invest in writing and selecting because those feel like "building features," while trimming old history and splitting contexts feel like cleanup work.</p>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 4: How Context Fails                                 -->
        <!-- ============================================================ -->
        <section id="failure-modes">
          <h2>How Context Fails</h2>

          <p>When an agent misbehaves, my first question is always "what kind of context failure is this?" because the mitigations are completely different depending on the answer. <a href="#ref-breunig">Drew Breunig</a> laid out four failure modes that I think cover most of what goes wrong, and I've started using them as a debugging checklist.</p>

          <div class="algorithm-box">
            <h4>Four Context Failure Modes</h4>
            <table>
              <thead>
                <tr><th>Mode</th><th>Cause</th><th>Fix</th></tr>
              </thead>
              <tbody>
                <tr><td>Poisoning</td><td>Error enters context, compounds downstream</td><td>Validate before memory writes</td></tr>
                <tr><td>Distraction</td><td>History too long, drowns out training</td><td>Aggressive trimming + summarization</td></tr>
                <tr><td>Confusion</td><td>Noise treated as signal</td><td>Curate ruthlessly, earn every token</td></tr>
                <tr><td>Clash</td><td>Contradictory instructions in prompt</td><td>Clear precedence hierarchy</td></tr>
              </tbody>
            </table>
          </div>

          <p><strong>Context poisoning</strong> is the scariest one. A hallucination or error enters the context and gets repeatedly referenced, compounding mistakes over time. Once a wrong fact lands in the conversation history, the model treats it as ground truth and builds on it. Google DeepMind's Gemini 2.5 technical report showed just how bad this gets: when the "goals" section of an agent's context was poisoned, agents developed completely nonsensical strategies downstream. The fix is to validate information before writing to long-term memory, treating memory writes like database writes where you check constraints before committing.</p>

          <p><strong>Context distraction</strong> is subtler. The context grows so long that the model over-focuses on accumulated history and neglects what it learned during training. Beyond ~100k tokens, I've noticed agents tend toward repeating actions from history rather than synthesizing novel plans. Aggressive trimming and summarization help, along with actively removing completed or irrelevant sections. I suspect most people's context windows are 2-3x larger than they need to be.</p>

          <p><strong>Context confusion</strong> is what happens when superfluous information gets treated as signal because the model can't distinguish noise from relevant information when everything is dumped in together. <a href="#ref-anthropic">Anthropic's</a> guiding principle is the right one here: "find the smallest set of high-signal tokens that maximize the likelihood of your desired outcome." Every token should earn its place, and most don't.</p>

          <p><strong>Context clash</strong> is the one I find most often in my own code, probably because it's the easiest to create accidentally. It happens when new information conflicts with existing information already in the prompt, and contradictory instructions produce unpredictable behavior. My own agent has one I caught during an audit for this post. The system prompt says "When READING information you already have in the family context above, use it directly, do not re-fetch with <code>get_family_profile</code>." But <code>get_family_profile</code> is still available as a callable tool. The instruction and the tool list contradict each other. The agent sometimes calls the tool anyway, wasting a round-trip to fetch data that's already in the prompt. The fix is straightforward (remove the tool), but the clash was easy to miss because the instruction is in the <code>&lt;tools&gt;</code> section of the prompt and the tool definition is in Python code, and I never reviewed them side by side until I went looking for exactly this kind of problem.</p>

          <div class="theory-only">
            <div class="callout">
              <p><span class="mode-label">Deep Dive</span></p>
              <p>There's a deeper mechanism behind context distraction that <a href="#ref-anthropic">Anthropic</a> calls <strong>context rot</strong>. Recall accuracy diminishes as token counts increase because the number of pairwise token relationships in self-attention scales as $O(n^2)$. At 100k+ tokens, the model's attention budget is spread thin across billions of pair interactions, and critical information gets drowned out.</p>
              <p>The survey quantifies this with the <strong>lost-in-the-middle</strong> finding, which is one of those results that seems obvious in hindsight but changed how I structure prompts: performance degrades by up to 73% when relevant information sits in the middle of long contexts, compared to the beginning or end. You can't fix this because it's an architectural property of self-attention. The practical implication is that position matters. Put your most important context at the beginning and end of the window, not the middle.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 5: A Real Token Budget                               -->
        <!-- ============================================================ -->
        <section id="token-budget">
          <h2>A Real Token Budget</h2>

          <p>All of the above is easier to understand with a concrete example, so let me walk through the ADHD coaching assistant I built as a ReAct agent with LangGraph. The system prompt ranges from ~2,000 to ~4,050 tokens depending on session maturity, assembled dynamically from several blocks.</p>

          <div style="border:1px solid var(--border); border-radius:8px; padding:24px; margin:24px 0; background:#fafafa; overflow-x:auto;">
            <div style="display:flex; align-items:center; gap:0; min-width:700px; justify-content:center; flex-wrap:nowrap;">
              <div style="background:white; border:2px solid var(--border); border-radius:8px; padding:8px 12px; text-align:center; min-width:70px;">
                <div style="font-weight:600; font-size:13px;">Parent</div>
                <div style="font-size:10px; color:var(--muted);">message</div>
              </div>
              <div style="color:#bbb; font-size:18px; padding:0 6px;">&rarr;</div>
              <div style="background:#e8e5de; border:2px solid #c5c2b8; border-radius:8px; padding:8px 12px; text-align:center; min-width:70px;">
                <div style="font-weight:600; font-size:13px;">Input Gate</div>
                <div style="font-size:10px; color:var(--muted);">safety check</div>
              </div>
              <div style="color:#bbb; font-size:18px; padding:0 6px;">&rarr;</div>
              <div style="display:flex; flex-direction:column; gap:4px;">
                <div style="background:white; border:2px solid #c5c2b8; border-radius:8px; padding:6px 12px; text-align:center;">
                  <div style="font-weight:600; font-size:13px;">Pro Agent</div>
                  <div style="font-size:10px; color:var(--muted);">complex + thinking</div>
                </div>
                <div style="background:white; border:2px solid var(--border); border-radius:8px; padding:6px 12px; text-align:center;">
                  <div style="font-weight:600; font-size:13px;">Flash Agent</div>
                  <div style="font-size:10px; color:var(--muted);">simple queries</div>
                </div>
              </div>
              <div style="color:#bbb; font-size:18px; padding:0 6px;">&rarr;</div>
              <div style="background:#e8e5de; border:2px solid #c5c2b8; border-radius:8px; padding:8px 12px; text-align:center; min-width:70px;">
                <div style="font-weight:600; font-size:13px;">Output Gate</div>
                <div style="font-size:10px; color:var(--muted);">scope check</div>
              </div>
              <div style="color:#bbb; font-size:18px; padding:0 6px;">&rarr;</div>
              <div style="background:white; border:2px solid var(--border); border-radius:8px; padding:8px 12px; text-align:center; min-width:70px;">
                <div style="font-weight:600; font-size:13px;">Response</div>
              </div>
            </div>
            <div style="display:flex; justify-content:center; margin-top:12px;">
              <div style="border:1px dashed #c5c2b8; border-radius:8px; padding:8px 16px; display:flex; gap:16px; font-size:11px; color:var(--muted);">
                <span>Background: rolling summary</span>
                <span style="color:#ddd;">|</span>
                <span>profile updates</span>
                <span style="color:#ddd;">|</span>
                <span>outcome tracking</span>
              </div>
            </div>
            <div style="text-align:center; margin-top:8px; font-size:12px; color:var(--muted);">Each node runs in its own isolated LLM call with its own context window</div>
          </div>

          <table>
            <thead>
              <tr><th>Component</th><th>Tokens</th><th>Type</th></tr>
            </thead>
            <tbody>
              <tr><td>Conversation state (turn count, phase, datetime)</td><td>50-100</td><td>Dynamic (every turn)</td></tr>
              <tr><td>Identity block (personality, voice)</td><td>~350</td><td>Static</td></tr>
              <tr><td>Boundaries (5 hard scope rules)</td><td>~280</td><td>Static</td></tr>
              <tr><td>Family context (profile, summary, goals)</td><td>80-700</td><td>Dynamic (per session)</td></tr>
              <tr><td>Approach + tools + response guides</td><td>~470</td><td>Static</td></tr>
              <tr><td>Few-shot examples (6)</td><td>~550</td><td>Static</td></tr>
              <tr><td>RAG results (last 3, conditional)</td><td>0-1,500</td><td>Conditional</td></tr>
              <tr><td><strong>System prompt total</strong></td><td><strong>~2,000-4,050</strong></td><td><strong>Mixed</strong></td></tr>
              <tr><td>Conversation window (6 turns max)</td><td>500-5,000</td><td>Dynamic (rolling)</td></tr>
              <tr><td>Tool results (<code>search_knowledge_base</code>)</td><td>0-10,000</td><td>On-demand</td></tr>
            </tbody>
          </table>

          <p>The conversation window holds 6 turns max, char-budgeted at 120,000 characters. Messages from turns already covered by the rolling summary are excluded.</p>

          <p>Everything gets assembled in a single <code>prepare_context</code> hook that runs before every LLM call in the ReAct loop:</p>

<pre><code>async def prepare_context(state: CoachingState):
    session_id = state.get("session_id", "default")
    session_state = await session_store.get(session_id)
    turn_count = session_state.turn_count
    cache_key = (session_id, turn_count)

    # Skip rebuild if nothing mutated since last call
    has_mutation = _has_mutating_tool(state["messages"])
    if cache_key in _prompt_cache and not has_mutation:
        system_prompt = _prompt_cache[cache_key]
    else:
        summary = await session_store.get_latest_summary(session_id)
        episodes = await session_store.get_recent_episodes(session_id, limit=5)
        tool_results = await session_store.get_recent_tool_results(session_id, limit=3)

        system_prompt = build_system_prompt(
            profile=session_state.family_profile,
            active_strategies=session_state.active_strategies,
            goals=session_state.goals, outcomes=session_state.outcomes,
            session_summary=summary_text,
        )

        state_block = build_conversation_state(turn_count, phase, recent_tool_names, ...)
        system_prompt = state_block + "\n\n" + system_prompt  # &lt;-- problematic ordering

    # Trim conversation to fit budget
    max_messages = CONTEXT_WINDOW_TURNS * 2
    conversation = [m for m in messages if not isinstance(m, SystemMessage)]
    conversation = conversation[-max_messages:]
    while estimate_chars(conversation) &gt; remaining_budget and len(conversation) &gt; 2:
        conversation = conversation[1:]

    return {"llm_input_messages": [SystemMessage(system_prompt)] + conversation}</code></pre>

          <p>This function is where all the context engineering actually happens. Load state, build the prompt from components, prepend the conversation state block, cache the result, and trim the conversation window. Every technique I described in the earlier sections (XML structure, rolling summaries, trimming, isolation) converges in this one function. And if you noticed the <code># &lt;-- problematic ordering</code> comment, good. I'll come back to it.</p>

          <div class="algorithm-box">
            <h4>Context Engineering Techniques Used</h4>
            <table>
              <thead><tr><th>Category</th><th>Technique</th><th>Implementation</th></tr></thead>
              <tbody>
                <tr><td>Structure</td><td>XML-tagged sections</td><td><code>&lt;identity&gt;</code>, <code>&lt;boundaries&gt;</code>, <code>&lt;family-context&gt;</code>, etc.</td></tr>
                <tr><td>Compression</td><td>Rolling summary</td><td>Every 5 turns via Gemini Flash</td></tr>
                <tr><td>Compression</td><td>History trimming</td><td>6-turn window + 120K char budget</td></tr>
                <tr><td>Memory</td><td>Semantic (family profile)</td><td>Structured schema, tool-driven updates</td></tr>
                <tr><td>Memory</td><td>Episodic (outcomes)</td><td>Created on <code>track_outcome</code>, stored with emotion</td></tr>
                <tr><td>RAG</td><td>Hybrid search</td><td>Dense + sparse + RRF + cross-encoder reranking</td></tr>
                <tr><td>RAG</td><td>Query rewriting</td><td>Pronoun resolution + profile context injection</td></tr>
                <tr><td>Routing</td><td>Dual-model</td><td>Flash for simple messages, Pro with thinking for complex</td></tr>
                <tr><td>Safety</td><td>Isolated gates</td><td>Input + output classifiers in separate LLM calls</td></tr>
              </tbody>
            </table>
          </div>

          <h3>What I Got Wrong</h3>

          <p>When I audited this agent against the best practices I'd just finished researching for this post, it was humbling. I found violations of principles I had literally just written about, in my own code, while the blog post draft was still open in the next tab. Five specific problems stood out.</p>

          <p>The most expensive one, and the one I'm most embarrassed about, is a <strong>KV-cache violation</strong>. Look at the <code>prepare_context</code> code above. The volatile <code>&lt;conversation_state&gt;</code> block (which changes every turn with new turn count, new timestamp, new tool history) gets prepended to the start of the system prompt. KV-cache works by matching a prefix: if the first N tokens are identical between calls, the provider can reuse the cached key-value pairs and charge you the cached rate. By putting volatile data at the very start, every single turn invalidates the entire cache. <a href="#ref-manus">Manus</a> reports this is a 10x cost difference (<span class="nokatex">$0.30/MTok cached vs $3/MTok uncached</span> on Claude Sonnet). The fix is one line, move the state block to the end of the prompt instead of the beginning, so the static sections (identity, boundaries, examples) form a stable prefix that caches across turns.</p>

          <div style="border:1px solid var(--border); border-radius:8px; padding:24px; margin:24px 0; background:#fafafa;">
            <div style="margin-bottom:16px;">
              <div style="display:flex; align-items:center; gap:8px; margin-bottom:10px;">
                <span style="font-size:14px; font-weight:600; color:#b45555;">Before</span>
                <span style="font-size:12px; color:var(--muted);">Cache breaks on every turn</span>
              </div>
              <div style="display:flex; gap:2px; font-size:11px; border-radius:6px; overflow:hidden;">
                <div style="background:#f2d4d4; border:1px solid #d4a0a0; padding:8px 10px; min-width:100px; text-align:center; position:relative;">
                  <div style="font-weight:600;">conversation_state</div>
                  <div style="color:#b45555; font-size:10px;">volatile</div>
                </div>
                <div style="background:#eee; border:1px solid #ddd; padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">identity</div>
                  <div style="color:var(--muted); font-size:10px;">static</div>
                </div>
                <div style="background:#eee; border:1px solid #ddd; padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">boundaries</div>
                  <div style="color:var(--muted); font-size:10px;">static</div>
                </div>
                <div style="background:#eee; border:1px solid #ddd; padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">examples</div>
                  <div style="color:var(--muted); font-size:10px;">static</div>
                </div>
                <div style="background:#eee; border:1px solid #ddd; padding:8px 10px; flex:1; text-align:center; color:var(--muted);">...</div>
              </div>
              <div style="font-size:10px; color:#b45555; margin-top:4px;">&uarr; Prefix changes every turn. 0% cache hit rate.</div>
            </div>
            <div>
              <div style="display:flex; align-items:center; gap:8px; margin-bottom:10px;">
                <span style="font-size:14px; font-weight:600; color:#4a8c5c;">After</span>
                <span style="font-size:12px; color:var(--muted);">Static prefix caches across turns</span>
              </div>
              <div style="display:flex; gap:2px; font-size:11px; border-radius:6px; overflow:hidden;">
                <div style="background:#d4edda; border:1px solid #a3d1ad; padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">identity</div>
                  <div style="color:#4a8c5c; font-size:10px;">cached</div>
                </div>
                <div style="background:#d4edda; border:1px solid #a3d1ad; padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">boundaries</div>
                  <div style="color:#4a8c5c; font-size:10px;">cached</div>
                </div>
                <div style="background:#d4edda; border:1px solid #a3d1ad; padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">examples</div>
                  <div style="color:#4a8c5c; font-size:10px;">cached</div>
                </div>
                <div style="background:#d4edda; border:1px solid #a3d1ad; padding:8px 10px; flex:1; text-align:center; color:#4a8c5c;">...</div>
                <div style="background:#fff8e5; border:1px solid #e0d5a0; padding:8px 10px; min-width:100px; text-align:center;">
                  <div style="font-weight:600;">conversation_state</div>
                  <div style="color:#9a8a3c; font-size:10px;">volatile</div>
                </div>
              </div>
              <div class="nokatex" style="font-size:10px; color:#4a8c5c; margin-top:4px;">&uarr; Stable prefix. $0.30/MTok cached vs $3.00/MTok uncached (10x).</div>
            </div>
          </div>

          <p>I also have <strong>no memory validation</strong>. When the agent calls <code>update_family_profile</code>, the LLM extracts facts from the conversation ("child is 8 years old," "diagnosed last year") and writes them directly to the profile. No conflict detection (what if the profile already says the child is 7?), no enum validation (what if the LLM writes "ADHD-ish" instead of one of the valid subtypes?), no versioning (if the extraction hallucinates, the bad data overwrites the good data with no rollback). This is exactly the context poisoning failure mode from Section 4, and I built it knowing better.</p>

          <p>Then there's the <strong>redundant <code>get_family_profile</code> tool</strong>. The family profile is already injected into the <code>&lt;family-context&gt;</code> section of the system prompt on every turn. The tool exists to let the agent "check what it knows," but the agent already knows; it's in the prompt. I mentioned this in Section 4 as a context clash, and it is, but it's also context confusion in a way I didn't appreciate until I watched the agent's behavior. The tool's existence signals to the model that the profile might not be in context, which makes it less likely to trust the data it already has. The tool is actively making the agent dumber.</p>

          <p>My <strong>FIFO trimming is content-blind</strong>, which means the trimming code drops the oldest message first, regardless of what's in it. A message like "we tried the timer strategy and it made things worse" (high signal, contains an outcome) gets dropped before a filler "thanks!" or "ok sounds good" message, purely because the filler is newer. A smarter approach would score messages by information density and preserve high-signal messages even if they're older. The Provence model from the research does exactly this: it trains a pruner to identify which messages contribute least to task performance. I haven't implemented anything like that yet, but it's high on my list.</p>

          <p>And finally, <strong>no importance scoring for episodic memory</strong>. Episodes (tracked outcomes, strategy results) are retrieved by recency alone. A breakthrough moment from 3 weeks ago where a family discovered their child responds to visual timers gets ranked below a routine check-in from yesterday. The Stanford generative agents paper showed that combining recency, importance, and relevance produces much better retrieval than any single factor. My agent uses one of the three. That's going to be a significant Part 3 topic.</p>

          <div class="theory-only">
            <div class="callout">
              <p><span class="mode-label">Deep Dive</span></p>
              <p>The benchmarks suggest context engineering matters a lot more than most people realize. On GAIA, human accuracy is 92% while GPT-4 hits 15%. A 77-point gap. On GTA, GPT-4 completes fewer than 50% of tasks. On WebArena, the top agent (IBM CUGA) reaches only 61.7%. These benchmarks all require integrating information from multiple sources, using tools, and maintaining state across steps, which is exactly what context engineering addresses.</p>
              <p>Memory systems fare poorly too. LongMemEval (500 curated questions) finds 30% accuracy degradation in commercial assistants during extended interactions. GPT-4, Claude, and Llama 3.1 all struggle with episodic memory involving interconnected events, even in brief contexts. The gap between model capability on narrow benchmarks and system capability on realistic tasks is, I think, the context engineering gap.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 6: What I Learned                                    -->
        <!-- ============================================================ -->
        <section id="what-i-learned">
          <h2>What I Learned</h2>

          <p>The thing that surprised me most writing this post wasn't any individual technique. It was the gap between knowing best practices and actually following them in my own code. I'd spent weeks reading the research, writing up the failure modes, explaining why you should validate memory writes and keep tool counts low, and then I audited my own agent and found every single anti-pattern I'd warned against. The KV-cache ordering was backwards. Memory writes had no validation. A redundant tool was actively making the agent dumber. It was a useful lesson in intellectual humility, and I suspect most engineers building agents right now would find the same thing if they looked.</p>

          <p>If I had to distill this post into one actionable idea, it's that context engineering is mostly about removal, not addition. The instinct is always to add more information, more tools, more history, more instructions. But every improvement I've made to my agent involved taking something out or moving it around, not putting more in. Remove the redundant tool. Move the volatile state block to the end of the prompt. Trim history more aggressively. Compress the conversation to its information-bearing skeleton. The constraint ($|C| \leq L_\text{max}$) is real, and the best systems I've seen treat it as a design principle rather than a limitation to work around.</p>

          <p>What I deliberately didn't cover here is how to build good retrieval pipelines (Part 2) and how to design memory systems that actually improve over time rather than accumulating garbage (Part 3). Both of those are deep enough to deserve their own treatment. I'm also not confident I have the right answer on tool management yet; the tension between Inngest's "remove low-usage tools" and Manus's "keep tools for cache stability" feels unresolved to me, and my current approach (just keep tool counts low) is probably too simplistic for agents with 20+ tools.</p>

          <p>One caveat worth keeping in mind as you apply any of this. Context strategy isn't portable across providers. Different models have different attention patterns, different context window behaviors, and different sensitivities to prompt structure. What works for Claude might fail on GPT-4 might fail on Gemini. I've been burned by this enough times that I now test prompts on every model I plan to support, rather than assuming my architecture generalizes.</p>

          <p>If you spot errors or have war stories from your own context engineering work, I'd love to hear about it on <a href="#">X</a> or <a href="#">LinkedIn</a>.</p>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 7: References                                        -->
        <!-- ============================================================ -->
        <section id="references">
          <h2>References</h2>

          <ol>
            <li id="ref-karpathy">Karpathy, A. "Context Engineering." <a href="https://x.com/karpathy/status/1937902205765607626">X/Twitter</a>, June 2025.</li>
            <li id="ref-lutke">Lutke, T. "Context Engineering over Prompt Engineering." <a href="https://x.com/tobi/status/1935533422589399127">X/Twitter</a>, June 2025.</li>
            <li id="ref-schmid">Schmid, P. "Context Engineering." <a href="https://www.philschmid.de/context-engineering">philschmid.de</a>, 2025.</li>
            <li id="ref-anthropic">Rajasekaran, P. et al. "Effective Context Engineering for AI Agents." <a href="https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents">Anthropic Engineering</a>, September 2025.</li>
            <li id="ref-langchain">Martin, L. "Context Engineering for Agents." <a href="https://blog.langchain.com/context-engineering-for-agents/">LangChain Blog</a>, July 2025.</li>
            <li id="ref-breunig">Breunig, D. "How Contexts Fail and How to Fix Them." <a href="https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html">dbreunig.com</a>, June 2025.</li>
            <li id="ref-manus">Ji, Y. "Context Engineering for AI Agents: Lessons from Building Manus." <a href="https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus">manus.im</a>, July 2025.</li>
            <li id="ref-spotify">Spotify Engineering. "Context Engineering: Background Coding Agents Part 2." <a href="https://engineering.atspotify.com/2025/11/context-engineering-background-coding-agents-part-2">engineering.atspotify.com</a>, November 2025.</li>
            <li id="ref-inngest">Inngest. "Five Critical Lessons for Context Engineering." <a href="https://www.inngest.com/blog/five-lessons-for-context-engineering">inngest.com</a>, 2025.</li>
            <li id="ref-mei">Mei, Z. et al. "A Survey of Context Engineering for Large Language Models." <a href="https://arxiv.org/abs/2507.13334">arXiv:2507.13334</a>, July 2025.</li>
            <li id="ref-willison">Willison, S. "Context Engineering." <a href="https://simonwillison.net/2025/Jun/27/context-engineering/">simonwillison.net</a>, June 2025.</li>
            <li id="ref-osmani">Osmani, A. "Context Engineering: Bringing Engineering Discipline to AI." <a href="https://addyo.substack.com/p/context-engineering-bringing-engineering">Substack</a>, 2025.</li>
            <li id="ref-flowhunt">FlowHunt. "Context Engineering: The Definitive Guide." <a href="https://www.flowhunt.io/blog/context-engineering/">flowhunt.io</a>, 2025.</li>
            <li id="ref-fowler">Fowler, M. "Context Engineering for Coding Agents." <a href="https://martinfowler.com/articles/exploring-gen-ai/context-engineering-coding-agents.html">martinfowler.com</a>, 2025.</li>
          </ol>
        </section>

      </article>
    </div>

    <footer class="footer">
      <a href="#">X</a> · <a href="#">LinkedIn</a> · <a href="#">GitHub</a>
    </footer>
  </main>

  <script src="../../js/katex-init.js"></script>
  <script src="../../js/mode-toggle.js"></script>
  <script src="../../js/sticky-toc.js"></script>
</body>
</html>
