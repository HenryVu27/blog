<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Multi-Armed Bandits Part 1: Foundations · Technical Blog</title>
  <link rel="stylesheet" href="styles.css">
  <link rel="stylesheet" href="post-styles.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <style>
    /* Safari-specific KaTeX fixes */
    .katex {
      line-height: 1 !important;
    }
    
    .katex-display {
      line-height: 1 !important;
      margin: 1em 0 !important;
    }
    
    /* Fix for sigma and other large operators in Safari */
    .katex .mop {
      vertical-align: baseline !important;
    }
    
    /* Ensure proper alignment of subscripts and superscripts */
    .katex .msubsup {
      vertical-align: baseline !important;
    }
    
    /* Override global page styles for mixed layout */
    .page{
      max-width: none;
      margin: 80px 0 64px 0;
      padding: 0;
    }
    
    .page-header {
      max-width: 800px;
      margin: 0 auto;
      padding: 0 16px;
    }
    
    @media (max-width: 768px) {
      .page {
        margin: 40px 0 32px 0;
      }
      .page-header {
        padding: 0 16px;
      }
      .layout {
        display: block;
        max-width: 800px;
        padding: 0 16px;
      }
      .toc {
        position: static;
        border-right: none;
        border-bottom: 1px solid var(--border);
        padding: 0 0 16px 0;
        margin-bottom: 16px;
      }
      .toc ul {
        display: flex;
        flex-wrap: wrap;
        gap: 12px;
      }
      .toc li a {
        font-size: 16px;
        line-height: 1.4;
        white-space: nowrap;
      }
    }
    
    .layout{
      display: grid;
      grid-template-columns: 300px minmax(720px, 1000px) 1fr;
      gap: 32px;
      max-width: 1600px;
      margin: 0 auto;
      padding: 0 16px;
      align-items: start;
    }
    
    @media(min-width: 1200px){
      .layout{
        grid-template-columns: 320px minmax(800px, 1200px) 1fr;
        max-width: 1800px;
      }
    }
    .toc{
      position: static; 
      height:fit-content;
      border: none;
      border-right: 1px solid var(--border);
      border-radius: 0;
      padding: 0 16px 0 0;
      margin: 0;
      background: transparent;
    }
    @media(min-width: 1800px){
      .toc{
        padding: 0 24px 0 0;
        margin-right: 24px;
      }
    }
    .toc ul{list-style:none; padding-left:0}
    .toc li{margin:8px 0}
    .toc li a{
      display: flex;
      align-items: baseline;
      text-decoration: none;
      color: var(--text);
      font-size: 22px;
      line-height: 1.5;
    }
    .toc li a:hover{
      text-decoration: underline;
    }
    .toc-section-number{
      color: var(--text);
      font-weight: 400;
      margin-right: 8px;
      min-width: 20px;
      font-size: 22px;
    }
    @media(min-width: 1800px){
      .toc-section-number{
        margin-right: 16px;
        min-width: 28px;
      }
    }
    .toc-section-title{
      color: var(--text);
      font-weight: 400;
    }
    
    .toc h2{
      font-size: 29px;
      margin: 0 0 16px 0;
      font-weight: 600;
    }
    .series-nav {
      background: transparent;
      border: none;
      border-radius: 0;
      padding: 0;
      margin: 24px 0 0 0;
    }
    .series-nav h3 {
      margin: 0 0 12px 0;
      font-size: 22px;
    }
    .series-nav ul {
      margin: 0;
      padding-left: 20px;
    }
    .algorithm-box {
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 16px;
      margin: 16px 0;
      background: var(--code-bg);
    }
    .algorithm-box h4 {
      margin: 0 0 12px 0;
      font-size: 19px;
      font-weight: 700;
    }
    .edit-green { background: #e6ffed; border-radius: 3px; padding: 0 2px; }
    
    .content-divider {
      height: 1px;
      background-color: var(--border);
      /* Robust full-bleed divider */
      box-shadow: 0 0 0 100vmax var(--border);
      clip-path: inset(0 -100vmax);
      margin: 64px 0; /* equal spacing above and below */
    }

    /* Make the header's hr full-bleed as well */
    .page-header hr{
      border: 0;
      border-top: 1px solid var(--border);
      /* Full-bleed */
      box-shadow: 0 0 0 100vmax var(--border);
      clip-path: inset(0 -100vmax);
      margin: 32px 0; /* keeps header spacing tidy */
    }

    /* Proof popover system */
    .proof-popover-trigger{
      display: inline-block;
      background: #efeee6;
      border: 1px solid #6b6a61;
      border-radius: 8px;
      padding: 2px 8px;
      cursor: pointer;
      box-shadow: 0 1px 0 rgba(0,0,0,0.15), inset 0 0 0 1px rgba(255,255,255,0.4);
      transition: background 120ms ease, border-color 120ms ease, box-shadow 120ms ease;
    }
    .proof-popover-trigger:hover{
      background: #e8e6d9;
      border-color: #56554d;
      box-shadow: 0 2px 6px rgba(0,0,0,0.15), inset 0 0 0 1px rgba(255,255,255,0.5);
    }
    .proof-popover{
      position: fixed;
      z-index: 2000;
      display: none;
      width: 720px;
      max-width: min(98vw, 900px);
      max-height: 480px;
      overflow: auto;
      background: #ffffff;
      color: var(--text);
      border: 1px solid var(--border);
      border-radius: 8px;
      box-shadow: 0 8px 24px rgba(0,0,0,0.18);
      padding: 12px 14px;
    }
    .proof-popover.visible{ display: block; }
    .proof-popover h4{ margin: 4px 0 8px 0; font-size: 18px; }
    .proof-popover .meta{ font-size: 13px; color: var(--muted, #666); margin-bottom: 8px; }
    .proof-center{ text-align: center; margin: 12px 0; }
  </style>
</head>
<body>
  <main class="page">
    <div class="page-header">
      <nav class="nav">
        <a href="index.html" class="back-btn">← Back</a>
      </nav>

      <header>
        <h1 class="title">Multi-Armed Bandits: Foundations</h1>
        <div class="meta">2025-09-17</div>
        <hr>
      </header>

      <div class="series-nav">
        <h3>Multi-Armed Bandit Series</h3>
        <ul>
          <li><strong>Part 1: Foundations</strong> (current)</li>
          <li><a href="bandits-part2.html">Part 2: Advanced Algorithms</a></li>
          <li><a href="bandits-part3.html">Part 3: Modern Theory</a></li>
        </ul>
      </div>
    </div>

    <div class="content-divider"></div>

    <div class="layout">
      <aside class="toc">
        <h2>Contents</h2>
        <ul>
          <li><a href="#intro"><span class="toc-section-number">1</span><span class="toc-section-title">Introduction</span></a></li>
          <li><a href="#classic"><span class="toc-section-number">2</span><span class="toc-section-title">A Classic Dilemma</span></a></li>
          <li><a href="#warmup"><span class="toc-section-number">3</span><span class="toc-section-title">Finite-Armed Stochastic Bandits</span></a></li>
          <li><a href="#etc"><span class="toc-section-number">4</span><span class="toc-section-title">Explore-then-Commit</span></a></li>
          <li><a href="#ucb"><span class="toc-section-number">5</span><span class="toc-section-title">Upper Confidence Bound</span></a></li>
          <li><a href="#optimality"><span class="toc-section-number">6</span><span class="toc-section-title">Optimality and Information Theory</span></a></li>
          <li><a href="#minimax"><span class="toc-section-number">7</span><span class="toc-section-title">Minimax Lower Bounds</span></a></li>
          <li><a href="#instance"><span class="toc-section-number">8</span><span class="toc-section-title">Instance-Dependent Bounds</span></a></li>
          <li><a href="#conclusion"><span class="toc-section-number">9</span><span class="toc-section-title">Looking Ahead</span></a></li>
        </ul>
      </aside>

      <article class="post">
        <section id="intro">
          <h2>Multi-Armed Bandits and the Exploration–Exploitation Trade-off</h2>

          <p>If you have been around modern LLMs, you have probably heard of reinforcement learning. At its core, RL is an interaction loop: a learner chooses an action, receives feedback from the environment (reward), updates its rule for acting next time, and repeats. Much of today’s LLM finetuning to align with human preferences can be seen through this lens.</p>

          <p>Unlike representation learning where the learner tries to "imitate" the data, RL forces the learner to interact with the environment. The central question in this interaction is how to balance <i>exploration</i> and <i>exploitation</i>. Should the learner take advantage of current knowledge and exploit the option that currently looks best to maximize immediate reward? Or should it take some risks to explore, potentially sacrificing short-term gains to improve knowledge and long-term performance? Too much greed misses better options; too much exploration wastes opportunities.</p>

          <p>This fundamental tension between exploration and exploitation appears everywhere in sequential decision-making. Multi-armed bandits capture this trade-off in its simplest form: a single-state Markov decision process focused only on action selection under uncertainty. We now see applications of bandits across industries where information arrives in sequential manner. Examples range from ad placement, recommender systems, packet routing, to more theoretical settings like convex optimization and Brownian motion.</p>

          <p>But what are bandit problems exactly?</p>

          <p>A key early motivation came from clinical trials. <strong>William R. Thompson</strong>, in his 1933 paper, proposed assigning treatments adaptively using accumulating evidence, favoring promising options while still testing alternatives enough to learn.</p>

          <p>The name “multi-armed bandit” came later. In the 1950s, Mosteller and Bush studied learning in animals and humans. Mice faced left/right choices in a T-shaped maze with uncertain rewards. For humans, they built a two-armed machine: each lever paid out randomly with unknown probabilities. The setup echoed the “one-armed bandit” slot machine, and the term “two-armed,” then “multi-armed,” bandit stuck.</p>
        </section>

        

        <section id="classic">
          <h2>A Classic Dilemma</h2>

          <figure>
            <img src="MAB.jpg" alt="Choosing among uncertain options in a multi-armed bandit" style="max-width:100%; height:auto;" />
          </figure>

          <p>Imagine you're playing a three-armed bandit machine. You've already pulled each lever several times and observed these payoffs (in dollars):</p>

          <table style="border-collapse: collapse; margin: 16px 0;">
            <tr>
              <td style="border: 1px solid #ccc; padding: 8px; font-weight: bold;">Round</td>
              <td style="border: 1px solid #ccc; padding: 8px;">1</td>
              <td style="border: 1px solid #ccc; padding: 8px;">2</td>
              <td style="border: 1px solid #ccc; padding: 8px;">3</td>
              <td style="border: 1px solid #ccc; padding: 8px;">4</td>
              <td style="border: 1px solid #ccc; padding: 8px;">5</td>
              <td style="border: 1px solid #ccc; padding: 8px;">6</td>
            </tr>
            <tr>
              <td style="border: 1px solid #ccc; padding: 8px; font-weight: bold;">Arm 1</td>
              <td style="border: 1px solid #ccc; padding: 8px;">$5</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">$5</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">$5</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
            </tr>
            <tr>
              <td style="border: 1px solid #ccc; padding: 8px; font-weight: bold;">Arm 2</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">$0</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">$0</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
            </tr>
            <tr>
              <td style="border: 1px solid #ccc; padding: 8px; font-weight: bold;">Arm 3</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">$15</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">-</td>
              <td style="border: 1px solid #ccc; padding: 8px;">$0</td>
            </tr>
          </table>

          <p>So far, Arm 1 averages <span>$5</span>, Arm 2 averages <span>$0</span>, and Arm 3 averages <span>$7.50</span>. You have 10 more pulls remaining. Do you stick with Arm 1 (reliable <span>$5</span>), gamble on Arm 3 (high variance but higher average), or give Arm 2 another chance? How do you balance exploiting what seems best versus exploring to learn more?</p>

          <p>This illustrates the core dilemma in bandit problems: balancing <strong>exploration</strong> (trying uncertain options to gather information) and <strong>exploitation</strong> (using the currently best-performing option to maximize immediate reward).</p>
        </section>

        <section id="warmup">
          <h2>Finite-Armed Stochastic Bandits: Warming Up</h2>

          <p>Bandit theory can quickly become intimidating as you could dive arbitrarily deep into measure theory, σ-algebras, or martingales. But that isn't necessary to grasp the core ideas. Instead, let's build intuition with the simplest meaningful setting: finite-armed stochastic bandits. This framework captures the essence of exploration versus exploitation while remaining concrete enough to analyze common solution strategies.</p>

          <p>The problem statement is as follows: we have $K$ arms, where pulling arm $i$ at time $t$ yields reward $X_{i,t}$ drawn from an unknown distribution $\nu_i$ with mean $\mu_i$. The learner's goal is to minimize <em>regret</em> after $n$ rounds, which is the difference between the cumulative reward achieved by the learner, and that of an oracle that always pulls the optimal arm.</p>

          <p>Why not evaluate a learner by raw cumulative reward $\sum_{t=1}^n X_t$?</p>
          <ul>
            <li>It is random: comparing policies requires a utility for the distribution of $S_n = \sum_{t=1}^n X_t$.</li>
            <li>The instance is unknown: a policy that maximizes $\mathbb{E}[S_n]$ for one set of rewards may perform poorly on another.</li>
          </ul>
          <p><em>Regret</em> avoids both by comparing the learner to an oracle that always pulls the best arm.</p>

          <p>Define the optimal arm as $i^* = \arg\max_{i \in [K]} \mu_i$ with optimal mean $\mu^* = \mu_{i^*}$. The gap for arm $i$ is $\Delta_i = \mu^* - \mu_i$. After $n$ rounds, the cumulative regret is:</p>

          <p>$$R_n = n\mu^* - \mathbb{E}\!\left[\sum_{t=1}^n X_{I_t, t}\right]$$</p>

          <p>where $I_t$ is the arm selected at time $t$. Minimizing regret is equivalent to maximizing expected reward, but regret normalizes performance relative to the (unknown) optimum. If rewards were known, always pulling $i^*$ would give $R_n=0$.</p>
          <div class="algorithm-box">
            <h4>Regret Decomposition</h4>
            <p>Let $\Delta_i = \mu^* - \mu_i$ and let $N_i(n) = \sum_{t=1}^n \mathbb{1}_{\{I_t = i\}}$ be the (random) number of pulls of arm $i$ by round $n$. Then</p>
            <p>$$R_n = \sum_{i=1}^K \Delta_i\, \mathbb{E}[N_i(n)].$$</p>
            <p>By linearity, $\mathbb{E}[\sum_{t=1}^n X_{I_t,t}] = \sum_i \mu_i\, \mathbb{E}[N_i(n)]$.</p>
            <p>Since $R_n = n\mu^* - \mathbb{E}[\sum_{t=1}^n X_{I_t,t}]$, we get $R_n = \sum_i (\mu^* - \mu_i)\, \mathbb{E}[N_i(n)] = \sum_i \Delta_i\, \mathbb{E}[N_i(n)]$.</p>
          </div>
          <p>This identity exposes the exploration–exploitation trade-off: each pull of a suboptimal arm incurs $\Delta_i$ expected loss. Good algorithms keep $\mathbb{E}[N_i(n)]$ small for large gaps while still exploring when gaps are small, targeting sublinear regret $R_n = o(n)$. The decomposition also provides a powerful framework for algorithm analysis. Instead of directly bounding the complex random quantity $R_n$, we can analyze each $\mathbb{E}[N_i(n)]$ separately.</p>

          <p>Now let's formalize the problem setup.</p>

          <div class="algorithm-box">
            <h4>Problem Setup</h4>
            <ul>
              <li>$K$ arms with unknown reward distributions $\nu_1, \ldots, \nu_K$</li>
              <li>At round $t$, learner selects arm $I_t$ and receives reward $X_{I_t,t} \sim \nu_{I_t}$</li>
              <li>Arm $i$ has mean reward $\mu_i = \mathbb{E}[\nu_i]$</li>
              <li>Optimal arm: $i^* = \arg\max_i \mu_i$, gap: $\Delta_i = \mu^* - \mu_i$</li>
              <li>Assumption: rewards are 1-sub-Gaussian (e.g., bounded in [0,1])</li>
              <li>Goal: Minimize regret $R_n = n\mu^* - \mathbb{E}[\sum_{t=1}^n X_{I_t,t}]$</li>
            </ul>
          </div>
        </section>

        <section id="etc">
          <h2>First Steps: Explore-then-Commit</h2>

          <p>Before diving into principled algorithms, consider these simple heuristics:</p>

          <ul>
            <li><strong>Greedy:</strong> Always play the arm with the highest empirical mean. This can get stuck on a suboptimal arm due to lack of exploration, leading to linear regret.</li>
            <li><strong>Uniform exploration:</strong> Play all arms an equal number of times. This is pure exploration with no exploitation, also yielding linear regret.</li>
            <li><strong>ε-greedy:</strong> With probability $1-\epsilon$, exploit by playing the empirically best arm; with probability $\epsilon$, explore by playing a random arm. This achieves $O(\log n)$ regret with proper tuning.</li>
          </ul>

          <p>While ε-greedy can work reasonably well, it explores uniformly without considering confidence in the estimates. This motivates more sophisticated approaches.</p>

          <p>The first principled method we explore is called Explore-then-Commit. The approach is, explore each action/arm a fixed number of times ($m$), then exploit the knowledge by choosing the arm with the largest empirical payoff.</p>

          <div class="algorithm-box">
            <h4>Explore-then-Commit Algorithm</h4>
            <ol>
              <li><strong>Explore phase:</strong> Pull each arm exactly $m$ times</li>
              <li><strong>Compute empirical means:</strong> $\hat{\mu}_i = \frac{1}{m}\sum_{j=1}^m X_{i,j}$ for each arm $i$</li>
              <li><strong>Commit phase:</strong> Select $\hat{i} = \arg\max_i \hat{\mu}_i$ and pull it for all remaining $n - mK$ rounds</li>
            </ol>
          </div>

          <figure>
            <img src="ETC.png" alt="Explore-then-Commit strategy visualization" style="max-width:100%; height:auto;" />
          </figure>

          <p>To analyze the regret of ETC, recall that $R_n = \sum_{i=1}^K \Delta_i \mathbb{E}[N_i(n)]$ from the regret decomposition above. Let's denote $i^* = \arg\max_i \mu_i$ as the optimal arm.</p>

          <div class="algorithm-box">
            <h4>Theorem (ETC Regret Bound)</h4>
            <p>For the Explore-then-Commit algorithm with exploration budget $m$ per arm:</p>
            <p>$$R_n \leq m \sum_{i=1}^K \Delta_i + (n - mK) \sum_{i=1}^K \Delta_i \exp\left(-\frac{m\Delta_i^2}{4}\right)$$</p>
          </div>

          <p>Intuitively, this bound captures the exploration–exploitation balance. Choosing a large $m$ means more time sampling every arm, so the first term (proportional to $m$) grows. Picking $m$ too small raises the chance of committing to a suboptimal arm, inflating the second term, which decreases only when $m$ is large enough to make the misidentification probability tiny.</p>

          <p>To obtain a clean one-dimensional objective in $m$, specialize to the worst-case two-armed instance: set $K=2$ with $\Delta_1=0$ and denote $\Delta=\Delta_2$. Then only one suboptimal arm contributes and
          $$R_n \;\le\; m\,\Delta + (n-2m)\,\Delta\,\exp\!\left(-\tfrac{m\Delta^2}{4}\right) \;\le\; m\,\Delta + n\,\Delta\,\exp\!\left(-\tfrac{m\Delta^2}{4}\right),$$
          since $n-2m \le n$. This is the scalar function we minimize to pick $m$.</p>

          <p><strong>Proof:</strong> In the first $mK$ rounds, the policy is deterministic, choosing each action exactly $m$ times. Subsequently it chooses a single action maximizing the average reward during exploration. Thus,</p>

          <p>$$\mathbb{E}[N_i(n)] = m + (n - mK)\mathbb{P}(I_{mK+1} = i)$$</p>
          <p>$$\leq m + (n - mK)\mathbb{P}\left(\hat{\mu}_i(mK) \geq \max_{j \neq i} \hat{\mu}_j(mK)\right)$$</p>

          <p>The probability on the right-hand side is bounded by:</p>

          <p>$$\mathbb{P}\left(\hat{\mu}_i(mK) \geq \max_{j \neq i} \hat{\mu}_j(mK)\right) \leq \mathbb{P}(\hat{\mu}_i(mK) \geq \hat{\mu}_{i^*}(mK))$$</p>

          <p>This holds since beating all other arms implies beating the optimal arm $i^*$. Rewriting:</p>
          <p>$$= \mathbb{P}(\hat{\mu}_i(mK) - \mu_i - (\hat{\mu}_{i^*}(mK) - \mu_{i^*}) \geq \Delta_i)$$</p>

          <p>Then, we need to show that <span class="proof-popover-trigger" data-proof-id="etc-subg">$\hat{\mu}_i(mK) - \mu_i - (\hat{\mu}_{i^*}(mK) - \mu_{i^*})$ is $\sqrt{2/m}$-subgaussian</span>, which follows from standard subgaussian closure properties. Hence by Hoeffding's inequality:</p>
          <div id="etc-subg" style="display:none">
            <div><b>Lemma:</b> Suppose that $X$ is $\sigma$-subgaussian and $X_1$ and $X_2$ are independent and $\sigma_1$ and $\sigma_2$-subgaussian, respectively. Then:</div>
            <ol type="a" style="margin: 6px 0 10px 24px; padding-left: 0;">
              <li>$\mathbb{E}[X] = 0$ and $\mathrm{Var}[X] \le \sigma^2$.</li>
              <li>$cX$ is $|c|\sigma$-subgaussian for all $c \in \mathbb{R}$.</li>
              <li>$X_1 + X_2$ is $\sqrt{\sigma_1^2 + \sigma_2^2}$-subgaussian.</li>
            </ol>
            <p>During exploration, each arm is pulled $m$ times and rewards are independent and 1-subgaussian. The empirical mean deviation for any arm satisfies
            $$\hat\mu_i - \mu_i = \frac{1}{m} \sum_{s=1}^m (X_{i,s}-\mu_i)\quad \Rightarrow\quad \hat\mu_i - \mu_i\;\text{is}\;\frac{1}{\sqrt{m}}\text{-subgaussian}$$
            (average of $m$ independent 1-subgaussians). The same holds for the optimal arm: $\hat\mu_{i^*}-\mu_{i^*}$ is $1/\sqrt{m}$-subgaussian. These two terms are independent, so their difference has parameter
            $$\sqrt{\Big(\tfrac{1}{\sqrt{m}}\Big)^2 + \Big(\tfrac{1}{\sqrt{m}}\Big)^2}\;=\;\sqrt{\tfrac{2}{m}},$$
            i.e., $\hat\mu_i-\mu_i-(\hat\mu_{i^*}-\mu_{i^*})$ is $\sqrt{2/m}$-subgaussian. Applying the standard subgaussian tail bound (Hoeffding) gives
            $$\mathbb{P}\!\left(\hat\mu_i-\mu_i-\hat\mu_{i^*}+\mu_{i^*} \ge \Delta_i\right) \le \exp\!\left(-\frac{m\,\Delta_i^2}{4}\right),$$
            which is the term used in the ETC regret analysis.</p>
          </div>

          <p>$$\mathbb{P}(\hat{\mu}_i(mK) - \mu_i - \hat{\mu}_{i^*}(mK) + \mu_{i^*} \geq \Delta_i) \leq \exp\left(-\frac{m\Delta_i^2}{4}\right)$$</p>

          <p>Substituting this into the expression for $\mathbb{E}[N_i(n)]$ and the regret decomposition gives the regret bound.</p>

          <p>To choose the right $m$, we simplify to the basic two-arm instance: arm 1 is optimal ($\Delta_1=0$) and arm 2 is suboptimal with gap $\Delta_2=\Delta$. Then $K=2$ and the bound becomes
          $$R_n \;\le\; m\,\Delta + (n-2m)\,\Delta\,\exp\!\left(-\tfrac{m\Delta^2}{4}\right) \;\le\; m\,\Delta + n\,\Delta\,\exp\!\left(-\tfrac{m\Delta^2}{4}\right),$$
          which is a one-dimensional function of $m$ that we can minimize explicitly.</p>

          <p>The optimal choice is <span class="proof-popover-trigger" data-proof-id="etc-m-choice">$m = \max\left\{1, \left\lceil \frac{4}{\Delta^2} \log\!\left(\frac{n\Delta^2}{4}\right) \right\rceil \right\}$</span> for gap $\Delta$. For the worst-case over gaps, this gives:</p>
          <div id="etc-m-choice" style="display:none">
            <p>To select $m$, minimize a smooth upper bound on $R_n$. Using the standard two-arm simplification (or bounding $(n-mK)\le n$), write
            $$f(m) = m\,\Delta + n\,\Delta\,\exp\!\left(-\tfrac{m\Delta^2}{4}\right).$$
            Treating $m$ as real and differentiating,
            $$f'(m) = \Delta\Big[\,1 - \tfrac{n\Delta^2}{4}\,e^{-m\Delta^2/4}\Big].$$
            Setting $f'(m)=0$ gives $e^{-m\Delta^2/4}=\tfrac{4}{n\Delta^2}$ and hence
            $$m^* = \frac{4}{\Delta^2}\,\log\!\left(\frac{n\Delta^2}{4}\right).$$
            Because $m$ must be an integer and at least one pull per arm is required, we take
            $$m = \max\Big\{1,\;\big\lceil m^*\big\rceil\Big\} = \max\left\{1, \left\lceil \tfrac{4}{\Delta^2}\log\!\left(\tfrac{n\Delta^2}{4}\right) \right\rceil \right\}$$.</p>
          </div>

          <p class="proof-center"><span class="proof-popover-trigger" data-proof-id="etc-worstcase">$$R_n \leq \Delta + C\sqrt{n}$$</span></p>
          <div id="etc-worstcase" style="display:none">
            <p>From
            $$R_n \;\le\; \min\!\left\{\;n\Delta,\; \Delta + \tfrac{4}{\Delta}\Big(1+\max\{0,\log(\tfrac{n\Delta^2}{4})\}\Big)\;\right\},$$
            argue by cases:</p>
            <p><strong>1) Small gap.</strong> If $\Delta \le 2/\sqrt{n}$, then $n\Delta \le 2\sqrt{n}$, so $R_n \le n\Delta \le \Delta + 2\sqrt{n}$.</p>
            <p><strong>2) Larger gap.</strong> If $\Delta > 2/\sqrt{n}$, set $c=\Delta\sqrt{n}>2$. The second term applies since $\log(n\Delta^2/4)=\log(c^2/4)\ge0$ and becomes
            $$R_n \;\le\; \Delta + 4\sqrt{n}\,\frac{1+\log(c^2/4)}{c}.$$
            Let $g(c)=\frac{1+\log(c^2/4)}{c}$. Taking the derivative gives
            $$g'(c)=\frac{2-\big(1-\log 4\big)-2\log c}{c^2}=0\quad\Longrightarrow\quad c=2\sqrt{e},$$
            at which $g(c)=1/\!\sqrt{e}$. For all $c\ge2$, $g(c)\le 1/\!\sqrt{e}$. Hence
            $$R_n \;\le\; \Delta + \frac{4}{\sqrt{e}}\,\sqrt{n} \;\le\; \Delta + C\sqrt{n}$$
            for some constant $C$ (e.g., $C=3$).</p>
            <p>Together, both cases yield $R_n \le \Delta + C\sqrt{n}$.</p>
          </div>

          <p>where $C > 0$ is a universal constant. When $\Delta \leq 1$, we get $R_n \leq 1 + C\sqrt{n}$, or simply $R_n = O(\sqrt{n})$.</p>

          <p>The previous choice of $m$ is close to the optimal choice, but it depends on both the unknown gap $\Delta$ and the horizon $n$. A standard alternative is to pick
          $m$ as a function of horizon $n$ only. For two 1-subgaussian arms, this yields a <em>gap-free</em>
          bound $R_n = O(n^{2/3})$.</p>

          <p>Start from the two-arm bound derived above:
          $$R_n \;\le\; m\,\Delta + n\,\Delta\,\exp\!\left(-\tfrac{m\Delta^2}{4}\right).$$
          For fixed $m$, upper bound the second term uniformly over $\Delta>0$. Let $g(\Delta)=\Delta\,\exp(-\tfrac{m\Delta^2}{4})$. Then
          $$g'(\Delta) = \exp(-m\Delta^2/4)\Big(1 - \tfrac{m\Delta^2}{2}\Big)=0\;\Rightarrow\; \Delta_* = \sqrt{\tfrac{2}{m}},$$
          and
          $$\sup_{\Delta>0} g(\Delta) = g(\Delta_*) = \sqrt{\tfrac{2}{m}}\,\exp(-1/2) \;\le\; \frac{1}{\sqrt{m}}.$$
          Hence, for all $\Delta>0$,
          $$R_n \;\le\; m\,\Delta + \sqrt{2}\,\exp(-1/2)\,\frac{n}{\sqrt{m}}.$$
          Choosing $m = \lceil n^{2/3} \rceil$ gives
          $$R_n \;\le\; \Delta\,n^{2/3} + \sqrt{2}\,\exp(-1/2)\,n^{2/3} + O(\Delta),$$
          which is at most $(\Delta + C)\,n^{2/3}$ for some constant $C$. In other words, $R_n = O(n^{2/3})$.</p>
        </section>

        <section id="ucb">
          <h2>The Upper Confidence Bound (UCB) Algorithm</h2>

          <p>UCB elegantly addresses ETC's limitations by adaptively balancing exploration and exploitation. Instead of rigid phases, UCB constructs confidence intervals around arm means and selects the arm with highest upper confidence bound, a principle which we call <em>optimism under uncertainty</em>.</p>

          <p>At time $t$, having selected arm $i$ a total of $N_i(t-1)$ times and observed empirical mean $\hat{\mu}_i(t-1)$, UCB selects:</p>

          <p class="proof-center"><span class="proof-popover-trigger" data-proof-id="ucb-derivation">$$I_t = \arg\max_{i \in [K]} \left( \hat{\mu}_i(t-1) + \sqrt{\frac{2\log t}{N_i(t-1)}} \right)$$</span></p>
          <div id="ucb-derivation" style="display:none">
            <div><b>Where does this index come from?</b> Fix an arm $i$ and suppose the rewards $\{X_{i,s}\}_{s\ge1}$ are independent and 1-sub-Gaussian with mean $\mu_i$. If $\hat\mu_i(n)=\tfrac{1}{n}\sum_{s=1}^n X_{i,s}$ denotes the empirical mean after $n$ samples, the standard sub-Gaussian tail bound implies that for any $\delta\in(0,1)$,</div>
            <p>$$\mathbb{P}\!\left(\mu_i \ge \hat\mu_i(n) + \sqrt{\frac{2\log(1/\delta)}{n}}\right) \le \delta.$$</p>
            <p>At round $t$, the number of observations for arm $i$ is $n=N_i(t-1)$. Therefore, with probability at least $1-\delta$ we have</p>
            <p>$$\mu_i \le \hat\mu_i(t-1) + \sqrt{\frac{2\log(1/\delta)}{N_i(t-1)}}.$$
            This upper endpoint of a $(1-\delta)$-confidence interval is an optimistic estimate of $\mu_i$. To make the guarantee hold uniformly over time and arms, we use a time-dependent level $\delta_t$ (e.g., $\delta_t=1/t$ or $\delta_t=1/t^2$) and define the index</p>
            <p>$$\mathrm{UCB}_i(t) \;=\; \hat\mu_i(t-1) + \sqrt{\frac{2\log(1/\delta_t)}{N_i(t-1)}},$$</p>
            <p>with the convention $\mathrm{UCB}_i(t)=+\infty$ when $N_i(t-1)=0$ so each arm is sampled at least once. Choosing $\delta_t=1/t$ yields the expression used above.</p>
          </div>

          <p><strong>Exploitation vs. exploration:</strong> The term $\hat{\mu}_i(t-1)$ is the <em>exploitation</em> component, favoring arms that have produced large average rewards so far. The square-root term is the <em>exploration bonus</em> (confidence width): it is larger for arms with few samples ($N_i(t-1)$ small) and grows slowly with time through $\log(t)$, encouraging additional pulls until the estimate is accurate.</p>

          <p>The expression inside the argmax is called the <em>index</em> of arm $i$. Index algorithms form a broad class of bandit strategies where each arm is assigned a numerical score (the index) based on its historical performance and uncertainty. The algorithm then simply selects the arm with the highest index at each round. Intuitively, this leads to sublinear regret because when the optimal arm's index overestimates its true mean, any suboptimal arm must achieve an index value exceeding the optimal arm's index to be selected. However, this can't happen too often as the learner continues to explore the environment and update its estimates.</p>

          <p> The UCB algorithm is defined as follows:</p>
          <div class="algorithm-box">
            <h4>UCB Algorithm</h4>
            <p><strong>Initialization:</strong> Pull each arm once</p>
            <p><strong>For rounds $t = K+1, K+2, \ldots, n$:</strong></p>
            <ol>
              <li>Compute UCB for each arm: $UCB_i(t) = \hat{\mu}_i(t-1) + \sqrt{\frac{2\log t}{N_i(t-1)}}$</li>
              <li>Select arm: $I_t = \arg\max_i UCB_i(t)$</li>
              <li>Observe reward and update estimates</li>
            </ol>
          </div>

          <p>UCB's regret analysis relies on optimism under uncertainty. The algorithm maintains confidence intervals such that with high probability, the true arm means lie within these intervals. When sufficient evidence accumulates (tight confidence intervals), the optimal arm will have the highest UCB and be selected most often.</p>

          <p>The key lemma bounds how often UCB can select suboptimal arms:</p>

          <p><strong>Lemma:</strong> For any suboptimal arm $i$ with gap $\Delta_i > 0$, the number of times arm $i$ is selected satisfies:</p>

          <p>$$\mathbb{E}[N_i(n)] \leq \frac{8\log n}{\Delta_i^2} + 1 + \frac{\pi^2}{3}$$</p>

          <p>This leads to UCB's regret bound:</p>

          <p>$$\mathbb{E}[R_n] \leq \sum_{i: \Delta_i > 0} \Delta_i \left( \frac{8\log n}{\Delta_i^2} + 1 + \frac{\pi^2}{3} \right) = \sum_{i: \Delta_i > 0} \left( \frac{8\log n}{\Delta_i} + O(1) \right)$$</p>

          <p>Crucially, this scales as $O(\log n / \Delta_i)$ for each suboptimal arm, automatically adapting to problem difficulty. Arms with large gaps are quickly identified and avoided, while arms with small gaps are selected more often - exactly the desired behavior when arms have similar performance.</p>
        </section>

        <section id="optimality">
          <h2>Optimality Concepts and Information Theory</h2>

          <p>To understand whether UCB is optimal, we need information-theoretic lower bounds. The fundamental question: what is the minimum regret any algorithm must accept when balancing exploration with exploitation?</p>

          <p>The key insight comes from statistical hypothesis testing. Consider two bandit instances that are statistically "close" but have different optimal arms. An algorithm must distinguish between these instances, requiring sufficient data from each arm. This fundamental statistical requirement leads to unavoidable limits on learning efficiency.</p>

          <p>Let $\mathcal{E} = \{\nu_1, \ldots, \nu_K\}$ be a bandit instance. The Lai–Robbins instance-dependent lower bound states that for any consistent algorithm and each suboptimal arm $i$:</p>

          <p>$$\liminf_{n \to \infty} \frac{\mathbb{E}[N_i(n)]}{\log n} \geq \frac{1}{\text{KL}(\nu_i, \nu_{i^*})}.$$</p>

          <p>For Gaussian rewards with unit variance, $\text{KL}(\mathcal{N}(\mu_i,1),\mathcal{N}(\mu^*,1)) = \Delta_i^2/2$, so asymptotically $\mathbb{E}[N_i(n)] \gtrsim \frac{2\log n}{\Delta_i^2}$. This quantifies instance difficulty via KL; problems with many small gaps are inherently harder.</p>

          <p>The information-theoretic approach uses the Kullback-Leibler (KL) divergence between reward distributions. For distributions $P$ and $Q$:</p>

          <p>$$\text{KL}(P, Q) = \int p(x) \log \frac{p(x)}{q(x)} dx$$</p>

          <p>KL divergence measures the difficulty of distinguishing between distributions. Smaller KL divergence means harder discrimination, requiring more samples.</p>

          <p>The change-of-measure technique relates regret to KL divergence. Consider an algorithm $\pi$ and two bandit instances $\mathcal{E}$ and $\mathcal{E}'$ differing only in arms $i$ and $j$. If instance $\mathcal{E}$ has optimal arm $i$ and instance $\mathcal{E}'$ has optimal arm $j$, then:</p>

          <p>$$\mathbb{E}_{\mathcal{E}}[N_j(n)] \cdot \text{KL}(\nu_j, \nu_j') + \mathbb{E}_{\mathcal{E}}[N_i(n)] \cdot \text{KL}(\nu_i, \nu_i') \geq \text{kl}(\epsilon, 1-\epsilon)$$</p>

          <p>where $\epsilon$ is the probability of misidentifying the optimal arm, and $\text{kl}(p,q) = p\log(p/q) + (1-p)\log((1-p)/(1-q))$ is the binary KL divergence.</p>

          <p>This inequality captures the fundamental statistical trade-off: to achieve low error probability $\epsilon$ in arm identification, we must pull arm $i$ many times (if $\text{KL}(\nu_i, \nu_i')$ is small) or pull arm $j$ many times (if $\text{KL}(\nu_j, \nu_j')$ is small). This mathematical constraint reflects the statistical requirements for reliable learning.</p>
        </section>

        <section id="minimax">
          <h2>More Information Theory and Minimax Lower Bounds</h2>

          <p>The minimax lower bound provides the worst-case regret any algorithm must suffer. It's derived by constructing a particularly challenging family of bandit instances.</p>

          <p><strong>Theorem (Minimax Lower Bound):</strong> For any algorithm and horizon $n \geq K$:</p>

          <p>$$\inf_{\pi} \sup_{\mathcal{E}} \mathbb{E}_{\mathcal{E}}^{\pi}[R_n] \geq c\,\sqrt{K n}$$</p>

          <p>for a universal constant $c>0$. One standard construction takes $K-1$ arms with mean $1/2 - \Delta$ and one arm with mean $1/2 + \Delta$, with $\Delta$ on the order of $\sqrt{K/n}$.</p>

          <p>The proof uses Fano's inequality, a fundamental result in information theory. For a parameter estimation problem with $M$ hypotheses:</p>

          <p>$$\mathbb{P}[\hat{\theta} \neq \theta] \geq 1 - \frac{I(X; \theta) + \log 2}{\log M}$$</p>

          <p>where $I(X; \theta)$ is the mutual information between observations $X$ and parameter $\theta$.</p>

          <p>Applied to bandits, Fano's inequality lower bounds the probability of misidentifying the optimal arm, which directly translates to regret. The construction ensures that:</p>
          <ol>
            <li>The arms are sufficiently similar that many pulls are needed to distinguish them</li>
            <li>The gaps are large enough that mistakes incur significant regret</li>
            <li>The time horizon is short enough that perfect identification is impossible</li>
          </ol>

          <p>Comparing with UCB's minimax regret bound of $O(\sqrt{K n \log n})$, we see that UCB is near-minimax optimal, suffering only an extra $\sqrt{\log n}$ factor. Algorithms like MOSS achieve minimax-optimal $O(\sqrt{K n})$ rates (up to constants), and KL-UCB attains optimal instance-dependent constants.</p>

          <div class="algorithm-box">
            <h4>Key Results Summary</h4>
            <ul>
              <li><strong>ETC:</strong> $O\!\left(\frac{K\log n}{\Delta_{\min}}\right)$ regret (requires gap knowledge)</li>
              <li><strong>UCB1 (Hoeffding):</strong> $O\!\left(\sum_{i:\Delta_i>0} \frac{\log n}{\Delta_i}\right)$ regret</li>
              <li><strong>Minimax lower bound:</strong> $\Omega(\sqrt{K n})$</li>
              <li><strong>UCB1 minimax regret:</strong> $O(\sqrt{K n \log n})$; MOSS achieves $O(\sqrt{K n})$</li>
            </ul>
          </div>
        </section>

        <section id="instance">
          <h2>Instance Dependent Lower Bounds</h2>

          <p>While minimax bounds characterize worst-case performance, instance-dependent bounds reveal that UCB is actually optimal for specific problem instances.</p>

          <p><strong>Theorem (Lai-Robbins Lower Bound):</strong> For any consistent algorithm (one whose regret grows sublinearly), and any bandit instance with gaps $\Delta_i > 0$:</p>

          <p>$$\liminf_{n \to \infty} \frac{\mathbb{E}[N_i(n)]}{\log n} \geq \frac{1}{\text{KL}(\nu_i, \nu_{i^*})}$$</p>

          <p>Here, consistency means $\mathbb{E}[R_n] = o(n^{\alpha})$ for all $\alpha > 0$. The bound states that any reasonable algorithm must pull suboptimal arm $i$ at least $\frac{\log n}{\text{KL}(\nu_i, \nu_{i^*})}$ times.</p>
          <p><span class="edit-green">This lower bound holds under standard regularity conditions (e.g., one-parameter exponential families with mild smoothness).</span></p>

          <p>For Gaussian rewards with variance 1, $\text{KL}(\mathcal{N}(\mu_i, 1), \mathcal{N}(\mu^*, 1)) = \frac{(\mu^* - \mu_i)^2}{2} = \frac{\Delta_i^2}{2}$. This gives:</p>

          <p>$$\liminf_{n \to \infty} \frac{\mathbb{E}[N_i(n)]}{\log n} \geq \frac{2}{\Delta_i^2}$$</p>

          <p>Comparing with UCB's bound $\mathbb{E}[N_i(n)] \leq \frac{8\log n}{\Delta_i^2} + O(1)$, we see UCB achieves the optimal rate up to constants!</p>

          <p>The Lai-Robbins proof employs the method of mixtures and change of measure. The key insight is that to distinguish between the true instance and a "confusing" alternative where arm $i$ is optimal, the algorithm must observe enough samples to resolve the statistical uncertainty.</p>

          <p>Specifically, consider two instances:</p>
          <ul>
            <li>$\mathcal{E}$: True instance with optimal arm $i^*$</li>
            <li>$\mathcal{E}'$: Modified instance where arm $i$ is optimal (swapping means of arms $i$ and $i^*$)</li>
          </ul>

          <p>If the algorithm pulls arm $i$ too few times, it cannot distinguish these instances, leading to suboptimal performance on one of them. The number of required samples is determined by the KL divergence between the arm distributions.</p>

          <p>This establishes UCB as an optimal algorithm for the stochastic bandit problem, achieving both minimax optimality (up to log factors) and instance-dependent optimality.</p>
        </section>

        <section id="conclusion">
          <h2>Looking Ahead</h2>

          <p>We've established the foundational elements of bandit theory: the exploration-exploitation trade-off, algorithms like ETC and UCB for optimal arm selection, and the information-theoretic limits that govern learning. Key takeaways include:</p>

          <ul>
            <li>Regret decomposes as $\sum_i \Delta_i \mathbb{E}[N_i(n)]$, quantifying loss from selecting suboptimal arms</li>
            <li>UCB achieves optimal instance-dependent regret through adaptive confidence intervals, embodying optimism under uncertainty</li>
            <li>Information theory provides fundamental limits on how quickly we can identify optimal arms</li>
            <li>Instance-dependent regret can scale as $\sum_{i:\Delta_i>0} O(\log n/\Delta_i)$; worst-case minimax regret scales as $\Theta(\sqrt{K n})$.</li>
          </ul>

          <p>This foundation prepares us for advanced topics. In <a href="bandits-part2.html">Part 2</a>, we'll explore adversarial settings where rewards can adapt to our actions, contextual bandits that incorporate side information, and structured approaches where rewards have known relationships. In <a href="bandits-part3.html">Part 3</a>, we'll dive into modern theory including sparse linear bandits, refined statistical bounds, and the elegant duality between Bayesian and frequentist approaches.</p>

          <p>The mathematical machinery we've developed - concentration inequalities, confidence sets, information-theoretic arguments - forms the backbone of modern bandit algorithms, extending from simple comparisons to complex structured problems.</p>
        </section>

        <hr>

        <section>
          <h2>References</h2>
          <ul>
            <li>Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. <em>Biometrika</em>, 25(3/4), 285-294.</li>
            <li>Lai, T. L., & Robbins, H. (1985). Asymptotically efficient adaptive allocation rules. <em>Advances in Applied Mathematics</em>, 6(1), 4-22.</li>
            <li>Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. <em>Machine Learning</em>, 47(2-3), 235-256.</li>
            <li>Bubeck, S., & Cesa-Bianchi, N. (2012). Regret analysis of stochastic and nonstochastic multi-armed bandit problems. <em>Foundations and Trends in Machine Learning</em>, 5(1), 1-122.</li>
            <li>Lattimore, T., & Szepesvári, C. (2020). <em>Bandit Algorithms</em>. Cambridge University Press.</li>
            <li>Garivier, A., & Cappé, O. (2011). The KL-UCB algorithm for bounded stochastic bandits and beyond. <em>COLT</em>, 359-376.</li>
            <li>Berry, D. A., & Fristedt, B. (1985). <em>Bandit Problems: Sequential Allocation of Experiments</em>. Chapman and Hall.</li>
            <li>Thall, P. F., & Wathen, J. K. (2007). Practical Bayesian adaptive randomisation in clinical trials. <em>European Journal of Cancer</em>, 43(6), 859-866.</li>
          </ul>
        </section>
      </article>
    </div>

    <footer class="footer">Technical Blog · <a href="bandits-part2.html">Next: Advanced Algorithms →</a></footer>
  </main>

  <!-- Proof Popover Container -->
  <div class="proof-popover" id="proof-popover" role="dialog" aria-hidden="true">
    <div id="proof-popover-content"></div>
  </div>

  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        delimiters: [
          {left: "$$", right: "$$", display: true},
          {left: "$", right: "$", display: false}
        ],
        ignoredTags: ["script", "noscript", "style", "textarea", "pre", "code"]
      });

      // Proof popover controller
      const popover = document.getElementById('proof-popover');
      const contentEl = document.getElementById('proof-popover-content');
      let activeTrigger = null;
      let hideTimer = null;

      function clearHideTimer(){ if(hideTimer){ clearTimeout(hideTimer); hideTimer = null; } }

      function startHideTimer(){
        clearHideTimer();
        hideTimer = setTimeout(() => {
          hidePopover();
        }, 2000);
      }

      function showPopoverFor(trigger){
        clearHideTimer();
        activeTrigger = trigger;
        const proofId = trigger.getAttribute('data-proof-id');
        const hidden = document.getElementById(proofId);
        if(!hidden){ return; }
        contentEl.innerHTML = hidden.innerHTML;

        // Render KaTeX inside the popover content
        try {
          renderMathInElement(contentEl, {
            delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
            ],
            ignoredTags: ["script", "noscript", "style", "textarea", "pre", "code"]
          });
        } catch(e) { /* no-op */ }

        // Make visible to measure and position
        popover.classList.add('visible');
        popover.style.visibility = 'hidden';

        positionPopover(trigger);

        popover.style.visibility = '';
        popover.setAttribute('aria-hidden', 'false');
      }

      function positionPopover(trigger){
        const rect = trigger.getBoundingClientRect();
        const spacing = 8;
        const maxLeft = window.innerWidth - popover.offsetWidth - 12;
        let left = Math.min(rect.left, maxLeft);
        left = Math.max(12, left);

        let top = rect.bottom + spacing;
        if (top + popover.offsetHeight > window.innerHeight - 12) {
          top = rect.top - popover.offsetHeight - spacing;
        }
        if (top < 12) { top = 12; }

        popover.style.left = left + 'px';
        popover.style.top = top + 'px';
      }

      function hidePopover(){
        popover.classList.remove('visible');
        popover.setAttribute('aria-hidden', 'true');
        activeTrigger = null;
      }

      // Wire up triggers
      const triggers = Array.from(document.querySelectorAll('.proof-popover-trigger'));
      triggers.forEach(tr => {
        tr.addEventListener('click', (e) => {
          e.preventDefault();
          e.stopPropagation();
          if (popover.classList.contains('visible') && activeTrigger === tr) {
            hidePopover();
          } else {
            showPopoverFor(tr);
          }
        });
        tr.addEventListener('mouseenter', clearHideTimer);
        tr.addEventListener('mouseleave', startHideTimer);
      });

      // Hover persistence on popover
      popover.addEventListener('mouseenter', clearHideTimer);
      popover.addEventListener('mouseleave', startHideTimer);

      // Close on outside click or Escape (immediate close)
      document.addEventListener('click', (e) => {
        if (!popover.contains(e.target) && !e.target.closest('.proof-popover-trigger')) {
          hidePopover();
        }
      });
      document.addEventListener('keydown', (e) => {
        if(e.key === 'Escape') hidePopover();
      });

      // Reposition on resize
      window.addEventListener('resize', () => {
        if (activeTrigger && popover.classList.contains('visible')) {
          positionPopover(activeTrigger);
        }
      });
    });
  </script>
</body>
</html>