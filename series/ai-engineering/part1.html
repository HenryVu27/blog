<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>What Fills the Context Window · Technical Blog</title>
  <link rel="stylesheet" href="../../css/styles.css">
  <link rel="stylesheet" href="../../css/post-styles.css">
  <link rel="stylesheet" href="../../css/deep-dive-layout.css">
  <link rel="stylesheet" href="../../css/components.css">
  <link rel="stylesheet" href="../../css/mode-toggle.css">
  <script>try{if(localStorage.getItem('blog-reading-mode')==='deep-dive')document.documentElement.classList.add('deep-dive-mode')}catch(e){}</script>
  <!-- KaTeX for Deep Dive math -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
</head>
<body>
  <main class="page">
    <div class="page-header">
      <nav class="nav">
        <a href="../../index.html" class="back-btn">← Back</a>
      </nav>

      <header>
        <h1 class="title">What Fills the Context Window</h1>
        <div class="meta">
          2026-02-23 · <span class="deep-dive-only">20 min read</span><span class="tldr-only">5 min read</span>
        </div>
        <div class="mode-toggle">
          <button data-mode="deep-dive">Deep Dive</button>
          <button data-mode="tldr" class="active">TLDR</button>
        </div>
        <hr>
      </header>

      <div class="series-nav">
        <h3>AI Engineering Series</h3>
        <ul>
          <li>Part 1: What Fills the Context Window (current)</li>
          <li>Part 2: RAG, From Naive to Agentic (coming soon)</li>
          <li>Part 3: Memory Engineering (coming soon)</li>
          <li>Part 4: Building AI Agents (coming soon)</li>
          <li>Part 5: Multi-Agent Systems (coming soon)</li>
        </ul>
      </div>
    </div>

    <div class="content-divider"></div>

    <div class="layout">
      <aside class="toc">
        <div class="toc-sticky">
          <h2>Contents</h2>
          <ul>
            <li class="deep-dive-only"><a href="#from-prompts-to-context"><span class="toc-section-number">1</span><span class="toc-section-title">From Prompts to Context</span></a></li>
            <li><a href="#seven-components"><span class="toc-section-number">2</span><span class="toc-section-title">The Seven Components</span></a></li>
            <li><a href="#four-strategies"><span class="toc-section-number">3</span><span class="toc-section-title">Write / Select / Compress / Isolate</span></a></li>
            <li><a href="#failure-modes"><span class="toc-section-number">4</span><span class="toc-section-title">How Context Fails</span></a></li>
            <li><a href="#token-budget"><span class="toc-section-number">5</span><span class="toc-section-title">A Real Token Budget</span></a></li>
            <li><a href="#what-i-learned"><span class="toc-section-number">6</span><span class="toc-section-title">What I Learned</span></a></li>
            <li class="deep-dive-only"><a href="#references"><span class="toc-section-number">7</span><span class="toc-section-title">References</span></a></li>
          </ul>
        </div>
      </aside>

      <article class="post">

        <!-- ============================================================ -->
        <!-- SECTION 1: From Prompts to Context                           -->
        <!-- ============================================================ -->
        <section id="from-prompts-to-context" class="deep-dive-only">
          <h2>From Prompts to Context</h2>

          <p>Sometime around mid-2025, the AI engineering community started calling what we do "context engineering" instead of "prompt engineering," and the rename stuck because it described what I was actually spending my time on. Not crafting the perfect instruction phrasing. Figuring out what information the model should see at each step, and building the infrastructure to assemble it reliably.</p>

          <p><a href="#ref-karpathy">Andrej Karpathy</a> called it "the delicate art and science of filling the context window with just the right information for the next step." I like that framing because it puts the emphasis on information selection, not wordsmithing.</p>

          <p>You could dismiss the rename as a branding exercise, same job with a fancier title. But the dominant failure mode actually shifted. Context windows went from 4K to 200K tokens. Base models got dramatically smarter. Hallucination rates dropped. By 2025, the models are capable enough that the bottleneck moved upstream: most agent failures I encounter are context failures, where the model can do what I need but doesn't have the right information when it needs it.</p>

          <p>The system prompt is one of seven things that fill the context window, and for a simple classification task, careful prompt engineering is all you need. But the moment you add retrieval, tools, multi-step reasoning, or agent workflows, you're doing context engineering whether you call it that or not. Context engineering is the bigger tent that prompt engineering lives inside.</p>

          <p>This post walks through those seven components, the strategies for managing them, how they fail, and what a real token budget looks like in a diabetes management coaching agent I built with LangGraph (with code). I'll go deeper on RAG in Part 2, memory in Part 3, agents in Part 4, and multi-agent coordination in Part 5.</p>

          <div class="theory-only">
            <div class="callout">
              <p><span class="mode-label">Deep Dive</span></p>
              <p><a href="#ref-mei">Mei et al.'s 166-page survey</a> (arXiv:2507.13334, analyzing 1,411 papers) provides the first mathematical formalization. They model context as a structured assembly of six typed components:</p>
              <p>$$C = A(c_\text{instr},\; c_\text{know},\; c_\text{tools},\; c_\text{mem},\; c_\text{state},\; c_\text{query})$$</p>
              <p>Context engineering then becomes a constrained optimization problem, where you find the assembly function $F$ that maximizes expected reward across tasks, subject to a hard window size limit:</p>
              <p>$$F^* = \arg\max_F \; \mathbb{E}_{\tau \sim \mathcal{T}} \left[\text{Reward}\!\left(P_\theta(Y \mid C_{F}(\tau)),\; Y^*_\tau\right)\right] \quad \text{s.t.} \;\; |C| \leq L_\text{max}$$</p>
              <p>Now, I'll be honest, my first reaction to this was that it's a classic case of ML researchers dressing up practitioner intuition in math to make it paper-worthy. "Maximize reward subject to a token budget" is just "put good stuff in, leave bad stuff out" wearing a tuxedo. But the formalization actually helped me think more clearly about one thing: every token you spend on tool definitions is a token you can't spend on conversation history, and that tradeoff is zero-sum in a way my gut wasn't fully appreciating before I saw it written as a constraint.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 2: The Seven Components of Context                   -->
        <!-- ============================================================ -->
        <section id="seven-components">
          <h2>The Seven Components</h2>

          <div class="tldr-only">
            <p>Seven components fill every context window: system prompt, user prompt, state/conversation history, long-term memory, retrieved information (RAG), tool definitions, and structured output. Version system prompts like code. Use pull-based RAG (model requests via tools) over front-loading. Full conversation dumps hurt performance; trim aggressively. Tool definitions consume tokens whether used or not.</p>
          </div>

          <div class="deep-dive-only">
          <p>Every LLM call consumes a context window, a fixed-size buffer of tokens containing everything the model can see. What you put in that buffer determines what the model can do. Several people have converged on roughly the same decomposition (<a href="#ref-schmid">Philipp Schmid</a>, <a href="#ref-anthropic">Anthropic</a>, <a href="#ref-langchain">LangChain</a>, Google), and I've found seven components to be the right granularity. I'll walk through each one with code from my diabetes coaching agent, but what I want to emphasize is how unevenly they matter in practice.</p>
          </div>

          <div style="border:1px solid var(--border); border-radius:8px; padding:20px 24px; margin:24px 0; background:#fafafa;">
            <div style="display:flex; align-items:center; gap:8px; margin-bottom:14px;">
              <span style="font-size:12px; font-weight:500; text-transform:uppercase; letter-spacing:0.05em; color:var(--muted);">Context Window</span>
              <span style="flex:1; border-top:1px solid var(--border);"></span>
              <span style="font-size:12px; color:var(--muted);">8K-200K tokens</span>
            </div>
            <div style="display:flex; flex-direction:column; gap:3px; font-size:13px;">
              <div style="display:flex; align-items:center; background:#eeedea; border-radius:4px; padding:8px 12px;">
                <span style="flex:1;">System prompt</span>
                <span style="color:var(--muted); font-size:12px;">500-2,000</span>
              </div>
              <div style="display:flex; align-items:center; background:#eeedea; border-radius:4px; padding:6px 12px;">
                <span style="flex:1;">User prompt</span>
                <span style="color:var(--muted); font-size:12px;">variable</span>
              </div>
              <div style="display:flex; align-items:center; background:#eeedea; border-radius:4px; padding:14px 12px;">
                <span style="flex:1;">State + conversation history</span>
                <span style="color:var(--muted); font-size:12px;">500-5,000</span>
              </div>
              <div style="display:flex; align-items:center; background:#eeedea; border-radius:4px; padding:8px 12px;">
                <span style="flex:1;">Long-term memory</span>
                <span style="color:var(--muted); font-size:12px;">variable</span>
              </div>
              <div style="display:flex; align-items:center; background:#eeedea; border-radius:4px; padding:20px 12px;">
                <span style="flex:1;">Retrieved information (RAG)</span>
                <span style="color:var(--muted); font-size:12px;">0-10,000</span>
              </div>
              <div style="display:flex; align-items:center; background:#eeedea; border-radius:4px; padding:10px 12px;">
                <span style="flex:1;">Tool definitions</span>
                <span style="color:var(--muted); font-size:12px;">500-3,000</span>
              </div>
              <div style="display:flex; align-items:center; background:#eeedea; border-radius:4px; padding:6px 12px;">
                <span style="flex:1;">Structured output</span>
                <span style="color:var(--muted); font-size:12px;">50-500</span>
              </div>
            </div>
          </div>

          <div class="deep-dive-only">
          <p>Of the seven, the <strong>system prompt</strong> is where most people start, and rightfully so. It sets behavioral guidelines, role definitions, and rules for your agent. What I didn't expect when I started building agents is how much the system prompt wants to grow. Every failure mode you encounter tempts you to add another instruction, and before long you're at 4,000 tokens of rules that sometimes contradict each other. I've learned (the hard way) to start minimal and iteratively add instructions based on observed failures, which is essentially what <a href="#ref-anthropic">Anthropic</a> recommends as finding the "right altitude" between specificity and flexibility. <a href="#ref-spotify">Spotify's engineering team</a> went even further and found that larger, static, version-controlled prompts proved more predictable than dynamic tool-based approaches. I treat my system prompts as code now: versioned, reviewed, tested.</p>

          <p>My agent's system prompt is broken into XML-tagged sections, each responsible for a distinct behavioral concern:</p>
          </div>

<pre><code>&lt;identity&gt;
  Personality, voice, coach name
&lt;/identity&gt;

&lt;boundaries&gt;
  5 hard scope rules + output gate awareness
  "If your response recommends specific medications...
   the entire response will be discarded."
&lt;/boundaries&gt;

&lt;patient-context&gt;
  Structured profile, session summary, goals
&lt;/patient-context&gt;

&lt;approach&gt;
  Progressive profiling, evidence-based guidance
&lt;/approach&gt;

&lt;tools&gt;
  When to search, when to update profile
&lt;/tools&gt;

&lt;response-guide&gt;
  Adaptive length, tone, situation matching
&lt;/response-guide&gt;

&lt;examples&gt;
  6 few-shot patient coaching conversations
&lt;/examples&gt;</code></pre>

          <div class="deep-dive-only">
          <p>I started using XML tags mostly for my own sanity (it's easier to review a prompt when you can collapse sections), but it turns out LLMs respond measurably better to structured input than unstructured dumps. Both <a href="#ref-anthropic">Anthropic</a> and Google recommend structured delimiters for this reason. The tags also give you a natural unit for version control diffs, which matters more than you'd think once your prompt is 2,000+ tokens and three people are editing it.</p>

          <p>I spend the least time worrying about the <strong>user prompt</strong> because it's the one thing I don't control. The immediate message from the human can be anything, from well-structured to incoherent, concise to rambling. The rest of your context engineering has to be robust enough to handle whatever arrives.</p>

          <p>Where I've seen the most waste is <strong>state and short-term history</strong>, the current conversation turns and prior exchanges that serve as the working memory of your system. Most implementations (my first version included) just dump the raw history in verbatim, greetings, acknowledgments, off-topic tangents, all of it. The <a href="#ref-longmemeval">LongMemEval benchmark</a> (Wu et al., ICLR 2025) showed that models given the full ~113K-token conversation history performed worse than models given only the relevant subset, which tells you everything about the cost of unfocused context. What you remove from history matters at least as much as what you keep.</p>

          <p>My agent injects a compact state block that gives the model temporal awareness and continuity across the ReAct loop:</p>
          </div>

<pre><code>def build_conversation_state(turn, phase, recent_tool_calls, active_topic, current_datetime):
    lines = [f"  &lt;turn&gt;{turn}&lt;/turn&gt;", f"  &lt;phase&gt;{phase}&lt;/phase&gt;"]
    if current_datetime:
        day_name = current_datetime.strftime("%A")
        time_of_day = "morning" if hour &lt; 12 else "afternoon" if hour &lt; 17 else "evening"
        lines.append(f"  &lt;datetime&gt;{day_name} {time_of_day}&lt;/datetime&gt;")
    if recent_tool_calls:
        lines.append(f"  &lt;last_tools&gt;{', '.join(recent_tool_calls)}&lt;/last_tools&gt;")
    if active_topic:
        lines.append(f"  &lt;focus&gt;{active_topic}&lt;/focus&gt;")
    return "&lt;conversation_state&gt;\n" + "\n".join(lines) + "\n&lt;/conversation_state&gt;"</code></pre>

          <div class="deep-dive-only">
          <p>This 50-100 token block tells the model what turn it's on, what phase the conversation is in, what time of day it is, and which tools it already called. Without it, the agent re-calls tools it used two turns ago or asks "how can I help you today?" on turn 8.</p>

          <p>I'm dedicating Part 3 entirely to <strong>long-term memory</strong> (persistent knowledge across conversations, things like user preferences, facts, summaries, learned patterns) because it's complex enough to need its own treatment.</p>
          </div>

          <div class="deep-dive-only">
          <p>I spend the most engineering time on <strong>retrieved information</strong>, the RAG layer. External knowledge from documents, databases, and APIs gets injected on-demand, and the design decision I keep coming back to is to let the model pull what it needs via tool calls rather than front-loading everything. I tried the front-loading approach first (stuffing all potentially relevant docs into context at the start of each turn) and it worked terribly. <a href="#ref-inngest">Inngest</a> measured the same thing; pull consistently beats push. Part 2 goes deep on retrieval.</p>

          <p>In my agent, the retrieval tool automatically enriches queries with what it already knows about the patient:</p>
          </div>

<pre><code>@tool
async def search_knowledge_base(query, document_type=None, tags=None, condition_type=None):
    state = await session_store.get(session_id)

    # Auto-apply condition filter from patient profile
    if not condition_type and state.patient_profile.diagnosis:
        condition_type = state.patient_profile.diagnosis

    filters = RetrievalFilters(document_type=document_type, tags=tags, condition_type=condition_type)
    response = await retriever.retrieve(query=query, filters=filters, state=state)</code></pre>

          <div class="deep-dive-only">
          <p>The agent decides when to search (pull, not push), and the tool enriches the query behind the scenes. The condition filter means a Type 2 patient never sees Type 1 insulin pump troubleshooting guides, without the patient or the model needing to specify that constraint explicitly.</p>

          <p><strong>Tool definitions</strong> are the sneaky budget item. Function signatures the system can invoke consume context tokens whether they're used or not, and there's a genuine tension in how to manage them. <a href="#ref-inngest">Inngest</a> recommends removing any tool used less than 10% of the time because performance degrades as tool count grows. But <a href="#ref-manus">Manus</a> found that dynamically loading and removing tools breaks KV-cache (a 10x cost difference between cached and uncached tokens), so their solution is logits masking, where tools stay in context but get suppressed at the decoding level through a state machine. I don't think there's a clean answer here yet. I'm currently leaning toward keeping tool counts low rather than worrying about cache-aware masking, mostly because my agents have 4-6 tools, not 40. If you're at Manus's scale with dozens of tools, the cache math probably dominates. Worth noting separately: Anthropic and OpenAI both offer explicit cache control headers that let you pin specific prefix segments in the KV-cache, complementing the implicit prefix caching that Manus optimizes for. If you're making repeated calls with stable tool definitions, these APIs can cut your per-call cost significantly even without restructuring your prompt.</p>

          <p><strong>Structured output</strong> specifications (JSON schemas, type definitions, output constraints) are easy to overlook in a token budget because they feel like "free" structure. They're not. At scale they add up, and I've seen schemas that consume 300+ tokens before the model even starts generating.</p>
          </div>

          <div class="algorithm-box deep-dive-only">
            <h4>Typical Token Allocation (Agentic System)</h4>
            <table>
              <thead>
                <tr><th>Component</th><th>Tokens</th><th>Behavior</th></tr>
              </thead>
              <tbody>
                <tr><td>System instructions</td><td>500-2,000</td><td>Static per session</td></tr>
                <tr><td>Conversation history</td><td>500-5,000</td><td>Grows, needs trimming</td></tr>
                <tr><td>RAG results</td><td>0-10,000</td><td>On-demand, per tool call</td></tr>
                <tr><td>Tool definitions</td><td>500-3,000</td><td>Permanent, always present</td></tr>
                <tr><td>State / structured output</td><td>50-500</td><td>Dynamic per turn</td></tr>
              </tbody>
            </table>
            <p>Total must fit the model's window (8k-200k depending on provider). Getting this budget right is less about cramming in more and more about being ruthless with what you leave out.</p>
          </div>

          <div class="theory-only">
            <div class="callout">
              <p><span class="mode-label">Deep Dive</span></p>
              <p>The survey also formalizes optimal retrieval as an information-theoretic problem. The best retrieval function maximizes mutual information between the target answer and retrieved knowledge, conditioned on the query:</p>
              <p>$$\text{Retrieve}^* = \arg\max_{\text{Retrieve}} \; I(Y^*;\; c_\text{know} \mid c_\text{query})$$</p>
              <p>And context assembly can be framed as Bayesian posterior inference, combining the likelihood of the query given a context with the prior over contexts given interaction history:</p>
              <p>$$P(C \mid c_\text{query}, \ldots) \propto P(c_\text{query} \mid C) \cdot P(C \mid \text{History}, \text{World})$$</p>
              <p>Again, more math than most practitioners will ever need day-to-day. But I found the mutual information framing genuinely clarifying for one specific question: when two retrieval approaches return different documents, the one that tells the model more *new* information (higher mutual information with the answer, conditional on the query) is the better one. That helped me reason about why hybrid retrieval beats pure semantic search in my own system.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 3: Write / Select / Compress / Isolate               -->
        <!-- ============================================================ -->
        <section id="four-strategies">
          <h2>Write / Select / Compress / Isolate</h2>

          <div class="tldr-only">
            <p><strong>Write:</strong> move information out of the window into external storage. <strong>Select:</strong> retrieve relevant information back via RAG or memory queries. <strong>Compress:</strong> summarize and trim, but never compress instructions and data together. <strong>Isolate:</strong> split work across separate LLM calls so each gets focused context. Most teams over-invest in writing and selecting; the real gains are in compression and isolation.</p>
          </div>

          <div class="deep-dive-only">
          <p>Every production context system I've seen uses some combination of four strategies, and <a href="#ref-langchain">LangChain's framework</a> gives them clean names. I find the vocabulary useful not because the categories are surprising, but because asking "which of these four am I underinvesting in?" is usually the fastest way to improve a system.</p>

          <p><strong>Writing</strong> means getting information out of the context window and into external storage for later retrieval. Scratchpads let the agent write intermediate notes during a session (observations, partial results, plans) that persist via tool calls or state objects without occupying the window continuously. Memories go further, enabling cross-session retention by extracting reflections or facts and storing them in a persistent backend. The <a href="#ref-manus">Manus</a> team has a concrete version of this that I really like: their agents maintain a <code>todo.md</code> file during complex tasks, writing and re-reading their plan to counteract the "lost-in-the-middle" problem across ~50 average tool calls. It's charmingly simple for a state-of-the-art agent.</p>

          <p>My agent writes rolling summaries every 5 turns, using a cheap Gemini Flash call to compress the conversation into 2-4 sentences:</p>
          </div>

<pre><code>async def _update_summary(self, session_id, current_turn):
    existing = await self._store.get_latest_summary(session_id)
    start_turn = (existing.covers_through_turn + 1) if existing else 1
    messages = await self._store.get_messages_range(session_id, start_turn, current_turn)

    conversation_text = "\n".join(f"{m['role'].upper()}: {m['content']}" for m in messages)

    prompt = f"""Summarize this coaching conversation concisely. Focus on:
    - Key patient health data shared (readings, symptoms, adherence)
    - Strategies discussed or recommended
    - Patient's concerns and motivation level
    Omit: greetings, small talk, generic acknowledgments.

    Previous summary: {existing.summary if existing else ""}
    New conversation: {conversation_text}

    Write a concise summary (2-4 sentences)."""

    summary = await self._gemini.generate(prompt, temperature=0.0)</code></pre>

          <div class="deep-dive-only">
          <p>The summary replaces the turns it covers, so the context window never accumulates unbounded history. On turn 20, the model sees a summary covering turns 1-15 plus the raw last 5 turns, rather than all 20 turns verbatim.</p>

          <p><strong>Selecting</strong> is the complement, retrieving relevant information back when needed. This includes reading back scratchpad notes from earlier steps, querying stored memories using embeddings or keyword search, and full RAG pipelines over documents or code. One pattern worth calling out specifically: if you have many tools, you can use RAG over tool descriptions to select the right one. This improves selection accuracy 3x compared to exposing all tools at once, which I find somewhat funny because it means the agent needs a search engine just to find its own capabilities.</p>

          <p>Selection quality depends heavily on query quality. My agent rewrites the user's message into a self-contained retrieval query before searching:</p>
          </div>

<pre><code># "My numbers have been all over the place lately"
# -&gt; "blood sugar management strategies Type 2 patient on metformin irregular post-meal readings"

async def rewrite(self, query, conversation_history, patient_profile):
    recent_turns = conversation_history[-3:]
    prompt = f"""Rewrite the search query to be self-contained.
    Resolve pronouns, add implicit context from conversation.
    Known: diagnosis={profile.diagnosis}, medications={profile.current_medications}

    Rules:
    - Keep it concise (under 30 words)
    - Resolve pronouns ("it" -&gt; the patient's condition)
    - Strip emotional language, focus on the information need"""</code></pre>

          <div class="deep-dive-only">
          <p>The comment at the top shows why this matters. "My numbers have been all over the place lately" has almost zero retrieval value as-is: no condition type, no medication context, no timeframe. The rewriter infers "post-meal readings" from the last 3 turns, adds the patient's diagnosis and medication context, and produces a query that actually hits relevant documents.</p>

          <p><strong>Compression</strong> reduces tokens while maintaining task performance. Summarization replaces older conversation history with a condensed version (Claude Code applies auto-compact when approaching context limits, though the exact threshold has shifted across releases). Trimming removes older messages using heuristics (drop oldest N) or trained pruners like the Provence model, which identifies which messages contribute least to task performance. One finding I keep coming back to from the recurrent context compression research, because I violated it in my first attempt: compressing instructions and context simultaneously degrades responses. You need to compress the data but preserve the instructions separately. I was summarizing entire turns including the system-injected guidance, and the model started ignoring its own rules.</p>

          <p>My agent uses a two-pass trimming strategy. First a hard message count cap, then a character budget that accounts for the system prompt size.</p>
          </div>

<pre><code># Pass 1: Message count cap (6 turns = 12 messages)
max_messages = CONTEXT_WINDOW_TURNS * 2  # 6 * 2 = 12
conversation = conversation[-max_messages:]

# Pass 2: Character budget (subtract system prompt from 120K char limit)
remaining = CONTEXT_MAX_CHARS - len(system_prompt)
while estimate_chars(conversation) &gt; remaining and len(conversation) &gt; 2:
    conversation = conversation[1:]  # Drop oldest</code></pre>

          <div class="deep-dive-only">
          <p>The two-pass approach handles two different problems. The message cap prevents the model from getting distracted by old turns, and the character budget prevents the total context from overflowing, accounting for the fact that system prompts vary in size depending on how much patient context has been accumulated.</p>

          <p><strong>Isolation</strong> means splitting information across separate processing units so each one gets a clean, focused context window. Multi-agent systems give each sub-agent its own window focused on a specific subtask, returning a condensed summary (1,000-2,000 tokens) to the lead agent. HuggingFace's CodeAgent isolates token-heavy objects in sandbox environments, keeping only references in the main context. You can also separate LLM-exposed fields from auxiliary context storage in your state schema, because not everything the system knows needs to be in the window.</p>

          <p>My agent isolates safety classification into separate LLM calls so the guardrail context never contaminates the main patient conversation:</p>
          </div>

<pre><code># Each node runs in its own isolated LLM call with its own context
graph = StateGraph(CoachingState)

graph.add_node("input_gate", input_gate_node)       # Safety classifier
graph.add_node("pro_react_agent", pro_agent)         # Main agent (complex queries)
graph.add_node("flash_react_agent", flash_agent)     # Main agent (simple messages)
graph.add_node("output_gate", output_gate_node)      # Scope validator

graph.set_entry_point("input_gate")
graph.add_conditional_edges("input_gate", route_after_input_gate, {
    END: END,                                        # Blocked -&gt; stop
    "pro_react_agent": "pro_react_agent",            # Complex -&gt; Pro + thinking
    "flash_react_agent": "flash_react_agent",        # Simple -&gt; Flash
})
graph.add_edge("pro_react_agent", "output_gate")
graph.add_edge("flash_react_agent", "output_gate")
graph.add_edge("output_gate", END)</code></pre>

          <div class="deep-dive-only">
          <p>The input gate sees only the latest user message and a short classification prompt. The output gate sees only the agent's response and a scope-checking prompt. Neither gate's context (safety rules, classification examples) appears in the main agent's window, keeping the patient conversation clean and focused.</p>

          <p>If you're building an agent and something feels off about the outputs, run through these four categories and ask which one you're neglecting. In my experience the answer is almost always compression or isolation; people over-invest in writing and selecting because those feel like "building features," while trimming old history and splitting contexts feel like cleanup work.</p>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 4: How Context Fails                                 -->
        <!-- ============================================================ -->
        <section id="failure-modes">
          <h2>How Context Fails</h2>

          <div class="tldr-only">
            <p><strong>Poisoning:</strong> hallucination enters context, compounds downstream. <strong>Distraction:</strong> context too long, model neglects training. <strong>Confusion:</strong> noise treated as signal. <strong>Clash:</strong> contradictory instructions. Classify the failure first; mitigations are completely different.</p>
          </div>

          <div class="deep-dive-only">
          <p>When an agent misbehaves, my first question is always "what kind of context failure is this?" because the mitigations are completely different depending on the answer. <a href="#ref-breunig">Drew Breunig</a> laid out four failure modes that I think cover most of what goes wrong, and I've started using them as a debugging checklist.</p>
          </div>

          <div class="algorithm-box">
            <h4>Four Context Failure Modes</h4>
            <table>
              <thead>
                <tr><th>Mode</th><th>Cause</th><th>Fix</th></tr>
              </thead>
              <tbody>
                <tr><td>Poisoning</td><td>Error enters context, compounds downstream</td><td>Validate before memory writes</td></tr>
                <tr><td>Distraction</td><td>History too long, drowns out training</td><td>Aggressive trimming + summarization</td></tr>
                <tr><td>Confusion</td><td>Noise treated as signal</td><td>Curate ruthlessly, earn every token</td></tr>
                <tr><td>Clash</td><td>Contradictory instructions in prompt</td><td>Clear precedence hierarchy</td></tr>
              </tbody>
            </table>
          </div>

          <div class="deep-dive-only">
          <p><strong>Context poisoning</strong> is the scariest one. A hallucination or error enters the context and gets repeatedly referenced, compounding mistakes over time. Once a wrong fact lands in the conversation history, the model treats it as ground truth and builds on it. Google DeepMind's Gemini 2.5 technical report showed just how bad this gets: when the "goals" section of an agent's context was poisoned, agents developed completely nonsensical strategies downstream. The fix is to validate information before writing to long-term memory, treating memory writes like database writes where you check constraints before committing.</p>

          <p><strong>Context distraction</strong> is subtler. The context grows so long that the model over-focuses on accumulated history and neglects what it learned during training. Beyond ~100k tokens, I've noticed agents tend toward repeating actions from history rather than synthesizing novel plans. Aggressive trimming and summarization help, along with actively removing completed or irrelevant sections. I suspect most people's context windows are 2-3x larger than they need to be.</p>

          <p><strong>Context confusion</strong> is what happens when superfluous information gets treated as signal because the model can't distinguish noise from relevant information when everything is dumped in together. <a href="#ref-anthropic">Anthropic's</a> guiding principle is the right one here: "find the smallest set of high-signal tokens that maximize the likelihood of your desired outcome." Every token should earn its place, and most don't.</p>

          <p><strong>Context clash</strong> is the one I find most often in my own code, probably because it's the easiest to create accidentally. It happens when new information conflicts with existing information already in the prompt, and contradictory instructions produce unpredictable behavior. My own agent has one I caught during an audit for this post. The system prompt says "When READING information you already have in the patient context above, use it directly, do not re-fetch with <code>get_patient_profile</code>." But <code>get_patient_profile</code> is still available as a callable tool. The instruction and the tool list contradict each other. The agent sometimes calls the tool anyway, wasting a round-trip to fetch data that's already in the prompt. The fix is straightforward (remove the tool), but the clash was easy to miss because the instruction is in the <code>&lt;tools&gt;</code> section of the prompt and the tool definition is in Python code, and I never reviewed them side by side until I went looking for exactly this kind of problem.</p>
          </div>

          <div class="theory-only">
            <div class="callout">
              <p><span class="mode-label">Deep Dive</span></p>
              <p>There's a deeper mechanism behind context distraction that <a href="#ref-anthropic">Anthropic</a> calls <strong>context rot</strong>. They attribute it to the $O(n^2)$ pairwise token relationships in self-attention: at 100K+ tokens, the model's attention budget is spread across billions of pair interactions, and critical information gets drowned out. The full story is more nuanced than that framing suggests. Positional encoding degradation (especially RoPE-based encodings at positions beyond the training distribution), attention sink phenomena where early tokens absorb disproportionate weight, and training data skew toward shorter contexts all contribute. But the practical consequence is the same regardless of which mechanism dominates.</p>
              <p>The survey cites the <strong>lost-in-the-middle</strong> finding (<a href="#ref-liu">Liu et al., 2023</a>) to quantify this: performance degrades by up to 73% when relevant information sits in the middle of long contexts versus the beginning or end, though the exact magnitude varies significantly by model, task, and number of documents. The core result, a consistent U-shaped curve where middle positions perform worst, has been replicated across multiple studies and model families. The practical implication is that position matters. Put your most important context at the beginning and end of the window, not the middle.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 5: A Real Token Budget                               -->
        <!-- ============================================================ -->
        <section id="token-budget">
          <h2>A Real Token Budget</h2>

          <div class="tldr-only">
            <p>Concrete token budget from a LangGraph diabetes coaching agent: ~2,000-4,050 token system prompt, 6-turn conversation window (120K char budget), on-demand RAG up to 10,000 tokens, safety gates isolated into separate LLM calls.</p>
          </div>

          <div class="deep-dive-only">
          <p>All of the above is easier to understand with a concrete example, so let me walk through the diabetes management coaching assistant I built as a ReAct agent with LangGraph. The system prompt ranges from ~2,000 to ~4,050 tokens depending on session maturity, assembled dynamically from several blocks.</p>
          </div>

          <div style="border:1px solid var(--border); border-radius:8px; padding:24px; margin:24px 0; background:#fafafa; overflow-x:auto;">
            <div style="display:flex; align-items:center; gap:0; min-width:700px; justify-content:center; flex-wrap:nowrap;">
              <div style="background:white; border:2px solid var(--border); border-radius:8px; padding:8px 12px; text-align:center; min-width:70px;">
                <div style="font-weight:600; font-size:13px;">Patient</div>
                <div style="font-size:10px; color:var(--muted);">message</div>
              </div>
              <div style="color:#bbb; font-size:18px; padding:0 6px;">&rarr;</div>
              <div style="background:#e8e5de; border:2px solid #c5c2b8; border-radius:8px; padding:8px 12px; text-align:center; min-width:70px;">
                <div style="font-weight:600; font-size:13px;">Input Gate</div>
                <div style="font-size:10px; color:var(--muted);">safety check</div>
              </div>
              <div style="color:#bbb; font-size:18px; padding:0 6px;">&rarr;</div>
              <div style="display:flex; flex-direction:column; gap:4px;">
                <div style="background:white; border:2px solid #c5c2b8; border-radius:8px; padding:6px 12px; text-align:center;">
                  <div style="font-weight:600; font-size:13px;">Pro Agent</div>
                  <div style="font-size:10px; color:var(--muted);">complex + thinking</div>
                </div>
                <div style="background:white; border:2px solid var(--border); border-radius:8px; padding:6px 12px; text-align:center;">
                  <div style="font-weight:600; font-size:13px;">Flash Agent</div>
                  <div style="font-size:10px; color:var(--muted);">simple queries</div>
                </div>
              </div>
              <div style="color:#bbb; font-size:18px; padding:0 6px;">&rarr;</div>
              <div style="background:#e8e5de; border:2px solid #c5c2b8; border-radius:8px; padding:8px 12px; text-align:center; min-width:70px;">
                <div style="font-weight:600; font-size:13px;">Output Gate</div>
                <div style="font-size:10px; color:var(--muted);">scope check</div>
              </div>
              <div style="color:#bbb; font-size:18px; padding:0 6px;">&rarr;</div>
              <div style="background:white; border:2px solid var(--border); border-radius:8px; padding:8px 12px; text-align:center; min-width:70px;">
                <div style="font-weight:600; font-size:13px;">Response</div>
              </div>
            </div>
            <div style="display:flex; justify-content:center; margin-top:12px;">
              <div style="border:1px dashed #c5c2b8; border-radius:8px; padding:8px 16px; display:flex; gap:16px; font-size:11px; color:var(--muted);">
                <span>Background: rolling summary</span>
                <span style="color:#ddd;">|</span>
                <span>profile updates</span>
                <span style="color:#ddd;">|</span>
                <span>outcome tracking</span>
              </div>
            </div>
            <div style="text-align:center; margin-top:8px; font-size:12px; color:var(--muted);">Each node runs in its own isolated LLM call with its own context window</div>
          </div>

          <table>
            <thead>
              <tr><th>Component</th><th>Tokens</th><th>Type</th></tr>
            </thead>
            <tbody>
              <tr><td>Conversation state (turn count, phase, datetime)</td><td>50-100</td><td>Dynamic (every turn)</td></tr>
              <tr><td>Identity block (personality, voice)</td><td>~350</td><td>Static</td></tr>
              <tr><td>Boundaries (5 hard scope rules)</td><td>~280</td><td>Static</td></tr>
              <tr><td>Patient context (profile, summary, goals)</td><td>80-700</td><td>Dynamic (per session)</td></tr>
              <tr><td>Approach + tools + response guides</td><td>~470</td><td>Static</td></tr>
              <tr><td>Few-shot examples (6)</td><td>~550</td><td>Static</td></tr>
              <tr><td>RAG results (last 3, conditional)</td><td>0-1,500</td><td>Conditional</td></tr>
              <tr><td><strong>System prompt total</strong></td><td><strong>~2,000-4,050</strong></td><td><strong>Mixed</strong></td></tr>
              <tr><td>Conversation window (6 turns max)</td><td>500-5,000</td><td>Dynamic (rolling)</td></tr>
              <tr><td>Tool results (<code>search_knowledge_base</code>)</td><td>0-10,000</td><td>On-demand</td></tr>
            </tbody>
          </table>

          <div class="deep-dive-only">
          <p>The conversation window holds 6 turns max, char-budgeted at 120,000 characters. Messages from turns already covered by the rolling summary are excluded.</p>

          <p>Everything gets assembled in a single <code>prepare_context</code> hook that runs before every LLM call in the ReAct loop:</p>
          </div>

<pre><code>async def prepare_context(state: CoachingState):
    session_id = state.get("session_id", "default")
    session_state = await session_store.get(session_id)
    turn_count = session_state.turn_count
    cache_key = (session_id, turn_count)

    # Skip rebuild if nothing mutated since last call
    has_mutation = _has_mutating_tool(state["messages"])
    if cache_key in _prompt_cache and not has_mutation:
        system_prompt = _prompt_cache[cache_key]
    else:
        summary = await session_store.get_latest_summary(session_id)
        episodes = await session_store.get_recent_episodes(session_id, limit=5)
        tool_results = await session_store.get_recent_tool_results(session_id, limit=3)

        system_prompt = build_system_prompt(
            profile=session_state.patient_profile,
            active_strategies=session_state.active_strategies,
            goals=session_state.goals, outcomes=session_state.outcomes,
            session_summary=summary_text,
        )

        state_block = build_conversation_state(turn_count, phase, recent_tool_names, ...)
        system_prompt = state_block + "\n\n" + system_prompt  # &lt;-- problematic ordering

    # Trim conversation to fit budget
    max_messages = CONTEXT_WINDOW_TURNS * 2
    conversation = [m for m in messages if not isinstance(m, SystemMessage)]
    conversation = conversation[-max_messages:]
    while estimate_chars(conversation) &gt; remaining_budget and len(conversation) &gt; 2:
        conversation = conversation[1:]

    return {"llm_input_messages": [SystemMessage(system_prompt)] + conversation}</code></pre>

          <div class="deep-dive-only">
          <p>This function is where all the context engineering actually happens. Load state, build the prompt from components, prepend the conversation state block, cache the result, and trim the conversation window. Every technique I described in the earlier sections (XML structure, rolling summaries, trimming, isolation) converges in this one function. And if you noticed the <code># &lt;-- problematic ordering</code> comment, good. I'll come back to it.</p>
          </div>

          <div class="algorithm-box deep-dive-only">
            <h4>Context Engineering Techniques Used</h4>
            <table>
              <thead><tr><th>Category</th><th>Technique</th><th>Implementation</th></tr></thead>
              <tbody>
                <tr><td>Structure</td><td>XML-tagged sections</td><td><code>&lt;identity&gt;</code>, <code>&lt;boundaries&gt;</code>, <code>&lt;patient-context&gt;</code>, etc.</td></tr>
                <tr><td>Compression</td><td>Rolling summary</td><td>Every 5 turns via Gemini Flash</td></tr>
                <tr><td>Compression</td><td>History trimming</td><td>6-turn window + 120K char budget</td></tr>
                <tr><td>Memory</td><td>Semantic (patient profile)</td><td>Structured schema, tool-driven updates</td></tr>
                <tr><td>Memory</td><td>Episodic (outcomes)</td><td>Created on <code>track_outcome</code>, stored with emotion</td></tr>
                <tr><td>RAG</td><td>Hybrid search</td><td>Dense + sparse + RRF + cross-encoder reranking</td></tr>
                <tr><td>RAG</td><td>Query rewriting</td><td>Pronoun resolution + profile context injection</td></tr>
                <tr><td>Routing</td><td>Dual-model</td><td>Flash for simple messages, Pro with thinking for complex</td></tr>
                <tr><td>Safety</td><td>Isolated gates</td><td>Input + output classifiers in separate LLM calls</td></tr>
              </tbody>
            </table>
          </div>

          <h3>What I Got Wrong</h3>

          <div class="tldr-only">
            <ul class="list-tight">
              <li><strong>KV-cache violation:</strong> Volatile state prepended to prompt prefix, invalidating cache every turn (10x cost). Fix: move to end.</li>
              <li><strong>No memory validation:</strong> LLM-extracted facts written directly with no conflict detection or rollback.</li>
              <li><strong>Redundant tool:</strong> <code>get_patient_profile</code> duplicates data already in system prompt, confusing the model.</li>
              <li><strong>Content-blind trimming:</strong> FIFO drops high-signal old messages before low-signal filler.</li>
              <li><strong>No importance scoring:</strong> Episodic memory retrieved by recency alone.</li>
            </ul>
          </div>

          <div class="deep-dive-only">
          <p>When I audited this agent against the best practices I'd just finished researching for this post, it was humbling. I found violations of principles I had literally just written about, in my own code, while the blog post draft was still open in the next tab. Five specific problems stood out.</p>

          <p>The most expensive one is a <strong>KV-cache violation</strong>. Look at the <code>prepare_context</code> code above. The volatile <code>&lt;conversation_state&gt;</code> block (which changes every turn with new turn count, new timestamp, new tool history) gets prepended to the start of the system prompt. KV-cache works by matching a prefix: if the first N tokens are identical between calls, the provider can reuse the cached key-value pairs and charge you the cached rate. By putting volatile data at the very start, every single turn invalidates the entire cache. <a href="#ref-manus">Manus</a> reports this is a 10x cost difference (<span class="nokatex">$0.30/MTok cached vs $3/MTok uncached</span> on Claude Sonnet). The fix is one line, move the state block to the end of the prompt instead of the beginning, so the static sections (identity, boundaries, examples) form a stable prefix that caches across turns.</p>
          </div>

          <div style="border:1px solid var(--border); border-radius:8px; padding:24px; margin:24px 0; background:#fafafa;">
            <div style="margin-bottom:16px;">
              <div style="display:flex; align-items:center; gap:8px; margin-bottom:10px;">
                <span style="font-size:14px; font-weight:600; color:#b45555;">Before</span>
                <span style="font-size:12px; color:var(--muted);">Cache breaks on every turn</span>
              </div>
              <div style="display:flex; gap:2px; font-size:11px; border-radius:6px; overflow:hidden;">
                <div style="background:#f2d4d4; border:1px solid #d4a0a0; padding:8px 10px; min-width:100px; text-align:center; position:relative;">
                  <div style="font-weight:600;">conversation_state</div>
                  <div style="color:#b45555; font-size:10px;">volatile</div>
                </div>
                <div style="background:#eee; border:1px solid #ddd; padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">identity</div>
                  <div style="color:var(--muted); font-size:10px;">static</div>
                </div>
                <div style="background:#eee; border:1px solid #ddd; padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">boundaries</div>
                  <div style="color:var(--muted); font-size:10px;">static</div>
                </div>
                <div style="background:#eee; border:1px solid #ddd; padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">examples</div>
                  <div style="color:var(--muted); font-size:10px;">static</div>
                </div>
                <div style="background:#eee; border:1px solid #ddd; padding:8px 10px; flex:1; text-align:center; color:var(--muted);">...</div>
              </div>
              <div style="font-size:10px; color:#b45555; margin-top:4px;">&uarr; Prefix changes every turn. 0% cache hit rate.</div>
            </div>
            <div>
              <div style="display:flex; align-items:center; gap:8px; margin-bottom:10px;">
                <span style="font-size:14px; font-weight:600; color:#4a8c5c;">After</span>
                <span style="font-size:12px; color:var(--muted);">Static prefix caches across turns</span>
              </div>
              <div style="display:flex; gap:2px; font-size:11px; border-radius:6px; overflow:hidden;">
                <div style="background:#d4edda; border:1px solid #a3d1ad; padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">identity</div>
                  <div style="color:#4a8c5c; font-size:10px;">cached</div>
                </div>
                <div style="background:#d4edda; border:1px solid #a3d1ad; padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">boundaries</div>
                  <div style="color:#4a8c5c; font-size:10px;">cached</div>
                </div>
                <div style="background:#d4edda; border:1px solid #a3d1ad; padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">examples</div>
                  <div style="color:#4a8c5c; font-size:10px;">cached</div>
                </div>
                <div style="background:#d4edda; border:1px solid #a3d1ad; padding:8px 10px; flex:1; text-align:center; color:#4a8c5c;">...</div>
                <div style="background:#fff8e5; border:1px solid #e0d5a0; padding:8px 10px; min-width:100px; text-align:center;">
                  <div style="font-weight:600;">conversation_state</div>
                  <div style="color:#9a8a3c; font-size:10px;">volatile</div>
                </div>
              </div>
              <div class="nokatex" style="font-size:10px; color:#4a8c5c; margin-top:4px;">&uarr; Stable prefix. $0.30/MTok cached vs $3.00/MTok uncached (10x).</div>
            </div>
          </div>

          <div class="deep-dive-only">
          <p>I also have <strong>no memory validation</strong>. When the agent calls <code>update_patient_profile</code>, the LLM extracts facts from the conversation ("A1C is 7.2," "started metformin three months ago") and writes them directly to the profile. No conflict detection (what if the profile already says A1C is 6.8 from a more recent lab?), no enum validation (what if the LLM writes "pre-diabetic-ish" instead of one of the valid condition categories?), no versioning (if the extraction hallucinates, the bad data overwrites the good data with no rollback). This is exactly the context poisoning failure mode from Section 4, and I built it knowing better.</p>

          <p>Then there's the <strong>redundant <code>get_patient_profile</code> tool</strong>. The patient profile is already injected into the <code>&lt;patient-context&gt;</code> section of the system prompt on every turn. The tool exists to let the agent "check what it knows," but the agent already knows; it's in the prompt. I mentioned this in Section 4 as a context clash, and it is, but it's also context confusion in a way I didn't appreciate until I watched the agent's behavior. The tool's existence signals to the model that the profile might not be in context, which makes it less likely to trust the data it already has. The tool is actively making the agent dumber.</p>

          <p>My <strong>FIFO trimming is content-blind</strong>, which means the trimming code drops the oldest message first, regardless of what's in it. A message like "we tried the post-meal walking routine and blood sugar actually spiked" (high signal, contains an outcome) gets dropped before a filler "thanks!" or "ok sounds good" message, purely because the filler is newer. A smarter approach would score messages by information density and preserve high-signal messages even if they're older. The Provence model from the research does exactly this: it trains a pruner to identify which messages contribute least to task performance. I haven't implemented anything like that yet, but it's high on my list.</p>

          <p>And finally, <strong>no importance scoring for episodic memory</strong>. Episodes (tracked outcomes, strategy results) are retrieved by recency alone. A breakthrough moment from 3 weeks ago where a patient discovered that 15-minute post-meal walks consistently dropped their blood sugar by 40 mg/dL gets ranked below a routine check-in from yesterday. The Stanford generative agents paper showed that combining recency, importance, and relevance produces much better retrieval than any single factor. My agent uses one of the three. That's going to be a significant Part 3 topic.</p>
          </div>

          <div class="theory-only">
            <div class="callout">
              <p><span class="mode-label">Deep Dive</span></p>
              <p>The benchmarks suggest context engineering matters a lot more than most people realize. On GAIA, human accuracy is 92% while GPT-4 hits 15%. A 77-point gap. On GTA, GPT-4 completes fewer than 50% of tasks. On WebArena, the top agent (IBM CUGA) reaches only 61.7%. These benchmarks all require integrating information from multiple sources, using tools, and maintaining state across steps, which is exactly what context engineering addresses.</p>
              <p>Memory systems fare poorly too. LongMemEval (500 curated questions) finds 30% accuracy degradation in commercial assistants during extended interactions. GPT-4, Claude, and Llama 3.1 all struggle with episodic memory involving interconnected events, even in brief contexts. The gap between model capability on narrow benchmarks and system capability on realistic tasks is, I think, the context engineering gap.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 6: What I Learned                                    -->
        <!-- ============================================================ -->
        <section id="what-i-learned">
          <h2>What I Learned</h2>

          <div class="tldr-only">
            <p>Context engineering is mostly about removal, not addition. Every improvement involved taking something out or moving it around. Context strategy isn't portable across providers; test on every model you support.</p>
          </div>

          <div class="deep-dive-only">
          <p>The thing that surprised me most writing this post wasn't any individual technique. It was the gap between knowing best practices and actually following them in my own code. I'd spent weeks reading the research, writing up the failure modes, explaining why you should validate memory writes and keep tool counts low, and then I audited my own agent and found every single anti-pattern I'd warned against. The KV-cache ordering was backwards. Memory writes had no validation. A redundant tool was actively making the agent dumber. It was a useful lesson in intellectual humility, and I suspect most engineers building agents right now would find the same thing if they looked.</p>

          <p>If I had to distill this post into one actionable idea, it's that context engineering is mostly about removal, not addition. The instinct is always to add more information, more tools, more history, more instructions. But every improvement I've made to my agent involved taking something out or moving it around, not putting more in. Remove the redundant tool. Move the volatile state block to the end of the prompt. Trim history more aggressively. Compress the conversation to its information-bearing skeleton. The constraint ($|C| \leq L_\text{max}$) is real, and the best systems I've seen treat it as a design principle rather than a limitation to work around.</p>

          <p>What I deliberately didn't cover here is how to build good retrieval pipelines (Part 2) and how to design memory systems that actually improve over time rather than accumulating garbage (Part 3). Both of those are deep enough to deserve their own treatment. I'm also not confident I have the right answer on tool management yet; the tension between Inngest's "remove low-usage tools" and Manus's "keep tools for cache stability" feels unresolved to me, and my current approach (just keep tool counts low) is probably too simplistic for agents with 20+ tools.</p>

          <p>One caveat worth keeping in mind as you apply any of this. Context strategy isn't portable across providers. Different models have different attention patterns, different context window behaviors, and different sensitivities to prompt structure. What works for Claude might fail on GPT-4 might fail on Gemini. I've been burned by this enough times that I now test prompts on every model I plan to support, rather than assuming my architecture generalizes.</p>

          <p>If you spot errors or have war stories from your own context engineering work, I'd love to hear about it on <a href="#">X</a> or <a href="#">LinkedIn</a>.</p>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 7: References                                        -->
        <!-- ============================================================ -->
        <section id="references" class="deep-dive-only">
          <h2>References</h2>

          <ol>
            <li id="ref-karpathy">Karpathy, A. "Context Engineering." <a href="https://x.com/karpathy/status/1937902205765607626">X/Twitter</a>, June 2025.</li>
            <li id="ref-lutke">Lutke, T. "Context Engineering over Prompt Engineering." <a href="https://x.com/tobi/status/1935533422589399127">X/Twitter</a>, June 2025.</li>
            <li id="ref-schmid">Schmid, P. "Context Engineering." <a href="https://www.philschmid.de/context-engineering">philschmid.de</a>, 2025.</li>
            <li id="ref-anthropic">Rajasekaran, P. et al. "Effective Context Engineering for AI Agents." <a href="https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents">Anthropic Engineering</a>, September 2025.</li>
            <li id="ref-langchain">Martin, L. "Context Engineering for Agents." <a href="https://blog.langchain.com/context-engineering-for-agents/">LangChain Blog</a>, July 2025.</li>
            <li id="ref-breunig">Breunig, D. "How Contexts Fail and How to Fix Them." <a href="https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html">dbreunig.com</a>, June 2025.</li>
            <li id="ref-manus">Ji, Y. "Context Engineering for AI Agents: Lessons from Building Manus." <a href="https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus">manus.im</a>, July 2025.</li>
            <li id="ref-spotify">Spotify Engineering. "Context Engineering: Background Coding Agents Part 2." <a href="https://engineering.atspotify.com/2025/11/context-engineering-background-coding-agents-part-2">engineering.atspotify.com</a>, November 2025.</li>
            <li id="ref-inngest">Inngest. "Five Critical Lessons for Context Engineering." <a href="https://www.inngest.com/blog/five-lessons-for-context-engineering">inngest.com</a>, 2025.</li>
            <li id="ref-mei">Mei, Z. et al. "A Survey of Context Engineering for Large Language Models." <a href="https://arxiv.org/abs/2507.13334">arXiv:2507.13334</a>, July 2025.</li>
            <li id="ref-willison">Willison, S. "Context Engineering." <a href="https://simonwillison.net/2025/Jun/27/context-engineering/">simonwillison.net</a>, June 2025.</li>
            <li id="ref-osmani">Osmani, A. "Context Engineering: Bringing Engineering Discipline to AI." <a href="https://addyo.substack.com/p/context-engineering-bringing-engineering">Substack</a>, 2025.</li>
            <li id="ref-fowler">Fowler, M. "Context Engineering for Coding Agents." <a href="https://martinfowler.com/articles/exploring-gen-ai/context-engineering-coding-agents.html">martinfowler.com</a>, 2025.</li>
            <li id="ref-liu">Liu, N.F. et al. "Lost in the Middle: How Language Models Use Long Contexts." <a href="https://arxiv.org/abs/2307.03172">arXiv:2307.03172</a>, 2023. Published in TACL, 2024.</li>
            <li id="ref-longmemeval">Wu, D. et al. "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory." <a href="https://arxiv.org/abs/2410.10813">arXiv:2410.10813</a>, 2024. Published at ICLR 2025.</li>
          </ol>
        </section>

      </article>
    </div>

    <footer class="footer">
      <a href="#">X</a> · <a href="#">LinkedIn</a> · <a href="#">GitHub</a>
    </footer>
  </main>

  <script src="../../js/katex-init.js"></script>
  <script src="../../js/mode-toggle.js"></script>
</body>
</html>
