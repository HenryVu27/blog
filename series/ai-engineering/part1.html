<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>What Fills the Context Window · Technical Blog</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:ital,wght@0,300..700;1,300..700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../../css/styles.css">
  <link rel="stylesheet" href="../../css/post-styles.css">
  <link rel="stylesheet" href="../../css/deep-dive-layout.css">
  <link rel="stylesheet" href="../../css/components.css">
  <link rel="stylesheet" href="../../css/mode-toggle.css">
  <link rel="stylesheet" href="../../css/prism-blog.css">
  <link rel="stylesheet" href="../../css/code-blocks.css">
  <script>try{if(localStorage.getItem('blog-reading-mode')==='deep-dive')document.documentElement.classList.add('deep-dive-mode')}catch(e){}</script>
  <!-- KaTeX for Deep Dive math -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
</head>
<body>
  <main class="page">
    <div class="page-header">
      <nav class="nav">
        <a href="../../index.html" class="back-btn">← Back</a>
      </nav>

      <header>
        <h1 class="title">What Fills the Context Window</h1>
        <div class="meta">
          2026-02-24 · <span class="deep-dive-only">25 min read</span><span class="tldr-only">5 min read</span>
        </div>
        <div class="mode-toggle">
          <button data-mode="deep-dive">Deep Dive</button>
          <button data-mode="tldr" class="active">TLDR</button>
        </div>
        <hr>
      </header>

      <div class="series-nav">
        <h3>AI Engineering Series</h3>
        <ul>
          <li>Part 1: What Fills the Context Window (current)</li>
          <li>Part 2: RAG, From Naive to Agentic (coming soon)</li>
          <li>Part 3: Memory Engineering (coming soon)</li>
          <li>Part 4: Building AI Agents (coming soon)</li>
          <li>Part 5: Multi-Agent Systems (coming soon)</li>
        </ul>
      </div>
    </div>

    <div class="content-divider"></div>

    <div class="layout">
      <aside class="toc deep-dive-only">
        <div class="toc-sticky">
          <h2>Contents</h2>
          <ul>
            <li class="deep-dive-only"><a href="#from-prompts-to-context"><span class="toc-section-number">1</span><span class="toc-section-title">From Prompts to Context</span></a></li>
            <li><a href="#seven-components"><span class="toc-section-number">2</span><span class="toc-section-title">The Seven Components</span></a></li>
            <li><a href="#four-strategies"><span class="toc-section-number">3</span><span class="toc-section-title">Write / Select / Compress / Isolate</span></a></li>
            <li><a href="#failure-modes"><span class="toc-section-number">4</span><span class="toc-section-title">How Context Fails</span></a></li>
            <li><a href="#token-budget"><span class="toc-section-number">5</span><span class="toc-section-title">A Real Token Budget</span></a></li>
            <li><a href="#measuring"><span class="toc-section-number">6</span><span class="toc-section-title">Measuring Context Quality</span></a></li>
            <li><a href="#what-i-learned"><span class="toc-section-number">7</span><span class="toc-section-title">What I Learned</span></a></li>
            <li class="deep-dive-only"><a href="#references"><span class="toc-section-number">8</span><span class="toc-section-title">References</span></a></li>
          </ul>
        </div>
      </aside>

      <article class="post">

        <!-- ============================================================ -->
        <!-- SECTION 1: From Prompts to Context                           -->
        <!-- ============================================================ -->
        <section id="from-prompts-to-context" class="deep-dive-only">
          <h2>From Prompts to Context</h2>

          <p>When GPT-3 landed in 2020, early adopters quickly discovered that tiny changes in phrasing produced wildly different outputs. Swap "summarize" for "explain briefly," add "step by step," rearrange a few words, tell it to roleplay as a character with no safety guidelines, and the same model would go from incoherent to useful to giving you a recipe for homemade biological weapons. That sensitivity spawned an entire discipline called prompt engineering, the craft of writing instructions that reliably steer language models toward useful behavior.</p>

          <p>For a while, prompt engineering was the whole game. Your system prompt was a paragraph or two, the context window was 4K tokens, and the main skill was wordsmithing, finding the exact phrasing, the right few-shot examples, the magic "think step by step" incantation that unlocked the behavior you wanted.</p>

          <p>Context windows eventually went from 4K to 200K tokens, and models got good enough that phrasing stopped being the bottleneck for most tasks. Sometime around mid-2025, the community started calling what we do "context engineering" instead, and the new label caught on fast. <a href="#ref-karpathy">Andrej Karpathy</a> called it "the delicate art and science of filling the context window with just the right information for the next step." I like that framing because it puts the emphasis on information selection, not wordsmithing.</p>

          <p>Most agent failures I encounter today are context failures, where the model can do what I need but doesn't have the right information when it needs it. The system prompt is but one of seven components that fill the context window, and for a simple single-turn task, careful prompt engineering is all you need. But the moment you add retrieval, tools, multi-step reasoning, or agent workflows, the challenge shifts from "how do I phrase this instruction" to "what information does the model need to see right now, and how do I assemble it reliably."</p>

          <p>This post walks through those seven components, the strategies for managing them, how they fail, and what a real token budget looks like in a diabetes management coaching agent I built with LangGraph. I'll go deeper on RAG in Part 2, memory in Part 3, agents in Part 4, and multi-agent coordination in Part 5.</p>

          <div class="theory-only">
            <div class="callout">
              <p><a href="#ref-mei">Mei et al.'s 166-page survey</a> (arXiv:2507.13334, analyzing 1,411 papers) provides the first mathematical formalization. They model context as a structured assembly of six typed components:</p>
              <p>$$C = A(c_\text{instr},\; c_\text{know},\; c_\text{tools},\; c_\text{mem},\; c_\text{state},\; c_\text{query})$$</p>
              <p>Context engineering then becomes a constrained optimization problem, where you find the assembly function $F$ that maximizes expected reward across tasks, subject to a hard window size limit:</p>
              <p>$$F^* = \arg\max_F \; \mathbb{E}_{\tau \sim \mathcal{T}} \left[\text{Reward}\!\left(P_\theta(Y \mid C_{F}(\tau)),\; Y^*_\tau\right)\right] \quad \text{s.t.} \;\; |C| \leq L_\text{max}$$</p>
              <p>My first reaction was that this is ML researchers dressing up practitioner intuition in math to make it paper-worthy. But coming from an optimization background, the formulation maps onto an intuition I'm already familiar with.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 2: The Seven Components of Context                   -->
        <!-- ============================================================ -->
        <section id="seven-components">
          <h2>The Seven Components</h2>

          <div class="deep-dive-only">
          <p>Every LLM call consumes a context window, a fixed-size buffer of tokens containing everything the model can see. What you put in that buffer determines what the model can do. Several people have converged on roughly the same decomposition (<a href="#ref-manus">Manus</a>, <a href="#ref-anthropic">Anthropic</a>, <a href="#ref-langchain">LangChain</a>, Google), and seven components turns out to be the right granularity.</p>
          </div>

          <div style="border:1px solid var(--border); border-radius:8px; padding:20px 24px; margin:24px 0; background:#f0ead9;">
            <div style="display:flex; align-items:center; gap:8px; margin-bottom:16px;">
              <span style="font-size:12px; font-weight:500; text-transform:uppercase; letter-spacing:0.05em; color:var(--muted);">Context Window</span>
              <span style="flex:1; border-top:1px solid var(--border);"></span>
              <span style="font-size:12px; color:var(--muted);">8K-200K tokens</span>
            </div>
            <div style="display:flex; flex-direction:column; gap:6px; font-size:13px;">
              <div style="display:flex; align-items:center; gap:10px;">
                <div style="width:20%; background:#b8a88a; border-radius:3px; padding:6px 10px; color:#f0ead9; font-weight:500; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">System prompt</div>
                <span style="color:var(--muted); font-size:12px;">500-2,000</span>
              </div>
              <div style="display:flex; align-items:center; gap:10px;">
                <div style="width:15%; background:#c4b69e; border-radius:3px; padding:6px 10px; color:#f0ead9; font-weight:500; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">User prompt</div>
                <span style="color:var(--muted); font-size:12px; font-style:italic;">variable</span>
              </div>
              <div style="display:flex; align-items:center; gap:10px;">
                <div style="width:50%; background:#9a8970; border-radius:3px; padding:6px 10px; color:#f0ead9; font-weight:500; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">State + conversation history</div>
                <span style="color:var(--muted); font-size:12px;">500-5,000</span>
              </div>
              <div style="display:flex; align-items:center; gap:10px;">
                <div style="width:15%; background:#c4b69e; border-radius:3px; padding:6px 10px; color:#f0ead9; font-weight:500; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Long-term memory</div>
                <span style="color:var(--muted); font-size:12px; font-style:italic;">variable</span>
              </div>
              <div style="display:flex; align-items:center; gap:10px;">
                <div style="width:100%; background:#7a6c58; border-radius:3px; padding:6px 10px; color:#f0ead9; font-weight:500; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Retrieved information (RAG)</div>
                <span style="color:var(--muted); font-size:12px; white-space:nowrap;">0-10,000</span>
              </div>
              <div style="display:flex; align-items:center; gap:10px;">
                <div style="width:30%; background:#a8987e; border-radius:3px; padding:6px 10px; color:#f0ead9; font-weight:500; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Tool definitions</div>
                <span style="color:var(--muted); font-size:12px;">500-3,000</span>
              </div>
              <div style="display:flex; align-items:center; gap:10px;">
                <div style="width:5%; min-width:80px; background:#d1c4ac; border-radius:3px; padding:6px 10px; color:#6d6355; font-weight:500; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Structured output</div>
                <span style="color:var(--muted); font-size:12px;">50-500</span>
              </div>
            </div>
          </div>

          <div class="tldr-only">
            <ul class="list-tight">
              <li>Version system prompts like code</li>
              <li>Use pull-based RAG (model requests via tools) over front-loading</li>
              <li>Full conversation dumps hurt performance; trim aggressively</li>
              <li>Tool definitions consume tokens whether used or not</li>
            </ul>
          </div>

          <div class="deep-dive-only">
          <p>Of the seven, the <strong>system prompt</strong> is where most people start, and rightfully so. It sets behavioral guidelines, role definitions, and rules for your agent. What I didn't expect when I started building agents is how much the system prompt wants to grow. Every failure mode you encounter tempts you to add another instruction, and before long you're at 4,000 tokens of rules that sometimes contradict each other. I've learned (the expensive way) to start minimal and iteratively add instructions based on observed failures. <a href="#ref-spotify">Spotify's engineering team</a> went even further and found that larger, static, version-controlled prompts proved more predictable than dynamic tool-based approaches. I treat my system prompts as code now: versioned, reviewed, tested.</p>

          <p>My agent's system prompt is broken into XML-tagged sections, each responsible for a distinct behavioral concern:</p>
          </div>

<div class="code-block" data-filename="system_prompt.xml" data-lang="xml">
<pre><code>&lt;identity&gt;
  Personality, voice, coach name
&lt;/identity&gt;

&lt;boundaries&gt;
  5 hard scope rules + output gate awareness
  "If your response recommends specific medications...
   the entire response will be discarded."
&lt;/boundaries&gt;

&lt;patient-context&gt;
  Structured profile, session summary, goals
&lt;/patient-context&gt;

&lt;approach&gt;
  Progressive profiling, evidence-based guidance
&lt;/approach&gt;

&lt;tools&gt;
  When to search, when to update profile
&lt;/tools&gt;

&lt;response-guide&gt;
  Adaptive length, tone, situation matching
&lt;/response-guide&gt;

&lt;examples&gt;
  6 few-shot patient coaching conversations
&lt;/examples&gt;</code></pre>
</div>

          <div class="deep-dive-only">
          <p>I started using XML tags mostly for my own sanity (it's easier to review a prompt when you can collapse sections), but it turns out LLMs respond measurably better to structured input than unstructured dumps. Both <a href="#ref-anthropic">Anthropic</a> and Google recommend structured delimiters for this reason. The tags also give you a natural unit for version control diffs, which matters more than you'd think once your prompt is 2,000+ tokens and three people are editing it.</p>

          <p>One surprisingly high-leverage system prompt technique comes from <a href="#ref-openai-prompting">OpenAI's GPT-4.1 prompting guide</a>: three specific instructions that increased their SWE-bench score by ~20%. (1) Persistence: "keep going until the user's query is completely resolved." (2) Tool-calling: "use your tools to answer questions rather than relying on memory." (3) Planning: explicitly asking the model to plan before acting. Three sentences transformed the model "from a chatbot-like state into a much more eager agent," which says something about how sensitive agent behavior is to system prompt phrasing even in 2025.</p>

          <p>I spend the least time worrying about the <strong>user prompt</strong> because it's the one thing I don't control. The immediate message from the human can be anything, from well-structured to incoherent, concise to rambling. The rest of your context engineering has to be robust enough to handle whatever arrives.</p>

          <p>Where I've seen the most waste is <strong>state and short-term history</strong>, the current conversation turns and prior exchanges that serve as the working memory of your system. Most implementations just dump the raw history in verbatim, greetings, acknowledgments, off-topic tangents. The <a href="#ref-longmemeval">LongMemEval benchmark</a> (Wu et al., ICLR 2025) showed that models given the full ~115K-token conversation history performed worse than models given only the relevant subset, which tells you everything about the cost of unfocused context. What you remove from history matters at least as much as what you keep.</p>

          <p>A design principle that took me longer than it should have to internalize: in mature systems, the authoritative data lives outside the window (database, filesystem, structured JSON store), and the context assembly function selects a <em>projection</em> for each turn. The context window is a view, not the source of truth. My agent's patient profile lives in a persistent store; what the model sees is a snapshot assembled fresh on every turn based on what's relevant right now. This becomes much more important in Part 4 when the filesystem itself becomes the agent's working memory, but even in a single-session agent, treating the window as a read-only view of external state keeps you from accidentally coupling your model's behavior to stale conversation history.</p>

          <p>My agent injects a compact state block that gives the model temporal awareness and continuity across the ReAct loop:</p>
          </div>

<div class="code-block" data-filename="state.py" data-lang="python">
<pre><code>def build_conversation_state(turn, phase, recent_tool_calls, active_topic, current_datetime):
    lines = [f"  &lt;turn&gt;{turn}&lt;/turn&gt;", f"  &lt;phase&gt;{phase}&lt;/phase&gt;"]
    if current_datetime:
        day_name = current_datetime.strftime("%A")
        time_of_day = "morning" if hour &lt; 12 else "afternoon" if hour &lt; 17 else "evening"
        lines.append(f"  &lt;datetime&gt;{day_name} {time_of_day}&lt;/datetime&gt;")
    if recent_tool_calls:
        lines.append(f"  &lt;last_tools&gt;{', '.join(recent_tool_calls)}&lt;/last_tools&gt;")
    if active_topic:
        lines.append(f"  &lt;focus&gt;{active_topic}&lt;/focus&gt;")
    return "&lt;conversation_state&gt;\n" + "\n".join(lines) + "\n&lt;/conversation_state&gt;"</code></pre>
</div>

          <div class="deep-dive-only">
          <p>This 50-100 token block tells the model what turn it's on, what phase the conversation is in, what time of day it is, and which tools it already called. Without it, the agent re-calls tools it used two turns ago or asks "how can I help you today?" on turn 8.</p>

          <p>I'm dedicating Part 3 entirely to <strong>long-term memory</strong> (persistent knowledge across conversations, things like user preferences, facts, summaries, learned patterns) because it's complex enough to need its own treatment.</p>
          </div>

          <div class="deep-dive-only">
          <p>I spend the most engineering time on <strong>retrieved information</strong>, the RAG layer. External knowledge from documents, databases, and APIs gets injected on-demand, and the design decision I keep coming back to is to let the model pull what it needs via tool calls rather than front-loading everything. The tricky part is evaluating retrieval quality, because the model's output can fail for reasons that have nothing to do with what you retrieved, and standard metrics like recall@k don't capture whether the retrieved context actually helped the model reason correctly. Part 2 goes deep on retrieval and evaluation.</p>

          <p>In my agent, the retrieval tool automatically enriches queries with what it already knows about the patient:</p>
          </div>

<div class="code-block" data-filename="tools.py" data-lang="python">
<pre><code>@tool
async def search_knowledge_base(query, document_type=None, tags=None, condition_type=None):
    state = await session_store.get(session_id)

    # Auto-apply condition filter from patient profile
    if not condition_type and state.patient_profile.diagnosis:
        condition_type = state.patient_profile.diagnosis

    filters = RetrievalFilters(document_type=document_type, tags=tags, condition_type=condition_type)
    response = await retriever.retrieve(query=query, filters=filters, state=state)</code></pre>
</div>

          <div class="deep-dive-only">
          <p>The agent decides when to search (pull, not push), and the tool enriches the query behind the scenes. The condition filter means a Type 2 patient never sees Type 1 insulin pump troubleshooting guides, without the patient or the model needing to specify that constraint explicitly.</p>

          <p><strong>Tool definitions</strong> are the sneaky budget item. Function signatures the system can invoke consume context tokens whether they're used or not, and there's a genuine tension in how to manage them. <a href="#ref-inngest">Inngest</a> recommends removing any tool used less than 10% of the time because performance degrades as tool count grows. But <a href="#ref-manus">Manus</a> found that dynamically loading and removing tools breaks KV-cache (a 10x cost difference between cached and uncached tokens), so their solution is logits masking, where tools stay in context but get suppressed at the decoding level through a state machine.</p>

          <p>I don't think there's a clean answer here yet. I'm currently leaning toward keeping tool counts low rather than worrying about cache-aware masking, mostly because my agents have 4-6 tools, not 40. If you're at Manus's scale with dozens of tools, the cache math probably dominates.</p>

          <p>The broader principle from <a href="#ref-manus">Manus's engineering blog</a> is worth internalizing regardless of tool count: never reorder or mutate tokens already in the KV-cache prefix. Treat your prompt prefix as append-only. Even changing JSON key ordering in a tool definition invalidates the cache from that point forward, and with agentic workloads running at roughly a 100:1 input-to-output token ratio, cache hits are existential for cost at scale. The major providers now offer explicit cache control: Anthropic supports up to 4 cache breakpoints per request with minimum cacheable lengths of 1,024 tokens for Sonnet and 4,096 for Opus, a default 5-minute TTL (refreshed on each hit), and a cache hierarchy where changes at any level (<code>tools</code> → <code>system</code> → <code>messages</code>) invalidate that level and everything after it. OpenAI offers automatic caching with a 50% discount on cached tokens. The gotchas are subtle: JSON key ordering instability in some languages (Swift, Go) breaks caches silently, toggling features like web search invalidates the system cache, and changing <code>tool_choice</code> invalidates the message cache.</p>

          <p><strong>Structured output</strong> specifications (JSON schemas, type definitions, output constraints) are easy to overlook in a token budget because they feel like "free" structure. They're not. At scale they add up, and I've seen schemas that consume 300+ tokens before the model even starts generating.</p>
          </div>

          <div class="algorithm-box deep-dive-only">
            <h4>Typical Token Allocation (Agentic System)</h4>
            <table>
              <thead>
                <tr><th>Component</th><th>Tokens</th><th>Behavior</th></tr>
              </thead>
              <tbody>
                <tr><td>System instructions</td><td>500-2,000</td><td>Static per session</td></tr>
                <tr><td>Conversation history</td><td>500-5,000</td><td>Grows, needs trimming</td></tr>
                <tr><td>RAG results</td><td>0-10,000</td><td>On-demand, per tool call</td></tr>
                <tr><td>Tool definitions</td><td>500-3,000</td><td>Permanent, always present</td></tr>
                <tr><td>State / structured output</td><td>50-500</td><td>Dynamic per turn</td></tr>
              </tbody>
            </table>
            <p>Total must fit the model's window (8k-200k depending on provider). Getting this budget right is less about cramming in more and more about being ruthless with what you leave out.</p>
          </div>

          <div class="theory-only">
            <div class="callout">
              <p><a href="#ref-mei">Mei et al.'s survey</a> also formalizes optimal retrieval as an information-theoretic problem. The best retrieval function maximizes mutual information between the target answer and retrieved knowledge, conditioned on the query:</p>
              <p>$$\text{Retrieve}^* = \arg\max_{\text{Retrieve}} \; I(Y^*;\; c_\text{know} \mid c_\text{query})$$</p>
              <p>And context assembly can be framed as Bayesian posterior inference, combining the likelihood of the query given a context with the prior over contexts given interaction history:</p>
              <p>$$P(C \mid c_\text{query}, \ldots) \propto P(c_\text{query} \mid C) \cdot P(C \mid \text{History}, \text{World})$$</p>
              <p>Again, more math than most practitioners will ever need day-to-day. But I found the mutual information framing genuinely clarifying for one specific question: when two retrieval approaches return different documents, the one that tells the model more *new* information (higher mutual information with the answer, conditional on the query) is the better one. That helped me reason about why hybrid retrieval beats pure semantic search in my own system.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 3: Write / Select / Compress / Isolate               -->
        <!-- ============================================================ -->
        <section id="four-strategies">
          <h2>Write / Select / Compress / Isolate</h2>

          <div class="tldr-only">
            <ul class="list-tight">
              <li><strong>Write:</strong> move information out of the window into external storage</li>
              <li><strong>Select:</strong> retrieve relevant information back via RAG or memory queries</li>
              <li><strong>Compress:</strong> tiered approach (raw context → compact by stripping filler → summarize only when needed), compress proactively at ~75% capacity, trim in atomic turn groups. Never compress instructions and data together</li>
              <li><strong>Isolate:</strong> split work across separate LLM calls so each gets focused context</li>
            </ul>
            <p>Most teams over-invest in writing and selecting; the real gains are in compression and isolation.</p>
          </div>

          <div class="deep-dive-only">
          <p>Every production context system I've seen uses some combination of four strategies, and <a href="#ref-langchain">LangChain's framework</a> gives them clean names. I find the vocabulary useful not because the categories are surprising, but because asking "which of these four am I underinvesting in?" is usually the fastest way to improve a system.</p>

          <p><strong>Writing</strong> means getting information out of the context window and into external storage for later retrieval. Scratchpads let the agent write intermediate notes during a session (observations, partial results, plans) that persist via tool calls or state objects without occupying the window continuously. Memories go further, enabling cross-session retention by extracting reflections or facts and storing them in a persistent backend. The <a href="#ref-manus">Manus</a> team has a concrete version of this that I really like: their agents maintain a <code>todo.md</code> file during complex tasks, writing and re-reading their plan to counteract the "lost-in-the-middle" problem across ~50 average tool calls. It's charmingly simple for a state-of-the-art agent. (Though Manus later found that roughly a third of all agent actions were spent updating the todo list, and shifted to a dedicated planner agent instead. Even clever patterns have costs.)</p>

          <p>My first version wrote a rolling summary every 5 turns and called it a day. That's fine for a prototype, but production systems benefit from a tiered approach that matches the intensity of compression to how much pressure the window is actually under. I think of it as three tiers:</p>

          <ol>
            <li><strong>Raw context</strong> when below ~75% capacity. If the window has room, don't compress at all. Raw turns carry more signal than any summary.</li>
            <li><strong>Compact first</strong> when approaching the budget. Strip low-signal content (greetings, filler acknowledgments, tool call boilerplate) while keeping messages structurally intact. No LLM call needed.</li>
            <li><strong>Summarize</strong> only when compaction isn't enough. Replace older turns with a generated summary, preserving the recent window verbatim.</li>
          </ol>

          <p>The 75% threshold matters more than it might seem. I call it the <strong>pre-rot threshold</strong>: compress proactively at ~75% capacity, not reactively at 95%. By the time you're near the wall, attention quality has already degraded across the last 20% of growth (this connects directly to the distraction failure mode in Section 4). Compressing early keeps the model in its high-performance zone.</p>
          </div>

<div class="code-block" data-filename="context_manager.py" data-lang="python">
<pre><code>async def manage_context(self, session_id, current_turn, window_budget):
    messages = await self._store.get_messages(session_id)
    current_usage = estimate_tokens(messages)

    # Tier 1: Raw context fits — do nothing
    if current_usage < window_budget * 0.75:
        return messages

    # Tier 2: Compact — strip low-signal content, keep messages intact
    compacted = self._compact(messages)
    if estimate_tokens(compacted) < window_budget * 0.85:
        return compacted

    # Tier 3: Summarize — replace old turns with generated summary
    cutoff = self._find_summary_boundary(compacted)
    summary = await self._summarize(compacted[:cutoff])
    return [summary_message(summary)] + compacted[cutoff:]</code></pre>
</div>

          <div class="deep-dive-only">
          <p>The <code>_compact</code> step does straightforward string surgery: it strips "thanks!", "ok sounds good", empty assistant acknowledgments, and tool call metadata that the model doesn't need to see on future turns. No LLM call, just pattern matching. The summarization tier only fires when compaction alone can't get below 85% capacity, which in practice means conversations beyond ~15 turns.</p>

          <p>One finding I keep coming back to from the <a href="#ref-rcc">recurrent context compression</a> research: compressing instructions and context simultaneously degrades responses. You need to compress the data but preserve the instructions separately. I was summarizing entire turns including the system-injected guidance in my first attempt, and the model started ignoring its own rules.</p>

          <p><strong>Selecting</strong> is the complement, retrieving relevant information back when needed. This includes reading back scratchpad notes from earlier steps, querying stored memories using embeddings or keyword search, and full RAG pipelines over documents or code. One pattern worth calling out specifically: if you have many tools, you can use RAG over tool descriptions to select the right one. The <a href="#ref-rag-mcp">RAG-MCP paper</a> (Writer.com) measured a 3x improvement in selection accuracy compared to exposing all tools at once (from 13.6% to 43.1%), which I find somewhat funny because it means the agent needs a search engine just to find its own capabilities.</p>

          <p>Selection quality depends heavily on query quality. My agent rewrites the user's message into a self-contained retrieval query before searching:</p>
          </div>

<div class="code-block" data-filename="query_rewriter.py" data-lang="python">
<pre><code># "My numbers have been all over the place lately"
# -&gt; "blood sugar management strategies Type 2 patient on metformin irregular post-meal readings"

async def rewrite(self, query, conversation_history, patient_profile):
    recent_turns = conversation_history[-3:]
    prompt = f"""Rewrite the search query to be self-contained.
    Resolve pronouns, add implicit context from conversation.
    Known: diagnosis={profile.diagnosis}, medications={profile.current_medications}

    Rules:
    - Keep it concise (under 30 words)
    - Resolve pronouns ("it" -&gt; the patient's condition)
    - Strip emotional language, focus on the information need"""</code></pre>
</div>

          <div class="deep-dive-only">
          <p>The comment at the top shows why this matters. "My numbers have been all over the place lately" has almost zero retrieval value as-is: no condition type, no medication context, no timeframe. The rewriter infers "post-meal readings" from the last 3 turns, adds the patient's diagnosis and medication context, and produces a query that actually hits relevant documents.</p>

          <p><strong>Compression</strong> reduces tokens while maintaining task performance. Summarization replaces older conversation history with a condensed version (Claude Code applies auto-compact when approaching context limits). Trimming removes older messages, and the critical detail is how you trim. Per-message FIFO (drop the oldest message) is the naive approach, and it breaks things in subtle ways. Dropping an individual assistant message can orphan a tool result from its tool call, producing a conversation that the API rejects or the model misinterprets. Production SDKs from both <a href="#ref-openai-agents">OpenAI</a> and Anthropic trim in <strong>atomic turn groups</strong>: a user message plus all assistant messages and tool results that follow it, removed as a unit.</p>
          </div>

<div class="code-block" data-filename="trimming.py" data-lang="python">
<pre><code>def trim_oldest_turn(messages):
    """Remove the oldest complete turn group atomically."""
    if not messages:
        return messages
    i = 0
    while i < len(messages):
        if i > 0 and messages[i].role == "user":
            break
        i += 1
    return messages[i:]</code></pre>
</div>

          <div class="deep-dive-only">
          <p>The function walks forward from the start until it hits the next user message, then slices off everything before it. One complete turn (user message, assistant response, any tool calls and results in between) gets removed as an atomic unit. My agent calls this in a loop until the conversation fits within the character budget, which handles the same two problems as a message cap plus character budget but without the risk of orphaned tool results.</p>

          <p><strong>Isolation</strong> means splitting information across separate processing units so each one gets a clean, focused context window. Multi-agent systems give each sub-agent its own window focused on a specific subtask, returning a condensed summary (1,000-2,000 tokens) to the lead agent. HuggingFace's CodeAgent isolates token-heavy objects in sandbox environments, keeping only references in the main context. You can also separate LLM-exposed fields from auxiliary context storage in your state schema, because not everything the system knows needs to be in the window.</p>

          <p>My agent isolates safety classification into separate LLM calls so the guardrail context never contaminates the main patient conversation:</p>
          </div>

<div class="code-block" data-filename="graph.py" data-lang="python">
<pre><code># Each node runs in its own isolated LLM call with its own context
graph = StateGraph(CoachingState)

graph.add_node("input_gate", input_gate_node)       # Safety classifier
graph.add_node("pro_react_agent", pro_agent)         # Main agent (complex queries)
graph.add_node("flash_react_agent", flash_agent)     # Main agent (simple messages)
graph.add_node("output_gate", output_gate_node)      # Scope validator

graph.set_entry_point("input_gate")
graph.add_conditional_edges("input_gate", route_after_input_gate, {
    END: END,                                        # Blocked -&gt; stop
    "pro_react_agent": "pro_react_agent",            # Complex -&gt; Pro + thinking
    "flash_react_agent": "flash_react_agent",        # Simple -&gt; Flash
})
graph.add_edge("pro_react_agent", "output_gate")
graph.add_edge("flash_react_agent", "output_gate")
graph.add_edge("output_gate", END)</code></pre>
</div>

          <div class="deep-dive-only">
          <p>The input gate sees only the latest user message and a short classification prompt. The output gate sees only the agent's response and a scope-checking prompt. Neither gate's context (safety rules, classification examples) appears in the main agent's window, keeping the patient conversation clean and focused.</p>

          <p>If you're building an agent and something feels off about the outputs, run through these four categories and ask which one you're neglecting. In my experience the answer is almost always compression or isolation; people over-invest in writing and selecting because those feel like "building features," while trimming old history and splitting contexts feel like cleanup work.</p>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 4: How Context Fails                                 -->
        <!-- ============================================================ -->
        <section id="failure-modes">
          <h2>How Context Fails</h2>

          <div class="tldr-only">
            <p>Four context failure modes to check when an agent misbehaves: poisoning (errors compound), distraction (too much history), confusion (noise as signal), clash (contradictory instructions). Different failures need different fixes.</p>
          </div>

          <div class="deep-dive-only">
          <p>When an agent misbehaves, my first question is always "what kind of context failure is this?" because the mitigations are completely different depending on the answer. <a href="#ref-breunig">Drew Breunig</a> laid out four failure modes that I think cover most of what goes wrong, and I've started using them as a debugging checklist.</p>
          </div>

          <div class="algorithm-box">
            <h4>Four Context Failure Modes</h4>
            <table>
              <thead>
                <tr><th>Mode</th><th>Cause</th><th>Fix</th></tr>
              </thead>
              <tbody>
                <tr><td>Poisoning</td><td>Error enters context, compounds downstream</td><td>Validate before memory writes</td></tr>
                <tr><td>Distraction</td><td>History too long, drowns out training</td><td>Aggressive trimming + summarization</td></tr>
                <tr><td>Confusion</td><td>Noise treated as signal</td><td>Curate ruthlessly, earn every token</td></tr>
                <tr><td>Clash</td><td>Contradictory instructions in prompt</td><td>Clear precedence hierarchy</td></tr>
              </tbody>
            </table>
          </div>

          <div class="deep-dive-only">
          <p><strong>Context poisoning</strong> is the scariest one. A hallucination or error enters the context and gets repeatedly referenced, compounding mistakes over time. Once a wrong fact lands in the conversation history, the model treats it as ground truth and builds on it. Google DeepMind's Gemini 2.5 technical report showed just how bad this gets: a Pokemon-playing agent hallucinated the existence of an item called "TEA," wrote it into its goals scratchpad, and then spent hundreds of actions trying to find something that doesn't exist in the game. The agent also developed a "black-out strategy" (intentionally fainting all its Pokemon to teleport) rather than navigating normally. Once a hallucination enters a persistent scratchpad, the model treats it as ground truth and builds increasingly nonsensical plans on top of it. The fix is to validate information before writing to long-term memory, treating memory writes like database writes where you check constraints before committing.</p>

          <p><strong>Context distraction</strong> is subtler. The context grows so long that the model over-focuses on accumulated history and neglects what it learned during training. Beyond ~100k tokens, I've noticed agents tend toward repeating actions from history rather than synthesizing novel plans. Aggressive trimming and summarization help, along with actively removing completed or irrelevant sections. I suspect most people's context windows are 2-3x larger than they need to be.</p>

          <p>This is why the pre-rot threshold from Section 3 matters: compress proactively at ~75% capacity, not reactively at 95%. By the time you're near the wall, attention quality has already degraded across the last 20% of growth.</p>

          <p><strong>Context confusion</strong> is what happens when superfluous information gets treated as signal because the model can't distinguish noise from relevant information when everything is dumped in together. <a href="#ref-anthropic">Anthropic's</a> guiding principle is the right one here: "find the smallest set of high-signal tokens that maximize the likelihood of your desired outcome." Every token should earn its place, and most don't.</p>

          <p><strong>Context clash</strong> is the one I find most often in my own code, probably because it's the easiest to create accidentally. It happens when new information conflicts with existing information already in the prompt, and contradictory instructions produce unpredictable behavior. My own agent has one I caught during an audit for this post. The system prompt says "When READING information you already have in the patient context above, use it directly, do not re-fetch with <code>get_patient_profile</code>." But <code>get_patient_profile</code> is still available as a callable tool. The instruction and the tool list contradict each other. The agent sometimes calls the tool anyway, wasting a round-trip to fetch data that's already in the prompt. The fix is straightforward (remove the tool), but the clash was easy to miss because the instruction is in the <code>&lt;tools&gt;</code> section of the prompt and the tool definition is in Python code, and I never reviewed them side by side until I went looking for exactly this kind of problem.</p>
          </div>

          <div class="theory-only">
            <div class="callout">
              <p>There's a deeper mechanism behind context distraction that <a href="#ref-anthropic">Anthropic</a> calls <strong>context rot</strong>. They attribute it to the $O(n^2)$ pairwise token relationships in self-attention: at 100K+ tokens, the model's attention budget is spread across billions of pair interactions, and critical information gets drowned out. The full story is more nuanced than that framing suggests. Positional encoding degradation (especially RoPE-based encodings at positions beyond the training distribution), attention sink phenomena where early tokens absorb disproportionate weight, and training data skew toward shorter contexts all contribute. But the practical consequence is the same regardless of which mechanism dominates.</p>
              <p><a href="#ref-mei">Mei et al.'s survey</a> cites the <strong>lost-in-the-middle</strong> finding (<a href="#ref-liu">Liu et al., 2023</a>) to quantify this: performance degrades significantly when relevant information sits in the middle of long contexts versus the beginning or end. Liu et al. found a consistent U-shaped curve across models and settings, with accuracy dropping by 20%+ in some configurations. The exact magnitude varies by model, task, and number of documents, but the core result (middle positions perform worst) has been replicated across multiple studies and model families.</p>
              <p>Frontier models are making progress on this. Gemini 1.5 Pro achieves >99.7% single-needle recall at 1M tokens, and Gemini 3 Pro improves further. But multi-needle recall (finding and synthesizing information from several places in the context, the more realistic agentic scenario) is still genuinely hard: Gemini 1.5 Pro drops to ~60% recall when multiple needles are scattered across 1M tokens. The practical implication still holds for most production systems: put your most important context at the beginning and end of the window, not the middle. <a href="#ref-openai-prompting">OpenAI's GPT-4.1 prompting guide</a> goes further and explicitly recommends placing instructions at both the beginning and end of the provided context for long-context usage.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 5: A Real Token Budget                               -->
        <!-- ============================================================ -->
        <section id="token-budget">
          <h2>A Real Token Budget</h2>

          <div class="tldr-only">
            <p>Token budget from the diabetes coaching agent I built with LangGraph:</p>
          </div>

          <div class="deep-dive-only">
          <p>All of the above is easier to understand with a concrete example, so let me walk through the diabetes management coaching assistant I built as a ReAct agent with LangGraph. The system prompt ranges from ~2,000 to ~4,050 tokens depending on session maturity, assembled dynamically from several blocks.</p>
          </div>

          <div style="border:1px solid var(--border); border-radius:8px; padding:24px; margin:24px 0; background:#f0ead9; overflow-x:auto;">
            <div style="display:flex; align-items:center; gap:0; min-width:700px; justify-content:center; flex-wrap:nowrap;">
              <div style="background:#f5f0e3; border:2px solid var(--border); border-radius:8px; padding:8px 12px; text-align:center; min-width:70px;">
                <div style="font-weight:600; font-size:13px;">Patient</div>
                <div style="font-size:10px; color:var(--muted);">message</div>
              </div>
              <div style="color:var(--border); font-size:18px; padding:0 6px;">&rarr;</div>
              <div style="background:#e5dccb; border:2px solid var(--border); border-radius:8px; padding:8px 12px; text-align:center; min-width:70px;">
                <div style="font-weight:600; font-size:13px;">Input Gate</div>
                <div style="font-size:10px; color:var(--muted);">safety check</div>
              </div>
              <div style="color:var(--border); font-size:18px; padding:0 6px;">&rarr;</div>
              <div style="display:flex; flex-direction:column; gap:4px;">
                <div style="background:#f5f0e3; border:2px solid var(--border); border-radius:8px; padding:6px 12px; text-align:center;">
                  <div style="font-weight:600; font-size:13px;">Pro Agent</div>
                  <div style="font-size:10px; color:var(--muted);">complex + thinking</div>
                </div>
                <div style="background:#f5f0e3; border:2px solid var(--border); border-radius:8px; padding:6px 12px; text-align:center;">
                  <div style="font-weight:600; font-size:13px;">Flash Agent</div>
                  <div style="font-size:10px; color:var(--muted);">simple queries</div>
                </div>
              </div>
              <div style="color:var(--border); font-size:18px; padding:0 6px;">&rarr;</div>
              <div style="background:#e5dccb; border:2px solid var(--border); border-radius:8px; padding:8px 12px; text-align:center; min-width:70px;">
                <div style="font-weight:600; font-size:13px;">Output Gate</div>
                <div style="font-size:10px; color:var(--muted);">scope check</div>
              </div>
              <div style="color:var(--border); font-size:18px; padding:0 6px;">&rarr;</div>
              <div style="background:#f5f0e3; border:2px solid var(--border); border-radius:8px; padding:8px 12px; text-align:center; min-width:70px;">
                <div style="font-weight:600; font-size:13px;">Response</div>
              </div>
            </div>
            <div style="display:flex; justify-content:center; margin-top:12px;">
              <div style="border:1px dashed #c5c2b8; border-radius:8px; padding:8px 16px; display:flex; gap:16px; font-size:11px; color:var(--muted);">
                <span>Background: rolling summary</span>
                <span style="color:var(--border);">|</span>
                <span>profile updates</span>
                <span style="color:var(--border);">|</span>
                <span>outcome tracking</span>
              </div>
            </div>
            <div style="text-align:center; margin-top:8px; font-size:12px; color:var(--muted);">Each node runs in its own isolated LLM call with its own context window</div>
          </div>

          <table>
            <thead>
              <tr><th>Component</th><th>Tokens</th><th>Type</th></tr>
            </thead>
            <tbody>
              <tr><td>Conversation state (turn count, phase, datetime)</td><td>50-100</td><td>Dynamic (every turn)</td></tr>
              <tr><td>Identity block (personality, voice)</td><td>~350</td><td>Static</td></tr>
              <tr><td>Boundaries (5 hard scope rules)</td><td>~280</td><td>Static</td></tr>
              <tr><td>Patient context (profile, summary, goals)</td><td>80-700</td><td>Dynamic (per session)</td></tr>
              <tr><td>Approach + tools + response guides</td><td>~470</td><td>Static</td></tr>
              <tr><td>Few-shot examples (6)</td><td>~550</td><td>Static</td></tr>
              <tr><td>RAG results (last 3, conditional)</td><td>0-1,500</td><td>Conditional</td></tr>
              <tr><td><strong>System prompt total</strong></td><td><strong>~2,000-4,050</strong></td><td><strong>Mixed</strong></td></tr>
              <tr><td>Conversation window (6 turns max)</td><td>500-5,000</td><td>Dynamic (rolling)</td></tr>
              <tr><td>Tool results (<code>search_knowledge_base</code>)</td><td>0-10,000</td><td>On-demand</td></tr>
            </tbody>
          </table>

          <div class="deep-dive-only">
          <p>The conversation window holds 6 turns max, char-budgeted at 120,000 characters. Messages from turns already covered by the rolling summary are excluded.</p>

          <p>Everything gets assembled in a single <code>prepare_context</code> hook that runs before every LLM call in the ReAct loop. The static sections form a stable prefix for KV-cache hits, and the volatile conversation state goes at the end:</p>
          </div>

<div class="code-block" data-filename="context.py" data-lang="python">
<pre><code>async def prepare_context(state: CoachingState):
    session_id = state.get("session_id", "default")
    session_state = await session_store.get(session_id)
    turn_count = session_state.turn_count
    cache_key = (session_id, turn_count)

    # Skip rebuild if nothing mutated since last call
    has_mutation = _has_mutating_tool(state["messages"])
    if cache_key in _prompt_cache and not has_mutation:
        system_prompt = _prompt_cache[cache_key]
    else:
        summary = await session_store.get_latest_summary(session_id)
        episodes = await session_store.get_recent_episodes(session_id, limit=5)
        tool_results = await session_store.get_recent_tool_results(session_id, limit=3)

        system_prompt = build_system_prompt(
            profile=session_state.patient_profile,
            active_strategies=session_state.active_strategies,
            goals=session_state.goals, outcomes=session_state.outcomes,
            session_summary=summary_text,
        )

        # Static prefix first (cacheable), volatile state appended last
        state_block = build_conversation_state(turn_count, phase, recent_tool_names, ...)
        system_prompt = system_prompt + "\n\n" + state_block

    # Trim conversation: drop oldest complete turn groups atomically
    conversation = [m for m in messages if not isinstance(m, SystemMessage)]
    while estimate_chars(conversation) &gt; remaining_budget and len(conversation) &gt; 2:
        conversation = trim_oldest_turn(conversation)

    return {"llm_input_messages": [SystemMessage(system_prompt)] + conversation}</code></pre>
</div>

          <div class="deep-dive-only">
          <p>This function is where all the context engineering actually happens. Load state, build the prompt from components, append the volatile state block at the end (so the static sections form a cacheable prefix), and trim the conversation using turn-group-aware removal. Every technique I described in the earlier sections (XML structure, tiered compression, atomic trimming, isolation) converges in this one function.</p>
          </div>

          <div class="algorithm-box deep-dive-only">
            <h4>Context Engineering Techniques Used</h4>
            <table>
              <thead><tr><th>Category</th><th>Technique</th><th>Implementation</th></tr></thead>
              <tbody>
                <tr><td>Structure</td><td>XML-tagged sections</td><td><code>&lt;identity&gt;</code>, <code>&lt;boundaries&gt;</code>, <code>&lt;patient-context&gt;</code>, etc.</td></tr>
                <tr><td>Compression</td><td>Tiered (raw → compact → summarize)</td><td>Pre-rot threshold at 75% capacity</td></tr>
                <tr><td>Compression</td><td>Atomic turn-group trimming</td><td>Drop oldest complete turn, never orphan tool results</td></tr>
                <tr><td>Memory</td><td>Semantic (patient profile)</td><td>Structured schema, tool-driven updates</td></tr>
                <tr><td>Memory</td><td>Episodic (outcomes)</td><td>Created on <code>track_outcome</code>, stored with emotion</td></tr>
                <tr><td>RAG</td><td>Hybrid search</td><td>Dense + sparse + RRF + cross-encoder reranking</td></tr>
                <tr><td>RAG</td><td>Query rewriting</td><td>Pronoun resolution + profile context injection</td></tr>
                <tr><td>Routing</td><td>Dual-model</td><td>Flash for simple messages, Pro with thinking for complex</td></tr>
                <tr><td>Safety</td><td>Isolated gates</td><td>Input + output classifiers in separate LLM calls</td></tr>
              </tbody>
            </table>
          </div>

          <h3>Two Catches from My Own Audit</h3>

          <div class="tldr-only">
            <ul class="list-tight">
              <li><strong>KV-cache violation:</strong> Volatile state prepended to prompt prefix, invalidating cache every turn (10x cost). Fix: move to end.</li>
              <li><strong>Profile-in-prompt-and-tool clash:</strong> <code>get_patient_profile</code> duplicates data already in system prompt; the tool's existence signals to the model that the profile might not be in context, making it less likely to trust data it already has.</li>
            </ul>
          </div>

          <div class="deep-dive-only">
          <p>When I audited this agent against the best practices I'd just finished researching, two problems jumped out that I wouldn't have caught without specifically looking.</p>

          <p>The expensive one is a <strong>KV-cache violation</strong>. Look at the <code>prepare_context</code> code above. The volatile <code>&lt;conversation_state&gt;</code> block (which changes every turn with new turn count, new timestamp, new tool history) gets prepended to the start of the system prompt. KV-cache works by matching a prefix: if the first N tokens are identical between calls, the provider can reuse the cached key-value pairs and charge you the cached rate. By putting volatile data at the very start, every single turn invalidates the entire cache. <a href="#ref-manus">Manus</a> reports this is a 10x cost difference (<span class="nokatex">$0.30/MTok cached vs $3/MTok uncached</span> on Claude Sonnet). The fix is one line, move the state block to the end of the prompt instead of the beginning, so the static sections (identity, boundaries, examples) form a stable prefix that caches across turns. The broader principle from Section 2 applies here too: treat the prompt prefix as append-only, and sort sections by volatility so the stable parts come first.</p>
          </div>

          <div style="border:1px solid var(--border); border-radius:8px; padding:24px; margin:24px 0; background:#f0ead9;">
            <div style="margin-bottom:16px;">
              <div style="display:flex; align-items:center; gap:8px; margin-bottom:10px;">
                <span style="font-size:14px; font-weight:600; color:var(--vermillion);">Before</span>
                <span style="font-size:12px; color:var(--muted);">Cache breaks on every turn</span>
              </div>
              <div style="display:flex; gap:2px; font-size:11px; border-radius:6px; overflow:hidden;">
                <div style="background:#e8cbc0; border:1px solid #c9a99e; padding:8px 10px; min-width:100px; text-align:center; position:relative;">
                  <div style="font-weight:600;">conversation_state</div>
                  <div style="color:var(--vermillion); font-size:10px;">volatile</div>
                </div>
                <div style="background:#e5dccb; border:1px solid var(--border); padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">identity</div>
                  <div style="color:var(--muted); font-size:10px;">static</div>
                </div>
                <div style="background:#e5dccb; border:1px solid var(--border); padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">boundaries</div>
                  <div style="color:var(--muted); font-size:10px;">static</div>
                </div>
                <div style="background:#e5dccb; border:1px solid var(--border); padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">examples</div>
                  <div style="color:var(--muted); font-size:10px;">static</div>
                </div>
                <div style="background:#e5dccb; border:1px solid var(--border); padding:8px 10px; flex:1; text-align:center; color:var(--muted);">...</div>
              </div>
              <div style="font-size:10px; color:var(--vermillion); margin-top:4px;">&uarr; Prefix changes every turn. 0% cache hit rate.</div>
            </div>
            <div>
              <div style="display:flex; align-items:center; gap:8px; margin-bottom:10px;">
                <span style="font-size:14px; font-weight:600; color:#5c7a4a;">After</span>
                <span style="font-size:12px; color:var(--muted);">Static prefix caches across turns</span>
              </div>
              <div style="display:flex; gap:2px; font-size:11px; border-radius:6px; overflow:hidden;">
                <div style="background:#d5dfc8; border:1px solid #b3c4a0; padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">identity</div>
                  <div style="color:#5c7a4a; font-size:10px;">cached</div>
                </div>
                <div style="background:#d5dfc8; border:1px solid #b3c4a0; padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">boundaries</div>
                  <div style="color:#5c7a4a; font-size:10px;">cached</div>
                </div>
                <div style="background:#d5dfc8; border:1px solid #b3c4a0; padding:8px 10px; flex:1; text-align:center;">
                  <div style="font-weight:500;">examples</div>
                  <div style="color:#5c7a4a; font-size:10px;">cached</div>
                </div>
                <div style="background:#d5dfc8; border:1px solid #b3c4a0; padding:8px 10px; flex:1; text-align:center; color:#5c7a4a;">...</div>
                <div style="background:#ece3c8; border:1px solid #d5c9a0; padding:8px 10px; min-width:100px; text-align:center;">
                  <div style="font-weight:600;">conversation_state</div>
                  <div style="color:#8a7a3c; font-size:10px;">volatile</div>
                </div>
              </div>
              <div class="nokatex" style="font-size:10px; color:#5c7a4a; margin-top:4px;">&uarr; Stable prefix. $0.30/MTok cached vs $3.00/MTok uncached (10x).</div>
            </div>
          </div>

          <div class="deep-dive-only">
          <p>The subtler one is a <strong>profile-in-prompt-and-tool clash</strong>. The patient profile is already injected into the <code>&lt;patient-context&gt;</code> section of the system prompt on every turn. But <code>get_patient_profile</code> also exists as a callable tool. The tool exists to let the agent "check what it knows," but the agent already knows; it's right there in the prompt. I mentioned this in Section 4 as a context clash, and it is, but it's also context confusion in a way I didn't appreciate until I watched the agent's behavior. The tool's existence signals to the model that the profile might <em>not</em> be in context, which makes it less likely to trust the data it already has. The instruction says "use the profile in <code>&lt;patient-context&gt;</code> directly," but the tool's mere availability creates an implicit counter-signal. Removing the tool entirely fixed the redundant fetches and, more importantly, made the agent more confident in referencing profile data from the prompt.</p>
          </div>

          <div class="theory-only">
            <div class="callout">
              <p>The benchmarks suggest context engineering matters a lot more than most people realize. On GAIA, human accuracy is 92% while GPT-4 hits 15%. A 77-point gap. On GTA, GPT-4 completes fewer than 50% of tasks. On WebArena, the top agent (IBM CUGA) reaches only 61.7%. These benchmarks all require integrating information from multiple sources, using tools, and maintaining state across steps, which is exactly what context engineering addresses.</p>
              <p>Memory systems fare poorly too. LongMemEval (500 curated questions) finds 30% accuracy degradation in commercial assistants during extended interactions. GPT-4, Claude, and Llama 3.1 all struggle with episodic memory involving interconnected events, even in brief contexts. The gap between model capability on narrow benchmarks and system capability on realistic tasks is, I think, the context engineering gap.</p>
            </div>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 6: Measuring Context Quality                         -->
        <!-- ============================================================ -->
        <section id="measuring">
          <h2>Measuring Context Quality</h2>

          <div class="tldr-only">
            <p>Track cache hit rate, cost per task, and task completion rate vs. context size. Run A/B tests on context strategies, not just prompts. Benchmarks exist (ContextBench, Letta Context-Bench) but your own eval suite matters more.</p>
          </div>

          <div class="deep-dive-only">
          <p>Everything I've described so far is a strategy, but strategies without measurement are just vibes. The hardest part of context engineering in practice is knowing whether your changes actually improved things, because the feedback loop is noisy. A model might fail a task because the retrieval was wrong, or because the instructions were ambiguous, or because the conversation history drowned out the relevant context, and the output alone doesn't tell you which.</p>

          <p>I track four operational metrics in my agent, and each one catches a different class of problems.</p>

          <p><strong>Cache hit rate</strong> is the cheapest signal. Anthropic returns <code>cache_read_input_tokens</code> and <code>cache_creation_input_tokens</code> in every API response, and the ratio tells you whether your prefix ordering is actually stable across turns. After fixing the KV-cache violation from Section 5, my cache hit rate went from ~0% to ~85% on turns 2+, which translated directly into a cost reduction I could verify from the billing dashboard. If your cache hit rate is low, something in your prompt prefix is changing between calls, and the fix is almost always moving volatile content later in the prompt.</p>

          <p><strong>Per-component token budget</strong> is the one I check least often but learn the most from when I do. I log the token count of each prompt section (identity, boundaries, patient context, examples, conversation history, tool results) on every call and review the distributions weekly. The surprises are always in the same place: conversation history growing larger than expected because the trimming threshold was too generous, or RAG results consuming 3x the budget I'd allocated because the reranker wasn't filtering aggressively enough. The budget table from Section 5 was my design-time estimate; the logged distributions are what actually happened.</p>

          <p><strong>Task completion rate as a function of context size</strong> is the metric that tells you whether more context is actually helping. I bucket conversations by total input tokens (under 5K, 5-15K, 15-30K, 30K+) and measure the rate at which the agent completes the patient's request without needing a human handoff. In my system, completion rate peaks in the 5-15K range and degrades slightly above 30K, which is consistent with the distraction failure mode from Section 4. If your completion rate doesn't degrade with context size, either your task is simple enough that context quality doesn't matter, or your compression strategy is working.</p>

          <p><strong>Cost per completed task</strong> is what ties everything together. <a href="#ref-letta-bench">Letta's Context-Bench</a> found that total cost is a better metric than per-token price because models with higher unit costs sometimes use far fewer tokens. Claude Sonnet 4.5 led their benchmark at 74.0% accuracy for $24.58, while GPT-5 reached 72.67% for $43.56, almost double the cost for slightly lower performance. I track this monthly by dividing total API spend by completed conversations, and it's the number I report to stakeholders because it captures the combined effect of cache efficiency, context size, model routing (Flash vs. Pro), and retrieval quality in a single figure.</p>
          </div>

          <div class="deep-dive-only">
          <p>Beyond operational metrics, several benchmarks now evaluate context engineering directly. <a href="#ref-contextbench">ContextBench</a> (February 2025) tests whether coding agents can retrieve the right context from 66 real repositories across 8 languages. The headline finding is sobering: even state-of-the-art models achieve block-level F1 below 0.45 and line-level F1 below 0.35. Higher recall was consistently favored over precision, suggesting it's better to include a few irrelevant chunks than to miss a critical one. And sophisticated scaffolding didn't necessarily lead to better retrieval, a finding that complicates the "just add more RAG infrastructure" instinct.</p>

          <p>What I don't yet have and want is an offline eval suite where I can replay recorded conversations against different context strategies and measure the output quality difference. Right now I A/B test by deploying two strategies to different user segments and comparing completion rates, which works but is slow and requires enough traffic to be statistically meaningful. If your system is low-traffic, logging every API call (prompt, completion, token counts, cache stats) and replaying against modified strategies offline is probably the more practical approach.</p>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 7: What I Learned                                    -->
        <!-- ============================================================ -->
        <section id="what-i-learned">
          <h2>What I Learned</h2>

          <div class="tldr-only">
            <p>Context engineering is mostly about removal, not addition. Every improvement involved taking something out or moving it around. Context strategy isn't portable across providers; test on every model you support.</p>
          </div>

          <div class="deep-dive-only">
          <p>The thing that surprised me most writing this post wasn't any individual technique. It was auditing my own agent against the best practices I'd just finished writing about, and finding violations in code I'd written that same week. The KV-cache ordering was backwards. A redundant tool was actively undermining the model's confidence in data it already had. Knowing and doing are different skills.</p>

          <p>If I had to distill this post into one actionable idea, it's that context engineering is mostly about removal, not addition. The instinct is always to add more information, more tools, more history, more instructions. But every improvement I've made to my agent involved taking something out or moving it around, not putting more in. Remove the redundant tool. Move the volatile state block to the end of the prompt. Compress proactively at 75% capacity instead of reactively at 95%. Trim in atomic turn groups instead of shaving individual messages. The best systems I've seen treat the token budget as a design principle rather than a limitation to work around.</p>

          <p>This covers within-session context management for a single agent. Multi-session continuity and memory systems that improve over time rather than accumulating garbage are Part 3 territory. Production agent harnesses where the filesystem becomes the source of truth, and the context window is just a view into external state, are Part 4. If you're building a single-session agent today, the patterns here apply directly; the later parts extend them to longer time horizons and more complex architectures.</p>

          <p>One caveat worth keeping in mind as you apply any of this. Context strategy isn't portable across providers. Different models have different attention patterns, different context window behaviors, and different sensitivities to prompt structure. What works for Claude might fail on GPT-4 might fail on Gemini. I've been burned by this enough times that I now test prompts on every model I plan to support, rather than assuming my architecture generalizes.</p>

          <p>If you spot errors or have war stories from your own context engineering work, I'd love to hear about it on <a href="https://x.com/HenryVu27">X</a> or <a href="https://www.linkedin.com/in/henry-vu27/">LinkedIn</a>.</p>
          </div>
        </section>

        <!-- ============================================================ -->
        <!-- SECTION 8: References                                        -->
        <!-- ============================================================ -->
        <section id="references" class="deep-dive-only">
          <h2>References</h2>

          <ol>
            <li id="ref-karpathy">Karpathy, A. "Context Engineering." <a href="https://x.com/karpathy/status/1937902205765607626">X/Twitter</a>, June 2025.</li>
            <li id="ref-lutke">Lutke, T. "Context Engineering over Prompt Engineering." <a href="https://x.com/tobi/status/1935533422589399127">X/Twitter</a>, June 2025.</li>
            <li id="ref-anthropic">Rajasekaran, P. et al. "Effective Context Engineering for AI Agents." <a href="https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents">Anthropic Engineering</a>, September 2025.</li>
            <li id="ref-langchain">Martin, L. "Context Engineering for Agents." <a href="https://blog.langchain.com/context-engineering-for-agents/">LangChain Blog</a>, July 2025.</li>
            <li id="ref-breunig">Breunig, D. "How Contexts Fail and How to Fix Them." <a href="https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html">dbreunig.com</a>, June 2025.</li>
            <li id="ref-manus">Ji, Y. "Context Engineering for AI Agents: Lessons from Building Manus." <a href="https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus">Manus Blog</a>, July 2025.</li>
            <li id="ref-spotify">Spotify Engineering. "Context Engineering: Background Coding Agents Part 2." <a href="https://engineering.atspotify.com/2025/11/context-engineering-background-coding-agents-part-2">engineering.atspotify.com</a>, November 2025.</li>
            <li id="ref-inngest">Inngest. "Five Critical Lessons for Context Engineering." <a href="https://www.inngest.com/blog/five-lessons-for-context-engineering">inngest.com</a>, 2025.</li>
            <li id="ref-mei">Mei, Z. et al. "A Survey of Context Engineering for Large Language Models." <a href="https://arxiv.org/abs/2507.13334">arXiv:2507.13334</a>, July 2025.</li>
            <li id="ref-willison">Willison, S. "Context Engineering." <a href="https://simonwillison.net/2025/Jun/27/context-engineering/">simonwillison.net</a>, June 2025.</li>
            <li id="ref-osmani">Osmani, A. "Context Engineering: Bringing Engineering Discipline to AI." <a href="https://addyo.substack.com/p/context-engineering-bringing-engineering">Substack</a>, 2025.</li>
            <li id="ref-fowler">Fowler, M. "Context Engineering for Coding Agents." <a href="https://martinfowler.com/articles/exploring-gen-ai/context-engineering-coding-agents.html">martinfowler.com</a>, 2025.</li>
            <li id="ref-liu">Liu, N.F. et al. "Lost in the Middle: How Language Models Use Long Contexts." <a href="https://arxiv.org/abs/2307.03172">arXiv:2307.03172</a>, 2023. Published in TACL, 2024.</li>
            <li id="ref-longmemeval">Wu, D. et al. "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory." <a href="https://arxiv.org/abs/2410.10813">arXiv:2410.10813</a>, 2024. Published at ICLR 2025.</li>
            <li id="ref-gaia">Mialon, G. et al. "GAIA: A Benchmark for General AI Assistants." <a href="https://arxiv.org/abs/2311.12983">arXiv:2311.12983</a>, 2023. Published at ICLR 2024.</li>
            <li id="ref-anthropic-harness">Anthropic. "How We Built Our Multi-Agent Research System." <a href="https://www.anthropic.com/engineering/built-multi-agent-research-system">Anthropic Engineering</a>, November 2025.</li>
            <li id="ref-openai-agents">OpenAI. "Agents SDK." <a href="https://github.com/openai/openai-agents-python">GitHub</a>, 2025.</li>
            <li id="ref-rcc">Huang, Y. et al. "Recurrent Context Compression: Efficiently Expanding the Context Window of LLM." <a href="https://arxiv.org/abs/2406.06110">arXiv:2406.06110</a>, 2024.</li>
            <li id="ref-rag-mcp">Writer.com. "RAG-MCP: Mitigating Prompt Bloat in Tool-Augmented LLMs." <a href="https://arxiv.org/abs/2505.03275">arXiv:2505.03275</a>, 2025.</li>
            <li id="ref-openai-prompting">OpenAI. "GPT-4.1 Prompting Guide." <a href="https://cookbook.openai.com/examples/gpt4-1_prompting_guide">OpenAI Cookbook</a>, 2025.</li>
            <li id="ref-letta-bench">Letta. "Context-Bench: Benchmarking Long-Horizon Agent Memory." <a href="https://www.letta.com/blog/context-bench">letta.com</a>, October 2025.</li>
            <li id="ref-contextbench">Fournier, C. et al. "ContextBench: A Benchmark for Context Retrieval in Coding Agents." <a href="https://arxiv.org/abs/2602.05892">arXiv:2602.05892</a>, February 2025.</li>
          </ol>
        </section>

      </article>
    </div>

    <footer class="footer">
      <a href="https://x.com/HenryVu27">X</a> · <a href="https://www.linkedin.com/in/henry-vu27/">LinkedIn</a> · <a href="#">GitHub</a>
    </footer>
  </main>

  <script src="../../js/katex-init.js"></script>
  <script src="../../js/mode-toggle.js"></script>
  <!-- Prism.js: syntax highlighting -->
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
  <script src="../../js/code-blocks.js"></script>
</body>
</html>
